[
  {
    "timestamp": "00:00 - 00:10",
    "context": "As they maneuver into the back seat, soft rustling sounds from clothing and movement are heard. The person narrates in Chinese, '这有个小坎哈哈哈... 可以往上, 抬脚抬脚... 上车了~' and the caption notes this speech serves as self-guiding instruction and confirmation of entering.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why does the speaker narrate those lines while getting into the car during 00:00–00:10?",
    "answer": "To self-guide themselves through the process of entering the vehicle and to confirm that they are in the car.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00929.mp4",
    "question_id": "00929_1",
    "clip_path": "clips/00929/00929__0000000_0010500.mp4"
  },
  {
    "timestamp": "00:00 - 00:10",
    "context": "As they maneuver into the seat, they produce soft rustling sounds from their clothing and movements.",
    "question_type": "Sound Source Identification",
    "question": "What generated the soft rustling sounds during 00:00–00:10?",
    "answer": "The person's clothing and bodily movements while maneuvering into the seat.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00929.mp4",
    "question_id": "00929_2",
    "clip_path": "clips/00929/00929__0000000_0010500.mp4"
  },
  {
    "timestamp": "00:00 - 00:10",
    "context": "They narrate the action in a clear, conversational female voice.",
    "question_type": "Sound Characteristics",
    "question": "How is the narrator's voice described during 00:00–00:10?",
    "answer": "A clear, conversational female voice.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00929.mp4",
    "question_id": "00929_3",
    "clip_path": "clips/00929/00929__0000000_0010500.mp4"
  },
  {
    "timestamp": "00:00 - 00:10",
    "context": "They narrate the action while maneuvering into the seat.",
    "question_type": "Temporal Information",
    "question": "When does the narration occur relative to the action of entering the vehicle?",
    "answer": "It occurs while maneuvering into the seat between 00:00 and 00:10.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00929.mp4",
    "question_id": "00929_4",
    "clip_path": "clips/00929/00929__0000000_0010500.mp4"
  },
  {
    "timestamp": "00:10 - 00:11",
    "context": "After settling into the seat, the person closes the car door, causing a single, loud, solid thud as the door latches shut.",
    "question_type": "Sound Source Identification",
    "question": "What produced the thud sound at 00:10–00:11?",
    "answer": "The car door latching shut when it was closed.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00929.mp4",
    "question_id": "00929_5",
    "clip_path": "clips/00929/00929__0009500_0011500.mp4"
  },
  {
    "timestamp": "00:10 - 00:11",
    "context": "Closing the car door causes a single, loud, solid thud.",
    "question_type": "Sound Characteristics",
    "question": "What are the acoustic qualities of the door-closing sound at 00:10–00:11?",
    "answer": "It is a single, loud, solid thud.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00929.mp4",
    "question_id": "00929_6",
    "clip_path": "clips/00929/00929__0009500_0011500.mp4"
  },
  {
    "timestamp": "00:10 - 00:11",
    "context": "The door-closing thud originates from the right side, less than a meter away.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the door-closing thud originate relative to the camera?",
    "answer": "From the right side, less than a meter away.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00929.mp4",
    "question_id": "00929_7",
    "clip_path": "clips/00929/00929__0009500_0011500.mp4"
  },
  {
    "timestamp": "00:10 - 00:11",
    "context": "After settling into the seat, the door is closed, producing the thud.",
    "question_type": "Temporal Information",
    "question": "When did the door-closing thud occur relative to settling into the seat?",
    "answer": "Immediately after settling into the seat, at 00:10–00:11.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00929.mp4",
    "question_id": "00929_8",
    "clip_path": "clips/00929/00929__0009500_0011500.mp4"
  },
  {
    "timestamp": "00:10 - 00:11",
    "context": "The caption specifies a single, loud thud as the door latches shut.",
    "question_type": "Counting",
    "question": "How many times did the door-closing thud occur at 00:10–00:11?",
    "answer": "Once.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00929.mp4",
    "question_id": "00929_9",
    "clip_path": "clips/00929/00929__0009500_0011500.mp4"
  },
  {
    "timestamp": "00:10 - 00:11",
    "context": "The loud, solid thud from the right side marks the door latching shut and the caption notes it marks the completion of entering the car.",
    "question_type": "Cross-Modal Reasoning",
    "question": "After the loud thud at 00:10–00:11, what does this indicate about the visual action?",
    "answer": "It indicates that the person has completed entering the car.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00929.mp4",
    "question_id": "00929_10",
    "clip_path": "clips/00929/00929__0009500_0011500.mp4"
  },
  {
    "timestamp": "00:00 - 00:16",
    "context": "[00:00 - 00:16] As the user ascends stone stairs in a public transit station, a clear, continuous monologue is heard. The user says: “我并不是在抱怨高铁的不便……12306重点旅客服务为我提供了巨大的帮助……让我在旅途中感受到了温暖关怀。”",
    "question_type": "Inferential & Contextual Causality",
    "question": "Based on the narration, why is the user delivering this monologue while ascending the stairs?",
    "answer": "To share a positive travel experience and express appreciation for the high-speed rail’s 12306 priority passenger service that helped them.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00936.mp4",
    "question_id": "00936_1",
    "clip_path": "clips/00936/00936__0000000_0016200.mp4"
  },
  {
    "timestamp": "00:00 - 00:16",
    "context": "[00:00 - 00:16] A clear, continuous monologue is heard; the caption states, “The user states: …” and then provides the user’s spoken words.",
    "question_type": "Sound Source Identification",
    "question": "What is the source of the clear, continuous monologue heard during 00:00–00:16?",
    "answer": "The user speaking.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00936.mp4",
    "question_id": "00936_2",
    "clip_path": "clips/00936/00936__0000000_0016200.mp4"
  },
  {
    "timestamp": "00:00 - 00:16",
    "context": "[00:00 - 00:16] The monologue is described as “a clear, continuous monologue.”",
    "question_type": "Sound Characteristics",
    "question": "How is the monologue’s acoustic quality described?",
    "answer": "It is clear and continuous.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00936.mp4",
    "question_id": "00936_3",
    "clip_path": "clips/00936/00936__0000000_0016200.mp4"
  },
  {
    "timestamp": "00:00 - 00:16",
    "context": "[00:00 - 00:16] The monologue spans the entire noted interval and is described as continuous.",
    "question_type": "Temporal Information",
    "question": "When does the monologue occur, and is it sustained or brief?",
    "answer": "It occurs from 00:00 to 00:16 and is sustained (continuous) throughout.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00936.mp4",
    "question_id": "00936_4",
    "clip_path": "clips/00936/00936__0000000_0016200.mp4"
  },
  {
    "timestamp": "00:06 - 00:10",
    "context": "[00:06 - 00:10] As the guide walks through the metal detector, she says, '稍等一下, 这边行李箱过一下安检' ('Wait a moment, the luggage needs to go through the security check here') to direct the camera holder to pause.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the guide tell the camera holder to 'wait a moment' at 00:06–00:10?",
    "answer": "Because the luggage needed to go through the security check, so she was directing them to pause.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00940.mp4",
    "question_id": "00940_1",
    "clip_path": "clips/00940/00940__0005500_0010500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "[00:00 - 00:06] The guide says she is not clear on how long it will take and notes she hasn't taken this route before.",
    "question_type": "Inferential & Contextual Causality",
    "question": "What was the likely reason the guide said she wasn't clear about how long it would take?",
    "answer": "She hadn't taken that route before, so she didn't know the timing.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00940.mp4",
    "question_id": "00940_2",
    "clip_path": "clips/00940/00940__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "[00:00 - 00:06] A woman, acting as a guide, speaks directly to the camera holder.",
    "question_type": "Sound Source Identification",
    "question": "Who is the source of the clear, mid-volume voice heard at the start?",
    "answer": "The woman acting as a guide.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00940.mp4",
    "question_id": "00940_3",
    "clip_path": "clips/00940/00940__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "[00:00 - 00:06] Her clear, mid-volume voice comes from directly in front.",
    "question_type": "Sound Characteristics",
    "question": "What are the clarity and volume characteristics of the guide's voice at 00:00–00:06?",
    "answer": "It is clear and mid-volume.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00940.mp4",
    "question_id": "00940_4",
    "clip_path": "clips/00940/00940__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "[00:00 - 00:06] The guide's voice is described as coming from directly in front, approximately 1–2 meters away.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where relative to the camera does the guide's voice originate at the start?",
    "answer": "Directly in front, about 1–2 meters away.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00940.mp4",
    "question_id": "00940_5",
    "clip_path": "clips/00940/00940__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:06 - 00:10",
    "context": "[00:06 - 00:10] As she passes through the metal detector, she continues to instruct the camera holder; her voice remains clear.",
    "question_type": "Temporal Information",
    "question": "Does the guide continue speaking after passing through the metal detector, and during what interval?",
    "answer": "Yes; she continues instructing from 00:06 to 00:10.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00940.mp4",
    "question_id": "00940_6",
    "clip_path": "clips/00940/00940__0005500_0010500.mp4"
  },
  {
    "timestamp": "00:00 - 00:10",
    "context": "[00:00 - 00:06] She says one quoted sentence. [00:06 - 00:10] She says another quoted sentence.",
    "question_type": "Counting",
    "question": "How many distinct quoted sentences does the guide say in the clip?",
    "answer": "Two.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00940.mp4",
    "question_id": "00940_7",
    "clip_path": "clips/00940/00940__0000000_0010500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "[00:00 - 00:06] As she speaks, she places her pink handbag onto the conveyor belt of the X-ray scanner.",
    "question_type": "Cross-Modal Reasoning",
    "question": "While the guide is speaking in the first segment, what concurrent visual action is she performing?",
    "answer": "She is placing her pink handbag onto the X-ray scanner's conveyor belt.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00940.mp4",
    "question_id": "00940_8",
    "clip_path": "clips/00940/00940__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:21",
    "context": "The user walks through a subway corridor while a clear, male narrator delivers a monologue. The audio consists of the narration and the faint, echoing ambiance of the large public space.",
    "question_type": "Temporal Information",
    "question": "During which time interval does the male narrator's monologue play?",
    "answer": "From 00:00 to 00:21.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00941.mp4",
    "question_id": "00941_1",
    "clip_path": "clips/00941/00941__0000000_0021500.mp4"
  },
  {
    "timestamp": "00:00 - 00:21",
    "context": "The audio consists of the narration and the faint, echoing ambiance of the large public space.",
    "question_type": "Sound Characteristics",
    "question": "How is the subway station’s ambient sound described during the narration?",
    "answer": "As a faint, echoing ambiance of a large public space.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00941.mp4",
    "question_id": "00941_2",
    "clip_path": "clips/00941/00941__0000000_0021500.mp4"
  },
  {
    "timestamp": "00:00 - 00:21",
    "context": "A clear, male narrator delivers a monologue expressing gratitude and hopes about kindness.",
    "question_type": "Sound Source Identification",
    "question": "What is the source of the spoken audio heard at the start?",
    "answer": "A clear, male narrator delivering a monologue.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00941.mp4",
    "question_id": "00941_3",
    "clip_path": "clips/00941/00941__0000000_0021500.mp4"
  },
  {
    "timestamp": "00:21 - 00:58",
    "context": "The user navigates along a long, white wall on their left with a white cane. The narrator explains preferring the edge for safety and familiarity, using roadside objects as reference landmarks and relying on perception and memory.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why does the user choose to walk along the edge near the wall instead of the middle?",
    "answer": "Because it feels safer and more familiar for a visually impaired person, allowing the use of nearby objects as reference landmarks and relying on perception and memory for direction.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00941.mp4",
    "question_id": "00941_4",
    "clip_path": "clips/00941/00941__0020500_0058500.mp4"
  },
  {
    "timestamp": "00:21 - 00:58",
    "context": "A continuous series of sharp, rhythmic tapping sounds accompanies walking as the cane's tip strikes the tiled floor, providing auditory feedback.",
    "question_type": "Sound Source Identification",
    "question": "What generates the sharp, rhythmic tapping sounds in this segment?",
    "answer": "The white cane’s tip striking the tiled floor.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00941.mp4",
    "question_id": "00941_5",
    "clip_path": "clips/00941/00941__0020500_0058500.mp4"
  },
  {
    "timestamp": "00:40",
    "context": "At 00:40, the cane makes a distinct, hollow thud as it strikes a metal trash can by the wall.",
    "question_type": "Sound Characteristics",
    "question": "What is the quality of the sound produced at 00:40?",
    "answer": "A distinct, hollow thud.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00941.mp4",
    "question_id": "00941_6",
    "clip_path": "clips/00941/00941__0039500_0043500.mp4"
  },
  {
    "timestamp": "00:40",
    "context": "At 00:40, the cane makes a distinct, hollow thud as it strikes a metal trash can by the wall.",
    "question_type": "Sound Source Identification",
    "question": "What object did the cane strike to produce the sound at 00:40?",
    "answer": "A metal trash can by the wall.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00941.mp4",
    "question_id": "00941_7",
    "clip_path": "clips/00941/00941__0039500_0043500.mp4"
  },
  {
    "timestamp": "00:40",
    "context": "The cane hits a metal trash can by the wall, demonstrating the use of objects as navigational landmarks.",
    "question_type": "Cross-Modal Reasoning",
    "question": "After the hollow thud at 00:40, what does that contact indicate about how the user navigates?",
    "answer": "It shows the user uses objects like the metal trash can as landmarks to orient themselves.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00941.mp4",
    "question_id": "00941_8",
    "clip_path": "clips/00941/00941__0039500_0043500.mp4"
  },
  {
    "timestamp": "00:58 - 01:09",
    "context": "After the monologue concludes, the user continues walking. The primary sound is the consistent and rhythmic tapping of the white cane on the floor.",
    "question_type": "Temporal Information",
    "question": "Once the narration ends, what becomes the primary sound between 00:58 and 01:09?",
    "answer": "The consistent, rhythmic tapping of the white cane on the floor.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00941.mp4",
    "question_id": "00941_9",
    "clip_path": "clips/00941/00941__0057500_0069500.mp4"
  },
  {
    "timestamp": "00:58 - 01:09",
    "context": "Periodically, a louder, sharper 'clack' is produced as the cane's tip makes contact with small fixtures and seams along the wall's base.",
    "question_type": "Sound Characteristics",
    "question": "How is the periodic 'clack' described in this interval, and what causes it?",
    "answer": "It is a louder, sharper 'clack' caused by the cane tip hitting small fixtures and seams along the wall’s base.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00941.mp4",
    "question_id": "00941_10",
    "clip_path": "clips/00941/00941__0057500_0069500.mp4"
  },
  {
    "timestamp": "00:00 - 00:21",
    "context": "The audio consists of a narration and the faint, echoing ambiance of the large public space.",
    "question_type": "Counting",
    "question": "How many distinct sound types are heard during 00:00–00:21, and what are they?",
    "answer": "Two: the narrator’s monologue and the faint, echoing station ambiance.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00941.mp4",
    "question_id": "00941_11",
    "clip_path": "clips/00941/00941__0000000_0021500.mp4"
  },
  {
    "timestamp": "00:58 - 01:09",
    "context": "Other people can be seen walking in the distance, but their sounds are indistinct.",
    "question_type": "Cross-Modal Reasoning",
    "question": "When people are seen walking in the distance during 00:58–01:09, how do they register in the audio?",
    "answer": "Their sounds are present but indistinct.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00941.mp4",
    "question_id": "00941_12",
    "clip_path": "clips/00941/00941__0057500_0069500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "[00:00 - 00:07] The camera user descends an escalator. The escalator produces a continuous, low-volume mechanical hum. A distant, echoing female PA announcement is audible.",
    "question_type": "Sound Characteristics",
    "question": "What are the acoustic quality and volume of the escalator's sound at the start?",
    "answer": "A continuous, low-volume mechanical hum.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00942.mp4",
    "question_id": "00942_1",
    "clip_path": "clips/00942/00942__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:07 - 00:11",
    "context": "[00:07 - 00:11] As they reach the bottom of the escalator, the woman on the right says '到了' ('Arrived'). The escalator's hum abruptly stops and is replaced by quiet shuffling of feet on the station floor.",
    "question_type": "Temporal Information",
    "question": "When does the escalator's hum stop?",
    "answer": "It stops abruptly as they reach the bottom, around 00:07–00:11.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00942.mp4",
    "question_id": "00942_2",
    "clip_path": "clips/00942/00942__0006500_0011500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "[00:00 - 00:07] A distant and echoing female voice from a public address system makes an announcement.",
    "question_type": "Sound Source Identification",
    "question": "What generated the distant, echoing female voice heard early in the scene?",
    "answer": "A public address (PA) system in the station.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00942.mp4",
    "question_id": "00942_3",
    "clip_path": "clips/00942/00942__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:07 - 00:11",
    "context": "[00:07 - 00:11] The woman on the right speaks in a clear, nearby voice, saying '到了' ('Arrived').",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From which side and at what proximity did the 'Arrived' utterance originate?",
    "answer": "From the right side, in a clear, nearby voice.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00942.mp4",
    "question_id": "00942_4",
    "clip_path": "clips/00942/00942__0006500_0011500.mp4"
  },
  {
    "timestamp": "00:07 - 00:11",
    "context": "[00:07 - 00:11] The woman on the right says '到了' ('Arrived') as they reach the bottom of the escalator.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the woman say 'Arrived' at this moment?",
    "answer": "Because they had reached the bottom of the escalator, indicating arrival at that level.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00942.mp4",
    "question_id": "00942_5",
    "clip_path": "clips/00942/00942__0006500_0011500.mp4"
  },
  {
    "timestamp": "00:11 - 00:14",
    "context": "[00:11 - 00:14] The camera user asks, '现在就是地铁口了吗?' ('Is this the subway entrance now?'). The woman does not reply and continues walking away. Footsteps and echoes are heard in the tiled passageway.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the camera user ask if this was the subway entrance?",
    "answer": "To confirm their location.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00942.mp4",
    "question_id": "00942_6",
    "clip_path": "clips/00942/00942__0010500_0014500.mp4"
  },
  {
    "timestamp": "00:14 - 00:15",
    "context": "[00:14 - 00:15] A brief, high-pitched cry from a child is heard from the front-left, coming from a man carrying a child who is walking away from the camera.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where relative to the camera did the child's cry originate?",
    "answer": "From the front-left, moving away from the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00942.mp4",
    "question_id": "00942_7",
    "clip_path": "clips/00942/00942__0013500_0015500.mp4"
  },
  {
    "timestamp": "00:14 - 00:15",
    "context": "[00:14 - 00:15] A brief, high-pitched cry from a child is heard.",
    "question_type": "Sound Characteristics",
    "question": "What are the characteristics of the child's cry?",
    "answer": "It is brief and high-pitched.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00942.mp4",
    "question_id": "00942_8",
    "clip_path": "clips/00942/00942__0013500_0015500.mp4"
  },
  {
    "timestamp": "00:14 - 00:15",
    "context": "[00:14 - 00:15] A child cries from the front-left; visually, a man carrying a child is walking away from the camera.",
    "question_type": "Cross-Modal Reasoning",
    "question": "When the child’s cry is heard, who is most likely associated with it visually and how are they moving?",
    "answer": "The child being carried by a man who is walking away from the camera toward the front-left.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00942.mp4",
    "question_id": "00942_9",
    "clip_path": "clips/00942/00942__0013500_0015500.mp4"
  },
  {
    "timestamp": "00:00 - 00:11",
    "context": "[00:00 - 00:07] A distant female PA announcement is heard. [00:07 - 00:11] A nearby woman on the right says 'Arrived.'",
    "question_type": "Counting",
    "question": "How many distinct human voices are heard between 00:00 and 00:11?",
    "answer": "Two: a distant female PA announcer and the nearby guiding woman.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00942.mp4",
    "question_id": "00942_10",
    "clip_path": "clips/00942/00942__0000000_0011500.mp4"
  },
  {
    "timestamp": "00:24 - 00:36",
    "context": "[00:24 - 00:36] As the user continues walking with the white cane tip visible, a calm male voiceover begins, explaining the user's motivation.",
    "question_type": "Temporal Information",
    "question": "When does the calm male voiceover begin?",
    "answer": "At 00:24.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00942.mp4",
    "question_id": "00942_11",
    "clip_path": "clips/00942/00942__0023500_0036500.mp4"
  },
  {
    "timestamp": "00:24 - 00:36",
    "context": "[00:24 - 00:36] The white cane tip is visible while faint, echoing ambient subway sounds continue and a calm male voiceover explains the journey.",
    "question_type": "Cross-Modal Reasoning",
    "question": "When the white cane tip is visible, what audio context accompanies it?",
    "answer": "A calm male voiceover with faint, echoing ambient subway sounds.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00942.mp4",
    "question_id": "00942_12",
    "clip_path": "clips/00942/00942__0023500_0036500.mp4"
  },
  {
    "timestamp": "00:00 - 00:08",
    "context": "As the user walks, the rhythmic, moderate-volume sound of their footsteps on a paved surface is clearly audible from directly below the camera.",
    "question_type": "Sound Characteristics",
    "question": "What are the volume and rhythmic qualities of the user's footsteps at the start?",
    "answer": "They are rhythmic and moderate-volume.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00943.mp4",
    "question_id": "00943_1",
    "clip_path": "clips/00943/00943__0000000_0008500.mp4"
  },
  {
    "timestamp": "00:00 - 00:08",
    "context": "As the user walks, the rhythmic, moderate-volume sound of their footsteps on a paved surface is clearly audible from directly below the camera.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where were the footsteps located relative to the camera?",
    "answer": "Directly below the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00943.mp4",
    "question_id": "00943_2",
    "clip_path": "clips/00943/00943__0000000_0008500.mp4"
  },
  {
    "timestamp": "00:08 - 00:14",
    "context": "A woman's voice, originating from nearby and slightly to the front, warns, \"你要坐电梯吗？这是扶梯噢，小心\" (Are you taking the elevator? This is an escalator, be careful). Her speech serves as a warning, suggesting she noticed the user's white cane and is concerned for their safety on the escalator.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the nearby woman warn, \"This is an escalator, be careful\"?",
    "answer": "Because she likely noticed the user's white cane and was concerned for the user's safety on the escalator.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00943.mp4",
    "question_id": "00943_3",
    "clip_path": "clips/00943/00943__0007500_0014500.mp4"
  },
  {
    "timestamp": "00:08 - 00:14",
    "context": "A woman's voice, originating from nearby and slightly to the front, warns the user about the escalator.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From what relative direction did the woman's warning originate?",
    "answer": "From nearby and slightly to the front of the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00943.mp4",
    "question_id": "00943_4",
    "clip_path": "clips/00943/00943__0007500_0014500.mp4"
  },
  {
    "timestamp": "00:15 - 00:27",
    "context": "The user politely declines help in a series of short exchanges. The user confirms, \"可以可以可以，谢谢\" (Yes, yes, I can. Thank you).",
    "question_type": "Counting",
    "question": "In the user's final reply during this exchange, how many times did they repeat the word \"可以\" consecutively?",
    "answer": "Three times.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00943.mp4",
    "question_id": "00943_5",
    "clip_path": "clips/00943/00943__0014500_0027500.mp4"
  },
  {
    "timestamp": "00:27 - 00:33",
    "context": "As the user moves towards the escalator, a series of sharp, metallic tapping sounds are produced by their white cane striking the metal floor plate and railings at the escalator entrance.",
    "question_type": "Sound Source Identification",
    "question": "What produced the sharp, metallic tapping sounds at the escalator entrance?",
    "answer": "The user's white cane striking the metal floor plate and railings.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00943.mp4",
    "question_id": "00943_6",
    "clip_path": "clips/00943/00943__0026500_0033500.mp4"
  },
  {
    "timestamp": "00:27 - 00:33",
    "context": "The woman's concerned voice is heard one last time as the user insists, \"不用不用，我可以自己\" (No need, no need, I can do it myself).",
    "question_type": "Counting",
    "question": "In the user's insistence \"不用不用，我可以自己\", how many times is \"不用\" repeated?",
    "answer": "Twice.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00943.mp4",
    "question_id": "00943_7",
    "clip_path": "clips/00943/00943__0026500_0033500.mp4"
  },
  {
    "timestamp": "00:33 - 00:51",
    "context": "The user steps onto the escalator. A continuous, low-frequency mechanical hum and a rhythmic clanking sound begin, indicating the escalator is in motion and descending.",
    "question_type": "Cross-Modal Reasoning",
    "question": "When the continuous low-frequency hum and rhythmic clanking begin, what do they indicate is happening visually?",
    "answer": "The escalator is moving and descending with the user on it.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00943.mp4",
    "question_id": "00943_8",
    "clip_path": "clips/00943/00943__0032500_0051500.mp4"
  },
  {
    "timestamp": "00:33 - 00:51",
    "context": "Shortly after, a woman's voice from the right warns with concern, \"这里有点危险，下也有点危险\" (It's a bit dangerous here, and getting off is also a bit dangerous).",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From which side relative to the camera does the woman's escalator warning originate?",
    "answer": "From the right.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00943.mp4",
    "question_id": "00943_9",
    "clip_path": "clips/00943/00943__0032500_0051500.mp4"
  },
  {
    "timestamp": "00:33 - 01:07",
    "context": "A continuous, low-frequency mechanical hum begins as the user steps onto the escalator and continues while descending, accompanied by its steady mechanical hum.",
    "question_type": "Temporal Information",
    "question": "Is the escalator’s mechanical hum steady or fluctuating during the descent from 00:33 to 01:07?",
    "answer": "It is steady and continuous.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00943.mp4",
    "question_id": "00943_10",
    "clip_path": "clips/00943/00943__0032500_0067500.mp4"
  },
  {
    "timestamp": "00:33 - 00:51",
    "context": "A continuous, low-frequency mechanical hum and a rhythmic clanking sound begin as the escalator moves.",
    "question_type": "Counting",
    "question": "Excluding speech, how many distinct escalator-related sounds are simultaneously present as the user descends?",
    "answer": "Two: a continuous low-frequency mechanical hum and a rhythmic clanking sound.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00943.mp4",
    "question_id": "00943_11",
    "clip_path": "clips/00943/00943__0032500_0051500.mp4"
  },
  {
    "timestamp": "00:33 - 00:51",
    "context": "After warning that it is dangerous, the woman says to a companion, \"等我一下，我扶一下这个美女\" (Wait for me, I'll help this lady), making her intention to assist explicit.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the woman say, \"等我一下，我扶一下这个美女\" after warning about danger?",
    "answer": "Because she perceived the situation as dangerous and intended to help the user.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00943.mp4",
    "question_id": "00943_12",
    "clip_path": "clips/00943/00943__0032500_0051500.mp4"
  },
  {
    "timestamp": "00:58 - 01:07",
    "context": "While descending on the escalator, another woman appears on the step to the user's right and asks, \"你眼睛看不到吗?\" (Can't you see with your eyes?).",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where is the second speaking woman located relative to the user when she speaks?",
    "answer": "On the step to the user's right.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00943.mp4",
    "question_id": "00943_13",
    "clip_path": "clips/00943/00943__0057500_0067500.mp4"
  },
  {
    "timestamp": "00:58 - 01:07",
    "context": "Another woman appears on the step to the user's right and asks in a direct but caring tone, \"你眼睛看不到吗?\"",
    "question_type": "Sound Characteristics",
    "question": "What was the tone of the second woman's question, \"你眼睛看不到吗?\"?",
    "answer": "Direct but caring.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00943.mp4",
    "question_id": "00943_14",
    "clip_path": "clips/00943/00943__0057500_0067500.mp4"
  },
  {
    "timestamp": "00:58 - 01:07",
    "context": "She then states, \"我帮你扶着电梯了，你不用动\" (I'm holding the escalator for you, you don't need to move), while the descent is accompanied by its steady mechanical hum.",
    "question_type": "Cross-Modal Reasoning",
    "question": "When the woman says she is holding the escalator handrail for the user, what ongoing sound accompanies the scene?",
    "answer": "The steady mechanical hum of the moving escalator.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00943.mp4",
    "question_id": "00943_15",
    "clip_path": "clips/00943/00943__0057500_0067500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "A continuous, low-rumbling sound of suitcase wheels rolling on the smooth tile floor, originating from a man walking directly in front of the camera.",
    "question_type": "Sound Source Identification",
    "question": "What generated the continuous, low-rumbling rolling sound at the beginning of the video?",
    "answer": "The suitcase wheels of a man walking directly in front of the camera on the smooth tile floor.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00945.mp4",
    "question_id": "00945_1",
    "clip_path": "clips/00945/00945__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "Originating from a man walking directly in front of the camera.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the rolling suitcase sound originate relative to the camera?",
    "answer": "Directly in front of the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00945.mp4",
    "question_id": "00945_2",
    "clip_path": "clips/00945/00945__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "Continuous, low-rumbling sound of suitcase wheels rolling on the smooth tile floor.",
    "question_type": "Sound Characteristics",
    "question": "How is the acoustic quality of the suitcase wheels' sound described at the start?",
    "answer": "It is a continuous, low-rumbling rolling sound on a smooth tile floor.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00945.mp4",
    "question_id": "00945_3",
    "clip_path": "clips/00945/00945__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "The scene takes place in a large, echoing public terminal, filled with the faint, distant chatter of other people.",
    "question_type": "Sound Characteristics",
    "question": "What is the volume and perceived distance of the background chatter in the terminal?",
    "answer": "The chatter is faint and distant within the large, echoing terminal.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00945.mp4",
    "question_id": "00945_4",
    "clip_path": "clips/00945/00945__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "As the video begins, a male voice from the camera's perspective says in a clear, conversational tone: \"最后一棒接力啦\" (The last leg of the relay).",
    "question_type": "Sound Characteristics",
    "question": "What was the tone of the male voice speaking at the beginning?",
    "answer": "Clear and conversational.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00945.mp4",
    "question_id": "00945_5",
    "clip_path": "clips/00945/00945__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:03 - 00:08",
    "context": "The camera holder lifts a white cane and begins to walk, introducing a new, prominent sound: the rhythmic, scraping tap of the cane's tip sweeping across the floor, used for navigation.",
    "question_type": "Sound Source Identification",
    "question": "What action produced the rhythmic, scraping tap introduced after 00:03?",
    "answer": "The white cane's tip sweeping back and forth across the floor as the camera holder walked.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00945.mp4",
    "question_id": "00945_6",
    "clip_path": "clips/00945/00945__0002500_0008500.mp4"
  },
  {
    "timestamp": "00:03 - 00:08",
    "context": "A new, prominent sound: the rhythmic, scraping tap of the cane's tip sweeping back and forth across the floor. The sound is very close to the microphone.",
    "question_type": "Sound Characteristics",
    "question": "What are the rhythm and texture of the cane sound after 00:03?",
    "answer": "A prominent, rhythmic, scraping tap that is very close to the microphone.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00945.mp4",
    "question_id": "00945_7",
    "clip_path": "clips/00945/00945__0002500_0008500.mp4"
  },
  {
    "timestamp": "00:03 - 00:08",
    "context": "The sound is very close to the microphone and synchronized with the movement of the cane.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where is the cane tapping sound located relative to the microphone?",
    "answer": "Very close to the microphone.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00945.mp4",
    "question_id": "00945_8",
    "clip_path": "clips/00945/00945__0002500_0008500.mp4"
  },
  {
    "timestamp": "00:03 - 00:08",
    "context": "This action introduces a new, prominent sound: the rhythmic, scraping tap of the cane's tip... used for navigation. The user follows a tactile paving path toward gate 7.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the cane tapping sound begin at 00:03?",
    "answer": "Because the camera holder lifted the white cane and started walking, using it for navigation along the tactile paving path.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00945.mp4",
    "question_id": "00945_9",
    "clip_path": "clips/00945/00945__0002500_0008500.mp4"
  },
  {
    "timestamp": "00:03 - 00:08",
    "context": "The sound of the rolling suitcase from the men ahead gradually becomes more distant.",
    "question_type": "Temporal Information",
    "question": "What volume change occurred to the rolling suitcase sound between 00:03 and 00:08?",
    "answer": "Its volume decreased as it gradually became more distant.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00945.mp4",
    "question_id": "00945_10",
    "clip_path": "clips/00945/00945__0002500_0008500.mp4"
  },
  {
    "timestamp": "00:03 - 00:08",
    "context": "This action introduces a new, prominent sound: the rhythmic, scraping tap of the cane's tip.",
    "question_type": "Counting",
    "question": "How many new prominent sounds were introduced when the camera holder began to walk at 00:03?",
    "answer": "One—the rhythmic, scraping tap of the cane's tip.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00945.mp4",
    "question_id": "00945_11",
    "clip_path": "clips/00945/00945__0002500_0008500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "Continuous suitcase wheel rumble and faint, distant chatter of other people; excluding the spoken line by the male voice.",
    "question_type": "Counting",
    "question": "Excluding speech, how many types of environmental sounds are heard at the start?",
    "answer": "Two: the rolling suitcase wheels and the faint, distant chatter.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00945.mp4",
    "question_id": "00945_12",
    "clip_path": "clips/00945/00945__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:00 - 00:08",
    "context": "[00:00 - 00:08] A female voice from the camera's position says, “要稍等一下哦” (“Please wait a moment”) and “她去坐高铁” (“She's going to take the high-speed rail”) as a male station staff member walks toward the camera from ~10 meters away.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Based on the context, why did the female voice likely say “Please wait a moment” at the start?",
    "answer": "She was asking someone to wait while the approaching station staff member came over to facilitate the upcoming handoff.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00946.mp4",
    "question_id": "00946_1",
    "clip_path": "clips/00946/00946__0000000_0008500.mp4"
  },
  {
    "timestamp": "00:00 - 00:08",
    "context": "[00:00 - 00:08] A female voice originating from the camera's position speaks at a conversational volume.",
    "question_type": "Sound Source Identification",
    "question": "What produced the spoken statements at the start of the video?",
    "answer": "A female voice originating from the camera's position.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00946.mp4",
    "question_id": "00946_2",
    "clip_path": "clips/00946/00946__0000000_0008500.mp4"
  },
  {
    "timestamp": "00:00 - 00:08",
    "context": "[00:00 - 00:08] The female voice speaks at a conversational volume.",
    "question_type": "Sound Characteristics",
    "question": "What was the volume level of the female voice in this segment?",
    "answer": "Conversational volume.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00946.mp4",
    "question_id": "00946_3",
    "clip_path": "clips/00946/00946__0000000_0008500.mp4"
  },
  {
    "timestamp": "00:00 - 00:08",
    "context": "[00:00 - 00:08] The speech originates from the camera's position.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the female speech originate relative to the camera?",
    "answer": "From the camera's position (right at the camera).",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00946.mp4",
    "question_id": "00946_4",
    "clip_path": "clips/00946/00946__0000000_0008500.mp4"
  },
  {
    "timestamp": "00:00 - 00:08",
    "context": "[00:00 - 00:08] The female says, “要稍等一下哦” and “她去坐高铁.”",
    "question_type": "Counting",
    "question": "How many distinct phrases did the female voice utter in this segment?",
    "answer": "Two: “Please wait a moment” and “She's going to take the high-speed rail.”",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00946.mp4",
    "question_id": "00946_5",
    "clip_path": "clips/00946/00946__0000000_0008500.mp4"
  },
  {
    "timestamp": "00:09 - 00:18",
    "context": "[00:09 - 00:18] The staff member takes a small white box and a red paper bag; the transfer includes a brief, muffled verbal exchange and the soft rustling sound of the paper bag.",
    "question_type": "Sound Source Identification",
    "question": "What generated the soft rustling sound during the transfer?",
    "answer": "The red paper bag being handled during the handoff.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00946.mp4",
    "question_id": "00946_6",
    "clip_path": "clips/00946/00946__0008500_0018500.mp4"
  },
  {
    "timestamp": "00:09 - 00:18",
    "context": "[00:09 - 00:18] The transfer is accompanied by a brief, muffled verbal exchange.",
    "question_type": "Sound Characteristics",
    "question": "What were the qualities and duration of the verbal exchange during the handoff?",
    "answer": "It was brief and muffled.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00946.mp4",
    "question_id": "00946_7",
    "clip_path": "clips/00946/00946__0008500_0018500.mp4"
  },
  {
    "timestamp": "00:09 - 00:18",
    "context": "[00:09 - 00:18] The transfer includes a brief, muffled verbal exchange and soft rustling of the paper bag.",
    "question_type": "Counting",
    "question": "During the handoff, how many distinct sound types were noted?",
    "answer": "Two: a brief, muffled verbal exchange and the soft rustling of the paper bag.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00946.mp4",
    "question_id": "00946_8",
    "clip_path": "clips/00946/00946__0008500_0018500.mp4"
  },
  {
    "timestamp": "00:18 - 00:23",
    "context": "[00:18 - 00:23] A loud, continuous, high-pitched beeping, signaling train activity, begins to chime throughout the station.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the loud beeping chime throughout the station?",
    "answer": "It was signaling train activity.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00946.mp4",
    "question_id": "00946_9",
    "clip_path": "clips/00946/00946__0017500_0023290.mp4"
  },
  {
    "timestamp": "00:18 - 00:23",
    "context": "[00:18 - 00:23] A loud, continuous, high-pitched beeping is heard.",
    "question_type": "Sound Characteristics",
    "question": "How is the beeping sound described in this interval?",
    "answer": "Loud, continuous, and high-pitched.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00946.mp4",
    "question_id": "00946_10",
    "clip_path": "clips/00946/00946__0017500_0023290.mp4"
  },
  {
    "timestamp": "00:18 - 00:23",
    "context": "[00:18 - 00:23] The beeping begins to chime throughout the station.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the beeping appear to originate relative to the camera?",
    "answer": "It was station-wide, chiming throughout the station (ambient around the camera).",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00946.mp4",
    "question_id": "00946_11",
    "clip_path": "clips/00946/00946__0017500_0023290.mp4"
  },
  {
    "timestamp": "00:18 - 00:23",
    "context": "[00:18 - 00:23] The beeping begins and continues as the camera follows the staff member.",
    "question_type": "Temporal Information",
    "question": "When did the beeping start and what was its temporal behavior during this segment?",
    "answer": "It began around 00:18 and continued continuously through the segment.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00946.mp4",
    "question_id": "00946_12",
    "clip_path": "clips/00946/00946__0017500_0023290.mp4"
  },
  {
    "timestamp": "00:18 - 00:23",
    "context": "[00:18 - 00:23] Over the beeping, a female voice from the camera's location announces, “最后一棒接力啦.”",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the announcement “The last leg of the relay!” originate relative to the camera?",
    "answer": "From the camera's location.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00946.mp4",
    "question_id": "00946_13",
    "clip_path": "clips/00946/00946__0017500_0023290.mp4"
  },
  {
    "timestamp": "00:18 - 00:23",
    "context": "[00:18 - 00:23] The female announcement occurs over the station-wide beeping.",
    "question_type": "Counting",
    "question": "How many distinct sound elements are simultaneously present when the announcement is made?",
    "answer": "Two: the station-wide beeping and the female announcement.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00946.mp4",
    "question_id": "00946_14",
    "clip_path": "clips/00946/00946__0017500_0023290.mp4"
  },
  {
    "timestamp": "00:18 - 00:23",
    "context": "[00:18 - 00:23] After the handoff, the staff member walks down the platform with the items while the female says, “最后一棒接力啦.”",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the speaker say “The last leg of the relay!” at this moment?",
    "answer": "To indicate the final stage of the handoff as the staff member carried the items down the platform.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00946.mp4",
    "question_id": "00946_15",
    "clip_path": "clips/00946/00946__0017500_0023290.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "A moderate-volume, intermittent beeping sound emanates from the train's open doors in front, signaling it is safe to board.",
    "question_type": "Sound Source Identification",
    "question": "What generated the beeping sound during 00:00 - 00:06?",
    "answer": "The train's open doors.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00949.mp4",
    "question_id": "00949_1",
    "clip_path": "clips/00949/00949__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "A moderate-volume, intermittent beeping sound emanates from the train's open doors in front.",
    "question_type": "Sound Characteristics",
    "question": "What were the volume and pattern of the beeping at the open doors?",
    "answer": "It was moderate in volume and intermittent.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00949.mp4",
    "question_id": "00949_2",
    "clip_path": "clips/00949/00949__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "The beeping sound emanates from the train's open doors in front.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the beeping come from relative to the camera?",
    "answer": "From directly in front, at the train's open doors.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00949.mp4",
    "question_id": "00949_3",
    "clip_path": "clips/00949/00949__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "A male volunteer, positioned directly in front of the user (approx. 0.5m), guides them verbally.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "What was the location (direction and distance) of the guiding volunteer's voice?",
    "answer": "Directly in front, approximately 0.5 meters from the user.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00949.mp4",
    "question_id": "00949_4",
    "clip_path": "clips/00949/00949__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "The intermittent beeping from the train's open doors is described as signaling it is safe to board.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why was the beeping sounding at the train's doors?",
    "answer": "To signal that it was safe to board.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00949.mp4",
    "question_id": "00949_5",
    "clip_path": "clips/00949/00949__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:07 - 00:11",
    "context": "While guiding the user into the subway car, the man warns from the front, \"跨一下跨一下, 小心脚下\" to ensure the user safely navigates the gap.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the man say \"跨一下跨一下, 小心脚下\" while guiding the user?",
    "answer": "To ensure the user safely stepped over the gap between the platform and the train.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00949.mp4",
    "question_id": "00949_6",
    "clip_path": "clips/00949/00949__0006500_0011500.mp4"
  },
  {
    "timestamp": "00:07 - 00:11",
    "context": "The ambient sound shifts from the open platform to the more enclosed humming and muted chatter inside the train car.",
    "question_type": "Sound Characteristics",
    "question": "What were the ambient sound characteristics inside the train car during 00:07 - 00:11?",
    "answer": "An enclosed humming with muted chatter.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00949.mp4",
    "question_id": "00949_7",
    "clip_path": "clips/00949/00949__0006500_0011500.mp4"
  },
  {
    "timestamp": "00:12 - 00:16",
    "context": "Inside the crowded train, the man raises his voice to ask, \"有没有乘客给这位乘客让个座呀? 谢谢\"",
    "question_type": "Sound Characteristics",
    "question": "How did the man adjust his voice when asking for a seat for the user?",
    "answer": "He raised his voice to be heard by other passengers.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00949.mp4",
    "question_id": "00949_8",
    "clip_path": "clips/00949/00949__0011500_0016500.mp4"
  },
  {
    "timestamp": "00:12 - 00:16",
    "context": "He asks passengers for a seat to find a safe and stable spot for the visually impaired user.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the man ask other passengers for a seat?",
    "answer": "To find a safe and stable spot for the visually impaired user.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00949.mp4",
    "question_id": "00949_9",
    "clip_path": "clips/00949/00949__0011500_0016500.mp4"
  },
  {
    "timestamp": "00:17 - 00:22",
    "context": "The man resumes guiding and speaks in a lower, more direct tone from in front: \"来背后后面哈, 小心一点\"",
    "question_type": "Sound Characteristics",
    "question": "What was the tone of the man's guidance during 00:17 - 00:22?",
    "answer": "Lower and more direct.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00949.mp4",
    "question_id": "00949_10",
    "clip_path": "clips/00949/00949__0016500_0022500.mp4"
  },
  {
    "timestamp": "00:17 - 00:22",
    "context": "He continues guiding from in front while moving through the crowded aisle.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From where relative to the user did the instructions during 00:17 - 00:22 originate?",
    "answer": "From in front of the user.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00949.mp4",
    "question_id": "00949_11",
    "clip_path": "clips/00949/00949__0016500_0022500.mp4"
  },
  {
    "timestamp": "00:23 - 00:30",
    "context": "The man hands over a red shopping bag; a soft rustling sound is produced as the user's gloved hands take the bag.",
    "question_type": "Sound Source Identification",
    "question": "What caused the soft rustling sound during 00:23 - 00:30?",
    "answer": "The user's gloved hands taking the red shopping bag.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00949.mp4",
    "question_id": "00949_12",
    "clip_path": "clips/00949/00949__0022500_0030500.mp4"
  },
  {
    "timestamp": "00:23 - 00:30",
    "context": "The man replies affirmatively, \"对对对对对\"",
    "question_type": "Counting",
    "question": "How many times did the man say \"对\" during his affirmative reply?",
    "answer": "Five times.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00949.mp4",
    "question_id": "00949_13",
    "clip_path": "clips/00949/00949__0022500_0030500.mp4"
  },
  {
    "timestamp": "00:23 - 00:30",
    "context": "The man instructs, \"你把袋子拿好\" as he hands the bag to the user.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the man instruct \"你把袋子拿好\" during the handover?",
    "answer": "To ensure the user held the bag properly during the handover of belongings.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00949.mp4",
    "question_id": "00949_14",
    "clip_path": "clips/00949/00949__0022500_0030500.mp4"
  },
  {
    "timestamp": "00:23 - 00:30",
    "context": "A soft rustling sound is produced as the user's gloved hands take the bag.",
    "question_type": "Temporal Information",
    "question": "When was the soft rustling heard relative to the events?",
    "answer": "During 00:23 - 00:30, as the bag was handed to and taken by the user.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00949.mp4",
    "question_id": "00949_15",
    "clip_path": "clips/00949/00949__0022500_0030500.mp4"
  },
  {
    "timestamp": "00:00 - 00:02",
    "context": "[00:00 - 00:02] A clear female voice, presumably the handler, says: \"她到7号线, 成都东\" (She's going to Line 7, Chengdu East). Faint station ambient noise is in the background.",
    "question_type": "Sound Source Identification",
    "question": "Who produced the clear voice stating the destination at the beginning?",
    "answer": "A clear female voice, presumably the handler.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00951.mp4",
    "question_id": "00951_1",
    "clip_path": "clips/00951/00951__0000000_0002500.mp4"
  },
  {
    "timestamp": "00:00 - 00:02",
    "context": "[00:00 - 00:02] The surrounding ambient noise of the subway station is faint in the background while the handler speaks.",
    "question_type": "Sound Characteristics",
    "question": "What was the volume level of the station's ambient noise during the initial segment?",
    "answer": "Faint.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00951.mp4",
    "question_id": "00951_2",
    "clip_path": "clips/00951/00951__0000000_0002500.mp4"
  },
  {
    "timestamp": "00:00 - 00:02",
    "context": "[00:00 - 00:02] The handler states: \"她到7号线, 成都东\" (She's going to Line 7, Chengdu East).",
    "question_type": "Temporal Information",
    "question": "When was the destination stated by the female voice?",
    "answer": "Between 00:00 and 00:02.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00951.mp4",
    "question_id": "00951_3",
    "clip_path": "clips/00951/00951__0000000_0002500.mp4"
  },
  {
    "timestamp": "00:02 - 00:04",
    "context": "[00:02 - 00:04] A young man in a black jacket walks in from the right, gestures toward the camera, and says, \"...诶, 等我一下\" (...Hey, wait for me).",
    "question_type": "Sound Source Identification",
    "question": "Who said \"...Hey, wait for me\"?",
    "answer": "A young man in a black jacket who walked in from the right.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00951.mp4",
    "question_id": "00951_4",
    "clip_path": "clips/00951/00951__0001500_0004500.mp4"
  },
  {
    "timestamp": "00:02 - 00:04",
    "context": "[00:02 - 00:04] The young man speaks while entering from the right side of the frame and addressing the camera holder.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From which direction relative to the camera did the speech \"...Hey, wait for me\" originate?",
    "answer": "From the right side, as he walked into the frame from the right.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00951.mp4",
    "question_id": "00951_5",
    "clip_path": "clips/00951/00951__0001500_0004500.mp4"
  },
  {
    "timestamp": "00:02 - 00:04",
    "context": "[00:02 - 00:04] The young man says, \"...诶, 等我一下\" (...Hey, wait for me), suggesting a brief, direct interaction before he walks away.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the young man say \"...Hey, wait for me\"?",
    "answer": "He was directly addressing the camera holder to ask them to wait for him.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00951.mp4",
    "question_id": "00951_6",
    "clip_path": "clips/00951/00951__0001500_0004500.mp4"
  },
  {
    "timestamp": "00:04 - 00:07",
    "context": "[00:04 - 00:07] Distinct, rhythmic footsteps on tiled floor grow progressively louder as someone approaches from the front.",
    "question_type": "Sound Characteristics",
    "question": "What were the qualities and volume change of the footsteps heard between 00:04 and 00:07?",
    "answer": "They were distinct and rhythmic, growing progressively louder.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00951.mp4",
    "question_id": "00951_7",
    "clip_path": "clips/00951/00951__0003500_0007454.mp4"
  },
  {
    "timestamp": "00:04 - 00:07",
    "context": "[00:04 - 00:07] The footsteps indicate someone approaching from the front, and a male security guard comes into view walking directly toward the dog and handler.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From what direction relative to the camera did the approaching footsteps come?",
    "answer": "From the front.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00951.mp4",
    "question_id": "00951_8",
    "clip_path": "clips/00951/00951__0003500_0007454.mp4"
  },
  {
    "timestamp": "00:04 - 00:07",
    "context": "[00:04 - 00:07] A male security guard wearing a dark uniform and a red armband walks directly toward the dog and handler as the footsteps grow louder.",
    "question_type": "Sound Source Identification",
    "question": "Who generated the approaching footsteps?",
    "answer": "A male security guard wearing a dark uniform and a red armband.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00951.mp4",
    "question_id": "00951_9",
    "clip_path": "clips/00951/00951__0003500_0007454.mp4"
  },
  {
    "timestamp": "00:04 - 00:07",
    "context": "[00:04 - 00:07] As the security guard arrives, the handler instructs the dog: \"你跟保安叔叔走吧\" (You go with the security guard uncle), signaling a handover of guidance to station staff.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the handler tell the dog, \"You go with the security guard uncle\"?",
    "answer": "Because the security guard had arrived to guide them, and she was handing over guidance to the station staff.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00951.mp4",
    "question_id": "00951_10",
    "clip_path": "clips/00951/00951__0003500_0007454.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "[00:00 - 00:02] The handler states the destination. [00:02 - 00:04] A young man says, \"...Hey, wait for me.\" [00:04 - 00:07] The handler instructs the dog to go with the security guard.",
    "question_type": "Counting",
    "question": "How many distinct quoted speech utterances occur in the clip?",
    "answer": "Three.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00951.mp4",
    "question_id": "00951_11",
    "clip_path": "clips/00951/00951__0000000_0007454.mp4"
  },
  {
    "timestamp": "00:00 - 00:17",
    "context": "An automated female voice from the subway's public address system announces arrival at '火车南站', with clear, moderately loud audio. The train's rumble and screech decrease as it slows to a stop.",
    "question_type": "Sound Source Identification",
    "question": "What generated the station arrival announcement at the start of the clip?",
    "answer": "An automated female voice from the subway's public address system.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00952.mp4",
    "question_id": "00952_1",
    "clip_path": "clips/00952/00952__0000000_0017500.mp4"
  },
  {
    "timestamp": "00:00 - 00:17",
    "context": "The announcement is described as clear and moderately loud.",
    "question_type": "Sound Characteristics",
    "question": "What were the clarity and volume of the announcement?",
    "answer": "It was clear and moderately loud.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00952.mp4",
    "question_id": "00952_2",
    "clip_path": "clips/00952/00952__0000000_0017500.mp4"
  },
  {
    "timestamp": "00:00 - 00:17",
    "context": "The decreasing rumble and screech of the train are heard as it slows to a stop.",
    "question_type": "Temporal Information",
    "question": "How did the train's rumble and screech change over this interval?",
    "answer": "They decreased as the train slowed to a stop.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00952.mp4",
    "question_id": "00952_3",
    "clip_path": "clips/00952/00952__0000000_0017500.mp4"
  },
  {
    "timestamp": "00:00 - 00:17",
    "context": "The arrival announcement plays while the train slows; visually, a woman stands by the door preparing to disembark.",
    "question_type": "Cross-Modal Reasoning",
    "question": "After the announcement and deceleration sounds, what is the woman seen doing?",
    "answer": "She stands by the door, preparing to disembark as the train pulls into the station.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00952.mp4",
    "question_id": "00952_4",
    "clip_path": "clips/00952/00952__0000000_0017500.mp4"
  },
  {
    "timestamp": "00:17 - 00:27",
    "context": "A series of loud, intermittent beeps signals that the doors are about to open.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why do the loud, intermittent beeps occur at this time?",
    "answer": "They signal that the subway doors are about to open.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00952.mp4",
    "question_id": "00952_5",
    "clip_path": "clips/00952/00952__0016500_0027500.mp4"
  },
  {
    "timestamp": "00:17 - 00:27",
    "context": "The beeps are described as loud and intermittent.",
    "question_type": "Sound Characteristics",
    "question": "What are the acoustic qualities of the beeps before the doors open?",
    "answer": "They are loud and intermittent.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00952.mp4",
    "question_id": "00952_6",
    "clip_path": "clips/00952/00952__0016500_0027500.mp4"
  },
  {
    "timestamp": "00:17 - 00:27",
    "context": "A distinct mechanical sliding sound is heard as the subway doors part.",
    "question_type": "Sound Source Identification",
    "question": "What produced the distinct mechanical sliding sound?",
    "answer": "The subway doors parting.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00952.mp4",
    "question_id": "00952_7",
    "clip_path": "clips/00952/00952__0016500_0027500.mp4"
  },
  {
    "timestamp": "00:17 - 00:27",
    "context": "Loud intermittent beeps are followed by a distinct mechanical sliding sound as the doors open.",
    "question_type": "Temporal Information",
    "question": "What sound occurs immediately after the beeps?",
    "answer": "The distinct mechanical sliding sound of the subway doors opening.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00952.mp4",
    "question_id": "00952_8",
    "clip_path": "clips/00952/00952__0016500_0027500.mp4"
  },
  {
    "timestamp": "00:17 - 00:27",
    "context": "After the doors part, the ambient sounds of the station platform immediately become audible. Visually, the woman exits and the camera holder with a white cane follows onto the platform.",
    "question_type": "Cross-Modal Reasoning",
    "question": "Once the doors open and platform sounds become audible, what actions are seen?",
    "answer": "The woman exits first, and the camera holder using a white cane follows onto the platform.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00952.mp4",
    "question_id": "00952_9",
    "clip_path": "clips/00952/00952__0016500_0027500.mp4"
  },
  {
    "timestamp": "00:17 - 00:27",
    "context": "This interval includes beeps, the mechanical sliding of doors, and the platform's ambient sounds becoming audible.",
    "question_type": "Counting",
    "question": "How many distinct sound events are described in this interval?",
    "answer": "Three: the loud intermittent beeps, the mechanical sliding of the doors, and the ambient platform sounds becoming audible.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00952.mp4",
    "question_id": "00952_10",
    "clip_path": "clips/00952/00952__0016500_0027500.mp4"
  },
  {
    "timestamp": "00:27 - 00:42",
    "context": "A man identified as station staff by his blue jacket and megaphone speaks directly and clearly to the camera holder from a close distance in front.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From where relative to the camera does the staff member's speech originate?",
    "answer": "From a close distance directly in front of the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00952.mp4",
    "question_id": "00952_11",
    "clip_path": "clips/00952/00952__0026500_0042423.mp4"
  },
  {
    "timestamp": "00:27 - 00:42",
    "context": "The station staff member says: '来我帮你拉吧, 你稍微等一下. 保安叔叔在旁边, 你往那边走一点点.'",
    "question_type": "Sound Source Identification",
    "question": "Who speaks the quoted lines offering help?",
    "answer": "A station staff member identified by his blue jacket and megaphone.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00952.mp4",
    "question_id": "00952_12",
    "clip_path": "clips/00952/00952__0026500_0042423.mp4"
  },
  {
    "timestamp": "00:27 - 00:42",
    "context": "He offers assistance and then proceeds to guide the camera holder, demonstrating a relay of help for a visually impaired individual.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why does the station staff member proceed to guide the camera holder?",
    "answer": "Because he had offered help and is assisting the visually impaired camera holder, prompting him to guide them.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00952.mp4",
    "question_id": "00952_13",
    "clip_path": "clips/00952/00952__0026500_0042423.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "As the user and two companions enter an elevator, their canes make soft tapping sounds on the floor.",
    "question_type": "Sound Source Identification",
    "question": "What generated the soft tapping sounds as they entered the elevator?",
    "answer": "The white canes tapping on the floor.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00962.mp4",
    "question_id": "00962_1",
    "clip_path": "clips/00962/00962__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:04 - 00:09",
    "context": "Inside the elevator, one companion presses a button, which produces a soft, electronic beep.",
    "question_type": "Sound Source Identification",
    "question": "What produced the soft, electronic beep inside the elevator?",
    "answer": "The elevator control panel button press.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00962.mp4",
    "question_id": "00962_2",
    "clip_path": "clips/00962/00962__0003500_0009500.mp4"
  },
  {
    "timestamp": "00:04 - 00:09",
    "context": "They discuss the elevator's accessibility: 'This has accessibility features, right?'—'No, it doesn't announce the floors.' This highlights their need for auditory cues to navigate.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why were they discussing the elevator's accessibility features?",
    "answer": "Because, as visually impaired travelers who rely on auditory cues, they needed features like spoken floor announcements to navigate.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00962.mp4",
    "question_id": "00962_3",
    "clip_path": "clips/00962/00962__0003500_0009500.mp4"
  },
  {
    "timestamp": "00:09 - 00:13",
    "context": "An automated female voice announces, 'The floor you selected is invalid.' Another person then presses a button.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the elevator system announce that the selected floor was invalid?",
    "answer": "Because an incorrect button was pressed.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00962.mp4",
    "question_id": "00962_4",
    "clip_path": "clips/00962/00962__0008500_0013500.mp4"
  },
  {
    "timestamp": "00:09 - 00:13",
    "context": "An automated female voice from the elevator's system announces, 'The floor you selected is invalid.'",
    "question_type": "Sound Source Identification",
    "question": "What was the source of the 'The floor you selected is invalid' announcement?",
    "answer": "The elevator system's automated female voice.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00962.mp4",
    "question_id": "00962_5",
    "clip_path": "clips/00962/00962__0008500_0013500.mp4"
  },
  {
    "timestamp": "00:14 - 00:18",
    "context": "The elevator doors open with a soft, mechanical whirring sound, followed by a clear, high-pitched chime indicating their arrival at a floor.",
    "question_type": "Temporal Information",
    "question": "In what order were the arrival sounds heard when the elevator stopped?",
    "answer": "First the doors' soft mechanical whirring, then a clear, high-pitched chime.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00962.mp4",
    "question_id": "00962_6",
    "clip_path": "clips/00962/00962__0013500_0018500.mp4"
  },
  {
    "timestamp": "00:14 - 00:18",
    "context": "Seeking confirmation of their location, the user asks, 'Is this the first floor?' A male voice from nearby immediately confirms, 'First floor.'",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the user ask, 'Is this the first floor?'",
    "answer": "To confirm their location after arriving at a floor.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00962.mp4",
    "question_id": "00962_7",
    "clip_path": "clips/00962/00962__0013500_0018500.mp4"
  },
  {
    "timestamp": "00:14 - 00:18",
    "context": "A male voice from nearby immediately confirms, 'First floor.'",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From where did the confirming 'First floor' reply come relative to the user?",
    "answer": "From a nearby male voice in the lobby.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00962.mp4",
    "question_id": "00962_8",
    "clip_path": "clips/00962/00962__0013500_0018500.mp4"
  },
  {
    "timestamp": "00:18 - 00:22",
    "context": "As they pass through the glass entrance doors, the door mechanism makes a soft, metallic click.",
    "question_type": "Sound Source Identification",
    "question": "What caused the soft, metallic click as they exited the building?",
    "answer": "The glass entrance door mechanism.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00962.mp4",
    "question_id": "00962_9",
    "clip_path": "clips/00962/00962__0017500_0022500.mp4"
  },
  {
    "timestamp": "00:18 - 00:22",
    "context": "The sound of the dog's paws pattering on the tiled floor is audible, along with the rhythmic tapping of the user's white cane.",
    "question_type": "Sound Characteristics",
    "question": "How are the guide dog's footsteps described on the floor?",
    "answer": "As the dog's paws pattering on the tiled floor.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00962.mp4",
    "question_id": "00962_10",
    "clip_path": "clips/00962/00962__0017500_0022500.mp4"
  },
  {
    "timestamp": "00:18 - 00:22",
    "context": "Audible sounds include the dog's paws pattering, the rhythmic cane tapping, and a soft, metallic door click.",
    "question_type": "Counting",
    "question": "Excluding speech, how many distinct types of sounds are audible in this segment?",
    "answer": "Three: the dog's paws pattering, the rhythmic cane tapping, and the door mechanism's soft metallic click.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00962.mp4",
    "question_id": "00962_11",
    "clip_path": "clips/00962/00962__0017500_0022500.mp4"
  },
  {
    "timestamp": "00:23 - 00:30",
    "context": "Outside on a paved walkway, the cane makes a series of light taps on the pavement.",
    "question_type": "Sound Source Identification",
    "question": "What produced the series of light taps on the pavement?",
    "answer": "The user's white cane.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00962.mp4",
    "question_id": "00962_12",
    "clip_path": "clips/00962/00962__0022500_0030500.mp4"
  },
  {
    "timestamp": "00:23 - 00:30",
    "context": "The user identifies a staircase at the entrance; a companion confirms with 'Right.'",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the companion say 'Right' after the user spoke?",
    "answer": "To acknowledge and confirm the user's identification of a staircase hazard at the entrance.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00962.mp4",
    "question_id": "00962_13",
    "clip_path": "clips/00962/00962__0022500_0030500.mp4"
  },
  {
    "timestamp": "00:30 - 00:36",
    "context": "The user commands the guide dog, 'Let's go,' followed by 'In this direction.' The cane taps rhythmically, providing a consistent auditory backdrop.",
    "question_type": "Temporal Information",
    "question": "Was the white cane tapping continuous or intermittent during this segment?",
    "answer": "Continuous and rhythmic, providing a consistent auditory backdrop.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00962.mp4",
    "question_id": "00962_14",
    "clip_path": "clips/00962/00962__0029500_0036500.mp4"
  },
  {
    "timestamp": "00:30 - 00:36",
    "context": "The user gives two distinct spoken commands to the guide dog: 'Let's go' and 'In this direction.'",
    "question_type": "Counting",
    "question": "How many distinct verbal commands did the user give to the guide dog in this segment?",
    "answer": "Two commands.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00962.mp4",
    "question_id": "00962_15",
    "clip_path": "clips/00962/00962__0029500_0036500.mp4"
  },
  {
    "timestamp": "00:00 - 00:08",
    "context": "A rhythmic, sharp tapping sound is produced by a white and red long cane hitting the ground directly in front of the camera.",
    "question_type": "Sound Source Identification",
    "question": "What object generated the rhythmic, sharp tapping sound at the start?",
    "answer": "A white and red long cane hitting the ground.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00971.mp4",
    "question_id": "00971_1",
    "clip_path": "clips/00971/00971__0000000_0008500.mp4"
  },
  {
    "timestamp": "00:00 - 00:08",
    "context": "The rhythmic, sharp tapping originates directly in front of the camera.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the tapping originate relative to the camera?",
    "answer": "Directly in front of the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00971.mp4",
    "question_id": "00971_2",
    "clip_path": "clips/00971/00971__0000000_0008500.mp4"
  },
  {
    "timestamp": "00:00 - 00:08",
    "context": "The tapping indicates the person is using the cane to navigate.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why was the rhythmic tapping occurring?",
    "answer": "Because the person was using the cane to navigate.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00971.mp4",
    "question_id": "00971_3",
    "clip_path": "clips/00971/00971__0000000_0008500.mp4"
  },
  {
    "timestamp": "00:08 - 00:13",
    "context": "A white scooter with a delivery box passes from the front left, emitting a faint, high-pitched whirring sound that quickly fades.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From which direction did the faint, high-pitched whirring pass by?",
    "answer": "From the front left.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00971.mp4",
    "question_id": "00971_4",
    "clip_path": "clips/00971/00971__0007500_0013500.mp4"
  },
  {
    "timestamp": "00:08 - 00:13",
    "context": "A white scooter with a delivery box passes, emitting a faint, high-pitched whirring sound that quickly fades.",
    "question_type": "Sound Source Identification",
    "question": "What produced the faint, high-pitched whirring sound?",
    "answer": "A white scooter with a delivery box.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00971.mp4",
    "question_id": "00971_5",
    "clip_path": "clips/00971/00971__0007500_0013500.mp4"
  },
  {
    "timestamp": "00:08 - 00:13",
    "context": "The scooter’s high-pitched whirring quickly fades as it passes.",
    "question_type": "Temporal Information",
    "question": "How did the scooter’s whirring change over time as it passed?",
    "answer": "It quickly faded.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00971.mp4",
    "question_id": "00971_6",
    "clip_path": "clips/00971/00971__0007500_0013500.mp4"
  },
  {
    "timestamp": "00:08 - 00:13",
    "context": "Immediately after the scooter, a three-wheeled vehicle with a green cover drives away, its engine producing a low hum.",
    "question_type": "Sound Source Identification",
    "question": "What produced the low hum that followed the scooter’s whirring?",
    "answer": "The engine of a three-wheeled vehicle with a green cover.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00971.mp4",
    "question_id": "00971_7",
    "clip_path": "clips/00971/00971__0007500_0013500.mp4"
  },
  {
    "timestamp": "00:08 - 00:13",
    "context": "A scooter passes, then a three-wheeled vehicle drives away.",
    "question_type": "Counting",
    "question": "How many vehicles passed during this interval?",
    "answer": "Two vehicles.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00971.mp4",
    "question_id": "00971_8",
    "clip_path": "clips/00971/00971__0007500_0013500.mp4"
  },
  {
    "timestamp": "00:16 - 00:30",
    "context": "The person says: \"I should have crossed the road already, and I can no longer hear the traffic flow in front.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "What auditory evidence did the person use to infer they had crossed the road?",
    "answer": "They could no longer hear the traffic flow in front.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00971.mp4",
    "question_id": "00971_9",
    "clip_path": "clips/00971/00971__0015500_0030500.mp4"
  },
  {
    "timestamp": "00:30 - 00:33",
    "context": "The person probes the unpaved area to the right, covered in dry grass and dirt, producing a soft, rustling sound.",
    "question_type": "Sound Source Identification",
    "question": "What produced the soft, rustling sound during the brief stop?",
    "answer": "The cane probing dry grass and dirt in the unpaved area to the right.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00971.mp4",
    "question_id": "00971_10",
    "clip_path": "clips/00971/00971__0029500_0033500.mp4"
  },
  {
    "timestamp": "00:30 - 00:33",
    "context": "After the rustling from probing dry grass and dirt, the person says, \"It seems I can't get up on this side.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the person conclude, \"It seems I can't get up on this side\"?",
    "answer": "Because probing revealed an unpaved area of dry grass and dirt, indicating it wasn’t an accessible entrance to the sidewalk.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00971.mp4",
    "question_id": "00971_11",
    "clip_path": "clips/00971/00971__0029500_0033500.mp4"
  },
  {
    "timestamp": "00:36 - 00:41",
    "context": "The cane’s tapping helps detect a change in the road’s edge; the person says, \"Oh, there's an intersection here,\" and turns right.",
    "question_type": "Cross-Modal Reasoning",
    "question": "What did the cane’s tapping help the person detect, and what action did they take?",
    "answer": "It helped detect a corner/intersection, and they turned right to follow the curb.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00971.mp4",
    "question_id": "00971_12",
    "clip_path": "clips/00971/00971__0035500_0041500.mp4"
  },
  {
    "timestamp": "00:41 - 00:47",
    "context": "The cane’s sound changes from a sharp tap on asphalt to a duller, clattering noise on sidewalk tiles.",
    "question_type": "Sound Characteristics",
    "question": "How did the cane’s sound change when transitioning from road to sidewalk?",
    "answer": "It changed from a sharp tap on asphalt to a duller, clattering noise on sidewalk tiles.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00971.mp4",
    "question_id": "00971_13",
    "clip_path": "clips/00971/00971__0040500_0047500.mp4"
  },
  {
    "timestamp": "00:41 - 00:47",
    "context": "The sound change accompanies the person moving onto the paved sidewalk.",
    "question_type": "Cross-Modal Reasoning",
    "question": "What does the change in cane sound indicate about the surface underfoot?",
    "answer": "It indicates a transition from the road to a paved sidewalk.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00971.mp4",
    "question_id": "00971_14",
    "clip_path": "clips/00971/00971__0040500_0047500.mp4"
  },
  {
    "timestamp": "00:49 - 01:03",
    "context": "The cane is swept left and right, creating a scraping and tapping sound across the tiles while searching for tactile paving.",
    "question_type": "Sound Characteristics",
    "question": "What sounds are produced when the cane is swept left and right during the search?",
    "answer": "Scraping and tapping across the tiles.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00971.mp4",
    "question_id": "00971_15",
    "clip_path": "clips/00971/00971__0048500_0063500.mp4"
  },
  {
    "timestamp": "00:49 - 01:03",
    "context": "Upon contacting the yellow, ridged tactile paving, the tapping becomes louder and more distinctly rhythmic.",
    "question_type": "Temporal Information",
    "question": "How does the tapping change when the cane contacts the tactile paving?",
    "answer": "It becomes louder and more distinctly rhythmic.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00971.mp4",
    "question_id": "00971_16",
    "clip_path": "clips/00971/00971__0048500_0063500.mp4"
  },
  {
    "timestamp": "00:49 - 01:03",
    "context": "They say they’ve found the tactile paving; the tapping on the raised ridges is louder and rhythmic, and visually they walk straight along it.",
    "question_type": "Cross-Modal Reasoning",
    "question": "After the cane contacts the yellow, ridged tactile paving, what does the audio suggest about their path?",
    "answer": "They are now walking straight along the tactile path.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00971.mp4",
    "question_id": "00971_17",
    "clip_path": "clips/00971/00971__0048500_0063500.mp4"
  },
  {
    "timestamp": "00:49 - 01:03",
    "context": "During the search technique, the cane produces scraping and tapping sounds across the tiles.",
    "question_type": "Counting",
    "question": "How many distinct cane-surface sound types are present during the search technique?",
    "answer": "Two: scraping and tapping.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00971.mp4",
    "question_id": "00971_18",
    "clip_path": "clips/00971/00971__0048500_0063500.mp4"
  },
  {
    "timestamp": "00:00 - 00:19",
    "context": "[00:00 - 00:19] A rhythmic, low-pitched beeping sound emanates from a speaker on the traffic light pole, located a few meters in front and above the camera.",
    "question_type": "Sound Source Identification",
    "question": "What generated the rhythmic, low-pitched beeping sound?",
    "answer": "A speaker on the traffic light pole.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00981.mp4",
    "question_id": "00981_1",
    "clip_path": "clips/00981/00981__0000000_0019500.mp4"
  },
  {
    "timestamp": "00:00 - 00:19",
    "context": "[00:00 - 00:19] The beeping emanates from a speaker on the traffic light pole, located a few meters in front and above the camera.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where was the beeping located relative to the camera?",
    "answer": "A few meters in front of and above the camera, from the traffic light pole’s speaker.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00981.mp4",
    "question_id": "00981_2",
    "clip_path": "clips/00981/00981__0000000_0019500.mp4"
  },
  {
    "timestamp": "00:00 - 00:19",
    "context": "[00:00 - 00:19] A rhythmic, low-pitched beeping sound is audible.",
    "question_type": "Sound Characteristics",
    "question": "What were the acoustic qualities of the beeping?",
    "answer": "It was rhythmic and low-pitched.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00981.mp4",
    "question_id": "00981_3",
    "clip_path": "clips/00981/00981__0000000_0019500.mp4"
  },
  {
    "timestamp": "00:00 - 00:19",
    "context": "[00:00 - 00:19] While the female speaker explains the accessibility features, a rhythmic beeping is heard concurrently.",
    "question_type": "Temporal Information",
    "question": "When did the beeping occur relative to the narration?",
    "answer": "It occurred concurrently during the speaker’s explanation between 00:00 and 00:19.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00981.mp4",
    "question_id": "00981_4",
    "clip_path": "clips/00981/00981__0000000_0019500.mp4"
  },
  {
    "timestamp": "00:00 - 00:19",
    "context": "[00:00 - 00:19] While she speaks, an automated voice from the traffic light's speaker announces a command.",
    "question_type": "Sound Source Identification",
    "question": "What was the source of the automated voice announcement?",
    "answer": "The traffic light’s speaker.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00981.mp4",
    "question_id": "00981_5",
    "clip_path": "clips/00981/00981__0000000_0019500.mp4"
  },
  {
    "timestamp": "00:00 - 00:19",
    "context": "[00:00 - 00:19] The speaker notes the light has voice prompts and a 'da-da-da' beeping sound. An automated voice is heard; the beeping is also audible.",
    "question_type": "Counting",
    "question": "How many distinct types of audible signals from the traffic light are heard in this segment?",
    "answer": "Two: rhythmic beeping and an automated voice prompt.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00981.mp4",
    "question_id": "00981_6",
    "clip_path": "clips/00981/00981__0000000_0019500.mp4"
  },
  {
    "timestamp": "00:00 - 00:19",
    "context": "[00:00 - 00:19] A female speaker explains the features and concludes, 'This is the pinnacle of accessibility,' describing voice prompts and variable-speed beeps indicating red vs. green for pedestrians.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the primary speaker conclude, 'This is the pinnacle of accessibility'?",
    "answer": "Because the traffic light provides both voice prompts and distinct beeping speeds to indicate stop and go, demonstrating advanced features for visually impaired pedestrians.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00981.mp4",
    "question_id": "00981_7",
    "clip_path": "clips/00981/00981__0000000_0019500.mp4"
  },
  {
    "timestamp": "00:00 - 00:19",
    "context": "[00:00 - 00:19] Her voice is clear and originates from the camera's location.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the primary speaker’s voice originate relative to the camera?",
    "answer": "Directly from the camera’s location (co-located with the camera).",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00981.mp4",
    "question_id": "00981_8",
    "clip_path": "clips/00981/00981__0000000_0019500.mp4"
  },
  {
    "timestamp": "00:19 - 00:21",
    "context": "[00:19 - 00:21] A second female voice nearby says, 'The best one we've seen recently.' The primary speaker replies, 'Right.'",
    "question_type": "Inferential & Contextual Causality",
    "question": "What was the likely purpose of the brief exchange between the two speakers?",
    "answer": "To agree with and reinforce the positive evaluation of the traffic light’s accessibility.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00981.mp4",
    "question_id": "00981_9",
    "clip_path": "clips/00981/00981__0018500_0021223.mp4"
  },
  {
    "timestamp": "00:00 - 00:21",
    "context": "[00:00 - 00:19] One female speaker narrates. [00:19 - 00:21] A second female voice agrees and the primary speaker affirms.",
    "question_type": "Counting",
    "question": "How many human speakers are heard in the clip?",
    "answer": "Two female speakers.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00981.mp4",
    "question_id": "00981_10",
    "clip_path": "clips/00981/00981__0000000_0021223.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "[00:00 - 00:06] From a first-person view, the user scrolls a food delivery app while speaking to someone (likely on the phone) to decide on an order, saying: \"Hello... Okay... Right now there's a two-item combo with boneless fried chicken, fries, and chicken nuggets... Is there anything else you want to eat?\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Based on the surrounding events, why does the speaker ask, \"Is there anything else you want to eat?\"",
    "answer": "To coordinate and finalize what to order with someone (likely on the phone) while browsing the food delivery app.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00985.mp4",
    "question_id": "00985_1",
    "clip_path": "clips/00985/00985__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "[00:00 - 00:06] Throughout the interaction, a continuous, moderately loud sound of running water is heard from the front-right, where another person in a red top is working at the kitchen sink.",
    "question_type": "Sound Source Identification",
    "question": "What generated the continuous, moderately loud background sound heard during this segment?",
    "answer": "Running water from the kitchen sink being used by another person in a red top.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00985.mp4",
    "question_id": "00985_2",
    "clip_path": "clips/00985/00985__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "[00:00 - 00:06] The speaker’s voice originates from the camera’s location and is described as clear and conversational.",
    "question_type": "Sound Characteristics",
    "question": "What was the quality of the speaker’s voice during this segment?",
    "answer": "Clear and conversational.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00985.mp4",
    "question_id": "00985_3",
    "clip_path": "clips/00985/00985__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "[00:00 - 00:06] A continuous, moderately loud running-water sound is heard from the front-right.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From what direction relative to the camera does the running water sound originate?",
    "answer": "From the front-right.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00985.mp4",
    "question_id": "00985_4",
    "clip_path": "clips/00985/00985__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "[00:00 - 00:06] The sound of running water persists throughout the interaction while the speaker talks.",
    "question_type": "Temporal Information",
    "question": "Is the running water sound brief or continuous, and over what time span is it audible?",
    "answer": "It is continuous and audible throughout 00:00–00:06.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00985.mp4",
    "question_id": "00985_5",
    "clip_path": "clips/00985/00985__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "[00:00 - 00:06] Speech from the camera location occurs while a single environmental sound—running water—continues in the background.",
    "question_type": "Counting",
    "question": "How many distinct non-speech environmental sounds are continuously present during 00:00–00:06?",
    "answer": "One—the continuous sound of running water.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00985.mp4",
    "question_id": "00985_6",
    "clip_path": "clips/00985/00985__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "[00:00 - 00:06] The running water is heard from the front-right; visually, another person in a red top is standing and working at the kitchen sink in that area.",
    "question_type": "Cross-Modal Reasoning",
    "question": "When the running water is heard from the front-right, what does the visual context indicate about its source and activity?",
    "answer": "Another person in a red top is at the kitchen sink, working there.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00985.mp4",
    "question_id": "00985_7",
    "clip_path": "clips/00985/00985__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "[00:00 - 00:06] The camera-wearer says in a medium-volume, questioning tone, '我不确定是哪一坨 (I'm not sure which one it is).' The second person, standing to the left, clarifies, '就是那坨盘旋着的就是了 (It's the one that's spiraling).'",
    "question_type": "Sound Characteristics",
    "question": "What was the volume and tone of the camera-wearer's initial statement?",
    "answer": "Medium-volume with a questioning tone.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00987.mp4",
    "question_id": "00987_1",
    "clip_path": "clips/00987/00987__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "[00:00 - 00:06] The second person, standing to the left of the camera, says, '就是那坨盘旋着的就是了 (It's the one that's spiraling).'",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From which direction relative to the camera did the clarification 'It's the one that's spiraling' originate?",
    "answer": "From the left side of the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00987.mp4",
    "question_id": "00987_2",
    "clip_path": "clips/00987/00987__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:06 - 00:11",
    "context": "[00:06 - 00:11] The camera-wearer says, '不要摸盘子 (Don't touch the tray),' then clarifies, '但是饼干是可以摸的 (But the cookies can be touched),' indicating a touch-based identification game.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the camera-wearer clarify, 'But the cookies can be touched'?",
    "answer": "To set the game's rule that only the cookies could be touched for identification, not the tray.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00987.mp4",
    "question_id": "00987_3",
    "clip_path": "clips/00987/00987__0005500_0011500.mp4"
  },
  {
    "timestamp": "00:06 - 00:11",
    "context": "[00:06 - 00:11] Rules are stated: '不要摸盘子 (Don't touch the tray)' and '但是饼干是可以摸的 (But the cookies can be touched).'",
    "question_type": "Counting",
    "question": "How many distinct touching rules were stated during this segment?",
    "answer": "Two rules.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00987.mp4",
    "question_id": "00987_4",
    "clip_path": "clips/00987/00987__0005500_0011500.mp4"
  },
  {
    "timestamp": "00:11 - 00:23",
    "context": "[00:11 - 00:23] Second person asks, '饼干可以摸了吗 (Can I touch the cookies now)?' The camera-wearer confirms, '可以摸了 (Yes, you can),' after which the second person begins feeling the cookies.",
    "question_type": "Temporal Information",
    "question": "When did the second person begin touching the cookies relative to the confirmation?",
    "answer": "Immediately after the camera-wearer affirmed, 'Yes, you can.'",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00987.mp4",
    "question_id": "00987_5",
    "clip_path": "clips/00987/00987__0010500_0023500.mp4"
  },
  {
    "timestamp": "00:11 - 00:23",
    "context": "[00:11 - 00:23] The second person jokes, '哦... 你的蛇已经变成了黄鳝我觉得 (Oh... I think your snake has turned into a swamp eel),' prompting a short, amused laugh from the camera-wearer.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the camera-wearer laugh briefly in this segment?",
    "answer": "In response to the second person's playful remark that the snake cookie looked like a swamp eel.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00987.mp4",
    "question_id": "00987_6",
    "clip_path": "clips/00987/00987__0010500_0023500.mp4"
  },
  {
    "timestamp": "00:23 - 00:29",
    "context": "[00:23 - 00:29] A third person (a man in a red shirt) joins from the left, laughs audibly, and says, '这个是诺诺那个烤的 (This is the one Nuonuo baked).'",
    "question_type": "Sound Source Identification",
    "question": "Who identified a cookie by saying, 'This is the one Nuonuo baked'?",
    "answer": "The third person, a man in a red shirt.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00987.mp4",
    "question_id": "00987_7",
    "clip_path": "clips/00987/00987__0022500_0029500.mp4"
  },
  {
    "timestamp": "00:23 - 00:29",
    "context": "[00:23 - 00:29] The man in a red shirt leans over the table from the left and laughs audibly while speaking.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From what direction relative to the camera did the man's laughter and identification originate?",
    "answer": "From the left side.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00987.mp4",
    "question_id": "00987_8",
    "clip_path": "clips/00987/00987__0022500_0029500.mp4"
  },
  {
    "timestamp": "00:23 - 00:29",
    "context": "[00:23 - 00:29] After the man identifies a cookie, 'This is the one Nuonuo baked,' others respond with more light-hearted laughter.",
    "question_type": "Sound Characteristics",
    "question": "How is the group's laughter described after the man's identification?",
    "answer": "Light-hearted.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00987.mp4",
    "question_id": "00987_9",
    "clip_path": "clips/00987/00987__0022500_0029500.mp4"
  },
  {
    "timestamp": "00:29 - 00:36",
    "context": "[00:29 - 00:36] While analyzing the misshapen snake cookie, the man explains, '它头竖不起来, 估计烤的时候 (Its head couldn't stand up, probably when it was baking).'",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the man offer the explanation about the snake cookie's head not standing up?",
    "answer": "Because the group was collaboratively analyzing why the snake cookie was misshapen.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00987.mp4",
    "question_id": "00987_10",
    "clip_path": "clips/00987/00987__0028500_0036500.mp4"
  },
  {
    "timestamp": "00:36 - 00:48",
    "context": "[00:36 - 00:48] The second person takes a bite, producing a soft crunching sound, as eating begins.",
    "question_type": "Sound Characteristics",
    "question": "What was the quality of the sound produced when the second person bit into a cookie?",
    "answer": "A soft crunching sound.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00987.mp4",
    "question_id": "00987_11",
    "clip_path": "clips/00987/00987__0035500_0048500.mp4"
  },
  {
    "timestamp": "00:36 - 00:48",
    "context": "[00:36 - 00:48] The camera-wearer later holds a half-eaten cookie and says, '好吃好吃 (Tasty, tasty).'",
    "question_type": "Counting",
    "question": "How many times did the camera-wearer say 'tasty' in their reaffirmation?",
    "answer": "Twice.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00987.mp4",
    "question_id": "00987_12",
    "clip_path": "clips/00987/00987__0035500_0048500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "They are at a glass table preparing something with small pink molds on a cutting board. The camera wearer tells her male companion, \"你先去洗个手先吧\" (You go wash your hands first) as he is about to join the activity.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why does the camera wearer tell the male companion to wash his hands first?",
    "answer": "Because he is about to join the hands-on preparation at the table, so she wants him to clean up before touching the items.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00991.mp4",
    "question_id": "00991_1",
    "clip_path": "clips/00991/00991__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "After being told to wash his hands, a nearby male voice asks, \"厕所在哪\" (Where is the bathroom?).",
    "question_type": "Inferential & Contextual Causality",
    "question": "What prompted the male companion to ask, \"Where is the bathroom?\"",
    "answer": "He was instructed to wash his hands and needed to know where the bathroom is.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00991.mp4",
    "question_id": "00991_2",
    "clip_path": "clips/00991/00991__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "In response to the male companion asking where the bathroom is, the user replies, \"再往门口走\" (Keep going towards the door).",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why does the user give the direction \"Keep going towards the door\"?",
    "answer": "She is answering his question about the bathroom’s location.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00991.mp4",
    "question_id": "00991_3",
    "clip_path": "clips/00991/00991__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "After the male says \"OK\" to acknowledge the directions, the user adds teasingly, \"你还知道门吗\" (Do you even know where the door is?).",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why does the user teasingly ask, \"Do you even know where the door is?\"",
    "answer": "She is playfully teasing him after he needed directions to the bathroom, implying he might not even know the door’s location.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00991.mp4",
    "question_id": "00991_4",
    "clip_path": "clips/00991/00991__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "The user speaks in a clear, close-range voice: \"你先去洗个手先吧\" (You go wash your hands first).",
    "question_type": "Sound Source Identification",
    "question": "Who says, \"你先去洗个手先吧\" (You go wash your hands first)?",
    "answer": "The camera wearer (female).",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00991.mp4",
    "question_id": "00991_5",
    "clip_path": "clips/00991/00991__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "A male voice from nearby asks, \"厕所在哪\" (Where is the bathroom?), then says \"OK\" after receiving directions.",
    "question_type": "Sound Source Identification",
    "question": "Who says \"OK\" during the exchange?",
    "answer": "The male companion nearby.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00991.mp4",
    "question_id": "00991_6",
    "clip_path": "clips/00991/00991__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "The user's line is described as a clear, close-range voice while at the glass table.",
    "question_type": "Sound Characteristics",
    "question": "What is the quality and proximity of the user's speech?",
    "answer": "It is clear and close-range.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00991.mp4",
    "question_id": "00991_7",
    "clip_path": "clips/00991/00991__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "The male voice is described as coming from nearby during the brief conversation.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where does the male companion's voice originate relative to the camera?",
    "answer": "From nearby, close to the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00991.mp4",
    "question_id": "00991_8",
    "clip_path": "clips/00991/00991__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "Sequence: The user gives directions (\"再往门口走\"). The male then confirms with \"OK\".",
    "question_type": "Temporal Information",
    "question": "Which occurs first, the user giving directions or the male companion saying \"OK\"?",
    "answer": "The user giving directions occurs first, followed by the male saying \"OK\".",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00991.mp4",
    "question_id": "00991_9",
    "clip_path": "clips/00991/00991__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "Within the short exchange, five utterances occur: instruction, question, directions, confirmation, and a teasing remark.",
    "question_type": "Counting",
    "question": "How many distinct spoken utterances are exchanged in this segment?",
    "answer": "Five.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00991.mp4",
    "question_id": "00991_10",
    "clip_path": "clips/00991/00991__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "Visually, they are preparing something with small pink molds on a cutting board at a glass table, and their hands are actively engaged while talking. The user instructs the male to wash his hands first.",
    "question_type": "Cross-Modal Reasoning",
    "question": "How does the visual context of their hands working with small pink molds help explain the instruction to wash hands?",
    "answer": "It indicates he will be handling the items, so washing hands is for cleanliness before participating.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00991.mp4",
    "question_id": "00991_11",
    "clip_path": "clips/00991/00991__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "The camera pans across a refrigerated display case with desserts. The person holding the camera says, “橘子现在去核券啦, 给大家看一下,” and a faint refrigerator hum is audible.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the speaker say “橘子现在去核券啦, 给大家看一下” at the start?",
    "answer": "To show the audience the selection of festive treats available.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00998.mp4",
    "question_id": "00998_1",
    "clip_path": "clips/00998/00998__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "A faint hum is audible while the camera pans across a refrigerated display case.",
    "question_type": "Sound Source Identification",
    "question": "What is the source of the faint hum heard during this segment?",
    "answer": "The refrigerated display case (refrigerator).",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00998.mp4",
    "question_id": "00998_2",
    "clip_path": "clips/00998/00998__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "A faint hum is audible in the background as the desserts are shown.",
    "question_type": "Sound Characteristics",
    "question": "How is the volume of the refrigerator hum described?",
    "answer": "Faint.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00998.mp4",
    "question_id": "00998_3",
    "clip_path": "clips/00998/00998__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "The camera is panning across a refrigerated display case while a faint hum is heard.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Relative to the camera, where does the hum originate?",
    "answer": "From the refrigerated display case immediately in front of the camera as it pans across.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00998.mp4",
    "question_id": "00998_4",
    "clip_path": "clips/00998/00998__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "The person, presumably holding the camera, speaks in a clear, conversational tone.",
    "question_type": "Sound Characteristics",
    "question": "What is the quality of the speaker’s voice during this segment?",
    "answer": "Clear and conversational.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00998.mp4",
    "question_id": "00998_5",
    "clip_path": "clips/00998/00998__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "The person holding the camera is the one speaking.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where does the speech originate relative to the camera?",
    "answer": "Very close to the camera, from the person holding it.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00998.mp4",
    "question_id": "00998_6",
    "clip_path": "clips/00998/00998__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:06 - 00:08",
    "context": "As the camera focuses on rows of cream puffs and pink mochi, the person says, “元宵节快乐.”",
    "question_type": "Temporal Information",
    "question": "When does the speaker say “元宵节快乐”?",
    "answer": "Between 00:06 and 00:08.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00998.mp4",
    "question_id": "00998_7",
    "clip_path": "clips/00998/00998__0005500_0008500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "During the initial pan across the display case, the speaker talks while a faint refrigerator hum is audible.",
    "question_type": "Counting",
    "question": "How many distinct types of sounds are audible in this interval?",
    "answer": "Two: the speaker’s voice and the faint refrigerator hum.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00998.mp4",
    "question_id": "00998_8",
    "clip_path": "clips/00998/00998__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "The speaker says, “给大家看一下” (“let me show everyone”) as the camera pans across desserts like cream puffs, mochi, and cakes.",
    "question_type": "Cross-Modal Reasoning",
    "question": "After saying “let me show everyone,” what is visually shown?",
    "answer": "The refrigerated display case with desserts such as cream puffs, mochi, and cakes, as the camera pans left to right.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00998.mp4",
    "question_id": "00998_9",
    "clip_path": "clips/00998/00998__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:06 - 00:08",
    "context": "While focusing on cream puffs and pink mochi, the speaker cheerfully says, “元宵节快乐.”",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why does the speaker say “元宵节快乐” here?",
    "answer": "To indicate the shopping is for the Lantern Festival and they are viewing desserts to celebrate the holiday.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00998.mp4",
    "question_id": "00998_10",
    "clip_path": "clips/00998/00998__0005500_0008500.mp4"
  },
  {
    "timestamp": "00:06 - 00:08",
    "context": "The camera focuses on rows of cream puffs and pink mochi; the person says, “元宵节快乐.”",
    "question_type": "Cross-Modal Reasoning",
    "question": "When the camera focuses on the desserts, what speech follows?",
    "answer": "The speaker says, “元宵节快乐” (“Happy Lantern Festival”).",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00998.mp4",
    "question_id": "00998_11",
    "clip_path": "clips/00998/00998__0005500_0008500.mp4"
  },
  {
    "timestamp": "00:08 - 00:11",
    "context": "The user's path is blocked by large, orange cardboard boxes. The user says in a slightly flustered tone, \"Aiya, I can't get through.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the user say, \"Aiya, I can't get through\"?",
    "answer": "Because large orange cardboard boxes were blocking their path in the aisle.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00999.mp4",
    "question_id": "00999_1",
    "clip_path": "clips/00999/00999__0007500_0011500.mp4"
  },
  {
    "timestamp": "00:00 - 00:08",
    "context": "A low, continuous hum is present while the user asks about flavors and a staff member lists options.",
    "question_type": "Sound Source Identification",
    "question": "What produced the low, continuous hum heard during the initial conversation?",
    "answer": "The refrigeration units.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00999.mp4",
    "question_id": "00999_2",
    "clip_path": "clips/00999/00999__0000000_0008500.mp4"
  },
  {
    "timestamp": "00:00 - 00:08",
    "context": "A low, continuous hum is audible in the background during the dialogue about dessert flavors.",
    "question_type": "Sound Characteristics",
    "question": "What are the acoustic characteristics of the background hum?",
    "answer": "It is low and continuous.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00999.mp4",
    "question_id": "00999_3",
    "clip_path": "clips/00999/00999__0000000_0008500.mp4"
  },
  {
    "timestamp": "00:30 - 00:33",
    "context": "Another person asks from the left, \"I'm on the left, right?\" The user replies, \"Ah, okay.\"",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From which direction relative to the camera did the companion's voice originate?",
    "answer": "From the left side.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00999.mp4",
    "question_id": "00999_4",
    "clip_path": "clips/00999/00999__0029500_0033500.mp4"
  },
  {
    "timestamp": "00:18 - 00:26",
    "context": "The staff member explains, \"It's 18.8 yuan per bag. There's an online group-buy promotion for 49 yuan for 4 bags.\"",
    "question_type": "Temporal Information",
    "question": "During which time interval did the staff member provide the detailed pricing explanation?",
    "answer": "Between 00:18 and 00:26.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00999.mp4",
    "question_id": "00999_5",
    "clip_path": "clips/00999/00999__0017500_0026500.mp4"
  },
  {
    "timestamp": "00:00 - 00:08",
    "context": "A staff member lists available flavors, including \"black sesame\" and \"durian.\"",
    "question_type": "Counting",
    "question": "How many specific flavors were explicitly named by the staff member?",
    "answer": "Two: black sesame and durian.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00999.mp4",
    "question_id": "00999_6",
    "clip_path": "clips/00999/00999__0000000_0008500.mp4"
  },
  {
    "timestamp": "00:18 - 00:26",
    "context": "The staff member mentions an online group-buy promotion: \"49 yuan for 4 bags.\"",
    "question_type": "Counting",
    "question": "How many bags are included in the online group-buy promotion mentioned?",
    "answer": "Four bags.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00999.mp4",
    "question_id": "00999_7",
    "clip_path": "clips/00999/00999__0017500_0026500.mp4"
  },
  {
    "timestamp": "00:00 - 00:08",
    "context": "A clear female voice, originating from the user, asks a staff member, \"What flavors do you have?\"",
    "question_type": "Sound Source Identification",
    "question": "Whose voice asks, \"What flavors do you have?\"",
    "answer": "The user's clear female voice.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00999.mp4",
    "question_id": "00999_8",
    "clip_path": "clips/00999/00999__0000000_0008500.mp4"
  },
  {
    "timestamp": "00:26 - 00:30",
    "context": "The staff member says, \"Then you need to go to the front to verify the coupon, it's over there.\" The user's hand appears, gesturing, as they ask, \"Where?\"",
    "question_type": "Cross-Modal Reasoning",
    "question": "After being told to verify the coupon at the front, what visual action accompanies the user's follow-up question?",
    "answer": "The user’s hand appears gesturing toward the freezer as they ask, \"Where?\"",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00999.mp4",
    "question_id": "00999_9",
    "clip_path": "clips/00999/00999__0025500_0030500.mp4"
  },
  {
    "timestamp": "00:37 - 00:40",
    "context": "While the camera pans across cakes and pastries, a staff member from behind the counter calls out from a moderate distance, \"Okay, wait a moment.\"",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the voice saying \"Okay, wait a moment\" originate relative to the camera?",
    "answer": "From behind the counter at a moderate distance.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00999.mp4",
    "question_id": "00999_10",
    "clip_path": "clips/00999/00999__0036500_0040500.mp4"
  },
  {
    "timestamp": "00:08 - 00:11",
    "context": "The user encounters blocked boxes and says, \"Aiya, I can't get through.\"",
    "question_type": "Sound Characteristics",
    "question": "What was the tone of the user's statement, \"Aiya, I can't get through\"?",
    "answer": "Slightly flustered.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/00999.mp4",
    "question_id": "00999_11",
    "clip_path": "clips/00999/00999__0007500_0011500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "[00:00] A person walks on tiled sidewalk using a white cane; a continuous, sharp tapping occurs each time the cane tip strikes the ground. [00:02] They say: \"We came out shopping today, and it's the Lantern Festival. Walking around here, I heard someone selling tangyuan, so I plan to go buy some.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Based on the person's spoken explanation, why do they plan to buy tangyuan?",
    "answer": "Because while out shopping during the Lantern Festival, they heard someone selling tangyuan.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01001.mp4",
    "question_id": "01001_1",
    "clip_path": "clips/01001/01001__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "[00:00] The person walks using a white cane; a continuous, sharp tapping is heard as the cane tip hits the ground.",
    "question_type": "Sound Source Identification",
    "question": "What generated the continuous, sharp tapping sound at the start?",
    "answer": "The tip of the white cane striking the ground tiles.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01001.mp4",
    "question_id": "01001_2",
    "clip_path": "clips/01001/01001__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "[00:00] Continuous, sharp tapping accompanies each cane strike on the tiles.",
    "question_type": "Sound Characteristics",
    "question": "How is the cane-tapping sound described at 00:00–00:06?",
    "answer": "Continuous and sharp.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01001.mp4",
    "question_id": "01001_3",
    "clip_path": "clips/01001/01001__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "[00:02] The person speaks while walking.",
    "question_type": "Sound Characteristics",
    "question": "How is the speaker's voice described when explaining their plans?",
    "answer": "Clear and conversational.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01001.mp4",
    "question_id": "01001_4",
    "clip_path": "clips/01001/01001__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:07 - 00:15",
    "context": "[00:07] A loud, cheerful promotional song begins to play from a source in front, growing louder as the person continues walking.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From which direction relative to the listener does the promotional song begin to play?",
    "answer": "From in front of them.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01001.mp4",
    "question_id": "01001_5",
    "clip_path": "clips/01001/01001__0006500_0015500.mp4"
  },
  {
    "timestamp": "00:07 - 00:15",
    "context": "[00:07-00:15] The promotional music increases in volume as the person continues walking.",
    "question_type": "Temporal Information",
    "question": "How does the volume of the promotional song change between 00:07 and 00:15?",
    "answer": "It gradually increases.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01001.mp4",
    "question_id": "01001_6",
    "clip_path": "clips/01001/01001__0006500_0015500.mp4"
  },
  {
    "timestamp": "00:07 - 00:15",
    "context": "[00:07-00:15] The person speaks over the music while the rhythmic cane tapping remains audible beneath both.",
    "question_type": "Counting",
    "question": "How many distinct sounds are simultaneously present as the person continues walking in this interval?",
    "answer": "Three—speech, the promotional music, and the cane tapping.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01001.mp4",
    "question_id": "01001_7",
    "clip_path": "clips/01001/01001__0006500_0015500.mp4"
  },
  {
    "timestamp": "00:15 - 00:22",
    "context": "[00:15] Approaching the entrance, the music is now very loud. [00:16] The cane sound shifts as it makes contact with a grooved entrance mat.",
    "question_type": "Sound Characteristics",
    "question": "How does the cane-tapping sound change upon contacting the entrance mat?",
    "answer": "It changes from a sharp click to a duller, more resonant thud.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01001.mp4",
    "question_id": "01001_8",
    "clip_path": "clips/01001/01001__0014500_0022500.mp4"
  },
  {
    "timestamp": "00:15 - 00:22",
    "context": "[00:15-00:22] The cane moves from tiled sidewalk onto a grooved entrance mat.",
    "question_type": "Inferential & Contextual Causality",
    "question": "What caused the change in the cane-tapping sound in this segment?",
    "answer": "The cane tip switched from striking hard tiles to contacting a grooved entrance mat.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01001.mp4",
    "question_id": "01001_9",
    "clip_path": "clips/01001/01001__0014500_0022500.mp4"
  },
  {
    "timestamp": "00:15 - 00:22",
    "context": "[00:15] The promotional music is very loud as the person nears and enters the building.",
    "question_type": "Inferential & Contextual Causality",
    "question": "What guided the person to the building entrance?",
    "answer": "The now very loud promotional music advertising tangyuan.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01001.mp4",
    "question_id": "01001_10",
    "clip_path": "clips/01001/01001__0014500_0022500.mp4"
  },
  {
    "timestamp": "00:15 - 00:22",
    "context": "[00:15-00:22] Clearly audible lyrics: \"If you want to eat tangyuan, come and buy quickly. Eating tangyuan brings happy reunions. Tangyuan, tangyuan, selling tangyuan.\"",
    "question_type": "Counting",
    "question": "In the quoted lyrics, how many times is the word “tangyuan” sung?",
    "answer": "Five times.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01001.mp4",
    "question_id": "01001_11",
    "clip_path": "clips/01001/01001__0014500_0022500.mp4"
  },
  {
    "timestamp": "00:07 - 00:15",
    "context": "[00:07] A promotional song begins to play, featuring a female singer.",
    "question_type": "Sound Source Identification",
    "question": "Who performs the vocals in the promotional song?",
    "answer": "A female singer.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01001.mp4",
    "question_id": "01001_12",
    "clip_path": "clips/01001/01001__0006500_0015500.mp4"
  },
  {
    "timestamp": "00:01 - 00:05",
    "context": "[00:01 - 00:04] The waiter approaches from the front (~1–2 m) and says, \"Here, I'll go get you chopsticks,\" while placing a bowl of noodles on the wooden table, creating a distinct, solid thud. [00:04 - 00:05] The user replies politely, \"Oh, okay, thank you.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the user say, \"Oh, okay, thank you\" at 00:04–00:05?",
    "answer": "To acknowledge the waiter's offer to go get chopsticks.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01003.mp4",
    "question_id": "01003_1",
    "clip_path": "clips/01003/01003__0000500_0005500.mp4"
  },
  {
    "timestamp": "00:01 - 00:04",
    "context": "The waiter carefully places a bowl of noodles onto the wooden table, and this action creates a distinct, solid thud.",
    "question_type": "Sound Source Identification",
    "question": "What generated the distinct, solid thud heard at 00:01–00:04?",
    "answer": "The bowl of noodles being placed onto the wooden table.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01003.mp4",
    "question_id": "01003_2",
    "clip_path": "clips/01003/01003__0000500_0004500.mp4"
  },
  {
    "timestamp": "00:01 - 00:04",
    "context": "As the bowl is placed on the wooden table, it creates a distinct, solid thud.",
    "question_type": "Sound Characteristics",
    "question": "How is the sound made when the bowl was placed described?",
    "answer": "As a distinct, solid thud.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01003.mp4",
    "question_id": "01003_3",
    "clip_path": "clips/01003/01003__0000500_0004500.mp4"
  },
  {
    "timestamp": "00:01 - 00:04",
    "context": "A man, likely the waiter, approaches the user's table from the front, approximately 1–2 meters away, and speaks.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the waiter's speech originate relative to the camera at 00:01–00:04?",
    "answer": "From directly in front, approximately 1–2 meters away.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01003.mp4",
    "question_id": "01003_4",
    "clip_path": "clips/01003/01003__0000500_0004500.mp4"
  },
  {
    "timestamp": "00:01 - 00:04",
    "context": "The waiter speaks in a clear, mid-volume voice: \"Here, I'll go get you chopsticks.\"",
    "question_type": "Sound Characteristics",
    "question": "What were the clarity and volume of the waiter's voice when he first spoke?",
    "answer": "Clear and mid-volume.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01003.mp4",
    "question_id": "01003_5",
    "clip_path": "clips/01003/01003__0000500_0004500.mp4"
  },
  {
    "timestamp": "00:09 - 00:11",
    "context": "The waiter returns to the table, approaches the user, presents a pair of black chopsticks, and says, \"Here.\"",
    "question_type": "Temporal Information",
    "question": "At what time does the waiter return and say \"Here\" while presenting chopsticks?",
    "answer": "Between 00:09 and 00:11.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01003.mp4",
    "question_id": "01003_6",
    "clip_path": "clips/01003/01003__0008500_0011500.mp4"
  },
  {
    "timestamp": "00:09 - 00:11",
    "context": "The waiter returns, says \"Here,\" and hands the user a pair of black chopsticks.",
    "question_type": "Cross-Modal Reasoning",
    "question": "When the waiter says \"Here\" at 00:09–00:11, what action accompanies his words?",
    "answer": "He hands the user a pair of black chopsticks.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01003.mp4",
    "question_id": "01003_7",
    "clip_path": "clips/01003/01003__0008500_0011500.mp4"
  },
  {
    "timestamp": "00:09 - 00:11",
    "context": "The waiter presents a pair of black chopsticks while saying, \"Here.\"",
    "question_type": "Counting",
    "question": "How many chopsticks did the waiter present when he returned?",
    "answer": "A pair—two black chopsticks.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01003.mp4",
    "question_id": "01003_8",
    "clip_path": "clips/01003/01003__0008500_0011500.mp4"
  },
  {
    "timestamp": "00:11 - 00:14",
    "context": "The user accepts the chopsticks, says, \"Oh, good, good,\" and with a small sigh remarks, \"Ah, I haven't eaten this in a long time,\" indicating anticipation.",
    "question_type": "Inferential & Contextual Causality",
    "question": "What was the likely reason for the user's small sigh at 00:11–00:14?",
    "answer": "He was expressing anticipation and looking forward to a meal he hasn't had in a long time.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01003.mp4",
    "question_id": "01003_9",
    "clip_path": "clips/01003/01003__0010500_0014500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "A sharp thud occurs as an object is placed on a wooden table, followed by a loud, rough scraping sound of a wooden stool being dragged across the tiled floor as the operator pulls it out to sit down, concluding with another thud as the operator sits.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the loud, rough scraping sound occur in the opening seconds?",
    "answer": "Because the camera operator was dragging a wooden stool across the tiled floor to pull it out and sit down.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01004.mp4",
    "question_id": "01004_1",
    "clip_path": "clips/01004/01004__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "A sharp thud occurs as an object is placed on a wooden table.",
    "question_type": "Sound Source Identification",
    "question": "What generated the initial sharp thud at the start?",
    "answer": "An object being placed on a wooden table.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01004.mp4",
    "question_id": "01004_2",
    "clip_path": "clips/01004/01004__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "A loud, rough scraping sound of a wooden stool being dragged across the tiled floor is heard.",
    "question_type": "Sound Characteristics",
    "question": "How is the stool-dragging sound described acoustically?",
    "answer": "It is a loud, rough scraping sound.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01004.mp4",
    "question_id": "01004_3",
    "clip_path": "clips/01004/01004__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "The video begins with a series of loud, distinct sounds originating from the camera operator's immediate vicinity.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the opening sounds originate relative to the camera?",
    "answer": "From the camera operator's immediate vicinity.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01004.mp4",
    "question_id": "01004_4",
    "clip_path": "clips/01004/01004__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "Sequence: a thud from placing an object, then loud stool scraping, then another thud as the operator sits.",
    "question_type": "Temporal Information",
    "question": "What was the sequence of sounds in the opening segment?",
    "answer": "First a sharp thud from placing an object, immediately followed by loud stool scraping, then a second thud as the operator sat.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01004.mp4",
    "question_id": "01004_5",
    "clip_path": "clips/01004/01004__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "The sequence concludes with another thud as the operator sits.",
    "question_type": "Sound Source Identification",
    "question": "What caused the second thud at the end of the opening sequence?",
    "answer": "The operator sitting down.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01004.mp4",
    "question_id": "01004_6",
    "clip_path": "clips/01004/01004__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:07 - 00:17",
    "context": "A financial transaction takes place, accompanied by a clear, crisp rustling sound of paper money being counted.",
    "question_type": "Sound Source Identification",
    "question": "What was the source of the clear, crisp rustling sound during the transaction?",
    "answer": "Paper money being counted.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01004.mp4",
    "question_id": "01004_7",
    "clip_path": "clips/01004/01004__0006500_0017500.mp4"
  },
  {
    "timestamp": "00:07 - 00:17",
    "context": "The rustling sound of money is described as clear and crisp.",
    "question_type": "Sound Characteristics",
    "question": "How is the money-counting rustle described?",
    "answer": "Clear and crisp.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01004.mp4",
    "question_id": "01004_8",
    "clip_path": "clips/01004/01004__0006500_0017500.mp4"
  },
  {
    "timestamp": "00:07 - 00:17",
    "context": "During this interval, a man hands cash while counting it aloud and both parties speak.",
    "question_type": "Temporal Information",
    "question": "When does the rustling of paper money occur?",
    "answer": "Between 00:07 and 00:17, during the financial transaction.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01004.mp4",
    "question_id": "01004_9",
    "clip_path": "clips/01004/01004__0006500_0017500.mp4"
  },
  {
    "timestamp": "00:07 - 00:17",
    "context": "P1 clarifies: 'These four bills... four bills make 20... this one is 5... so 25.'",
    "question_type": "Counting",
    "question": "How many bills in total did P1 indicate he handed over?",
    "answer": "Five bills.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01004.mp4",
    "question_id": "01004_10",
    "clip_path": "clips/01004/01004__0006500_0017500.mp4"
  },
  {
    "timestamp": "00:17 - 00:20",
    "context": "After completing the payment and making a parting statement, the man turns and walks away from the table.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the footsteps begin to recede into the background?",
    "answer": "Because the man walked away from the table after concluding the transaction and making his parting statement.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01004.mp4",
    "question_id": "01004_11",
    "clip_path": "clips/01004/01004__0016500_0020500.mp4"
  },
  {
    "timestamp": "00:17 - 00:20",
    "context": "The man turns and walks away from the table, his soft footsteps receding into the background of the restaurant.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the footsteps move relative to the camera?",
    "answer": "They moved away from the table toward the background of the restaurant, receding from the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01004.mp4",
    "question_id": "01004_12",
    "clip_path": "clips/01004/01004__0016500_0020500.mp4"
  },
  {
    "timestamp": "00:17 - 00:20",
    "context": "As the man leaves, his footsteps are described as soft.",
    "question_type": "Sound Characteristics",
    "question": "What is the acoustic character of the departing footsteps?",
    "answer": "They are soft.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01004.mp4",
    "question_id": "01004_13",
    "clip_path": "clips/01004/01004__0016500_0020500.mp4"
  },
  {
    "timestamp": "00:17 - 00:20",
    "context": "P2 responds to the parting statement with '行行行行行行' (Okay repeated).",
    "question_type": "Counting",
    "question": "How many times does P2 repeat '行' (Okay) in the parting response?",
    "answer": "Six times.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01004.mp4",
    "question_id": "01004_14",
    "clip_path": "clips/01004/01004__0016500_0020500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "[00:00 - 00:04] The user says, “今天不扫码了,我给你个现金吧. 搁兜里头容易丢,把它都花了” (I won't scan the code today; I'll give you cash. It's easy to lose it in my pocket, so I'll just spend it.).",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the user choose to pay with cash instead of scanning the code?",
    "answer": "Because he felt cash is easy to lose in his pocket, so he decided to spend it.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01005.mp4",
    "question_id": "01005_1",
    "clip_path": "clips/01005/01005__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:04 - 00:10",
    "context": "[00:04 - 00:10] The user's hand places a red banknote on the counter, producing a faint, soft rustle.",
    "question_type": "Sound Source Identification",
    "question": "What generated the faint, soft rustle heard during 00:04 - 00:10?",
    "answer": "The red banknote being placed on the counter.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01005.mp4",
    "question_id": "01005_2",
    "clip_path": "clips/01005/01005__0003500_0010500.mp4"
  },
  {
    "timestamp": "00:04 - 00:10",
    "context": "[00:04 - 00:10] Placing the banknote on the counter produces a faint, soft rustle.",
    "question_type": "Sound Characteristics",
    "question": "What was the acoustic quality and volume of the rustle produced when the banknote was placed?",
    "answer": "It was a faint, soft rustle.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01005.mp4",
    "question_id": "01005_3",
    "clip_path": "clips/01005/01005__0003500_0010500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "[00:00 - 00:04] The user explains his payment choice in a clear, conversational voice coming directly from the camera's location.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the user's speaking voice originate relative to the camera?",
    "answer": "Directly from the camera's location.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01005.mp4",
    "question_id": "01005_4",
    "clip_path": "clips/01005/01005__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:04 - 00:10",
    "context": "[00:04 - 00:10] As the shopkeeper asks what the user wants, the user places a banknote (creating a rustle). Simultaneously, he places his order: “哎,吃个青菜肉丝面.”",
    "question_type": "Temporal Information",
    "question": "When did the rustle occur, and did it coincide with any other event?",
    "answer": "It occurred during 00:04 - 00:10 and coincided with the user placing his order.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01005.mp4",
    "question_id": "01005_5",
    "clip_path": "clips/01005/01005__0003500_0010500.mp4"
  },
  {
    "timestamp": "00:00 - 00:10",
    "context": "[00:00 - 00:04] The user explains his payment choice. [00:04 - 00:10] He orders noodles and then adds, “可以吧?”.",
    "question_type": "Counting",
    "question": "How many distinct times did the user speak during the clip?",
    "answer": "Three times.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01005.mp4",
    "question_id": "01005_6",
    "clip_path": "clips/01005/01005__0000000_0010500.mp4"
  },
  {
    "timestamp": "00:04 - 00:10",
    "context": "[00:04 - 00:10] After the user orders, the shopkeeper verbally confirms the order.",
    "question_type": "Sound Source Identification",
    "question": "Who verbally confirmed the order?",
    "answer": "The shopkeeper.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01005.mp4",
    "question_id": "01005_7",
    "clip_path": "clips/01005/01005__0003500_0010500.mp4"
  },
  {
    "timestamp": "00:01 - 00:05",
    "context": "[00:01 - 00:05] A man, about 4 meters to the left, walks toward the camera; his footsteps make soft, rhythmic sounds on the paved sidewalk.",
    "question_type": "Sound Source Identification",
    "question": "What produced the soft, rhythmic sounds heard as the man approached?",
    "answer": "The man's footsteps on the paved sidewalk.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01006.mp4",
    "question_id": "01006_1",
    "clip_path": "clips/01006/01006__0000500_0005500.mp4"
  },
  {
    "timestamp": "00:01 - 00:05",
    "context": "[00:01 - 00:05] His footsteps make soft, rhythmic sounds on the paved sidewalk as he walks toward the camera.",
    "question_type": "Sound Characteristics",
    "question": "What are the acoustic qualities of the approaching footsteps?",
    "answer": "They are soft and rhythmic.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01006.mp4",
    "question_id": "01006_2",
    "clip_path": "clips/01006/01006__0000500_0005500.mp4"
  },
  {
    "timestamp": "00:01 - 00:05",
    "context": "[00:01 - 00:05] A man, positioned about 4 meters to the left, walks toward the camera and initiates a conversation as he approaches.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From where relative to the camera did the man's initial speech originate?",
    "answer": "From the left side, starting at about 4 meters away.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01006.mp4",
    "question_id": "01006_3",
    "clip_path": "clips/01006/01006__0000500_0005500.mp4"
  },
  {
    "timestamp": "00:01 - 00:05",
    "context": "[00:01 - 00:05] As he approaches, he initiates a conversation at a clear, conversational volume.",
    "question_type": "Sound Characteristics",
    "question": "What was the volume characteristic of the man's speech when he began talking?",
    "answer": "It was at a clear, conversational volume.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01006.mp4",
    "question_id": "01006_4",
    "clip_path": "clips/01006/01006__0000500_0005500.mp4"
  },
  {
    "timestamp": "00:01 - 00:05",
    "context": "[00:01 - 00:05] The man walks toward the camera, his footsteps making soft, rhythmic sounds throughout the approach.",
    "question_type": "Temporal Information",
    "question": "During 00:01–00:05, were the footsteps momentary or continuous through the approach?",
    "answer": "They were continuous through the approach.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01006.mp4",
    "question_id": "01006_5",
    "clip_path": "clips/01006/01006__0000500_0005500.mp4"
  },
  {
    "timestamp": "00:01 - 00:05",
    "context": "[00:01 - 00:05] The man asks, “这边, 现在吃面条啊?” and the camera wearer replies, then the man says “来来来,” and the camera wearer agrees with “好嘞.”",
    "question_type": "Counting",
    "question": "How many people participated in the conversation during this segment?",
    "answer": "Two people.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01006.mp4",
    "question_id": "01006_6",
    "clip_path": "clips/01006/01006__0000500_0005500.mp4"
  },
  {
    "timestamp": "00:01 - 00:05",
    "context": "[00:01 - 00:05] The man enthusiastically invites the camera wearer to enter, saying, “来来来.”",
    "question_type": "Counting",
    "question": "In the invitation “来来来,” how many times is the word “来” said?",
    "answer": "Three times.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01006.mp4",
    "question_id": "01006_7",
    "clip_path": "clips/01006/01006__0000500_0005500.mp4"
  },
  {
    "timestamp": "00:01 - 00:05",
    "context": "[00:01 - 00:05] The camera wearer responds “嗯对, 吃面条。” and later agrees with “好嘞.”",
    "question_type": "Counting",
    "question": "How many distinct replies does the camera wearer give in this exchange?",
    "answer": "Two replies.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01006.mp4",
    "question_id": "01006_8",
    "clip_path": "clips/01006/01006__0000500_0005500.mp4"
  },
  {
    "timestamp": "00:01 - 00:05",
    "context": "[00:01 - 00:05] The man proposes eating at the noodle shop they are in front of, asking, “这边, 现在吃面条啊?”",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the man propose eating noodles at that moment?",
    "answer": "Because they were standing in front of a noodle shop.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01006.mp4",
    "question_id": "01006_9",
    "clip_path": "clips/01006/01006__0000500_0005500.mp4"
  },
  {
    "timestamp": "00:01 - 00:05",
    "context": "[00:01 - 00:05] After the camera wearer agrees to eat noodles, the man invites with “来来来,” and the camera wearer answers “好嘞.”",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the camera wearer respond “好嘞”?",
    "answer": "He was accepting the man’s invitation to enter the noodle shop to eat.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01006.mp4",
    "question_id": "01006_10",
    "clip_path": "clips/01006/01006__0000500_0005500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "[00:00 - 00:04] A calm, clear male narrator (non-diegetic) says in Chinese, \"When I entered Nanjing, the night had quietly fallen,\" over melancholic background music. Visually, streetlights and building lights are reflected and blurred on the passenger-side window.",
    "question_type": "Cross-Modal Reasoning",
    "question": "Based on the narrator’s line about night having fallen, what visual cues confirm it is nighttime?",
    "answer": "The city’s streetlights and building lights reflected and blurred on the passenger-side window.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01012.mp4",
    "question_id": "01012_1",
    "clip_path": "clips/01012/01012__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "[00:00 - 00:04] A calm, clear male narrator provides context from a non-diegetic source.",
    "question_type": "Sound Source Identification",
    "question": "What is the source of the calm male voice at the beginning?",
    "answer": "A non-diegetic narrator providing context.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01012.mp4",
    "question_id": "01012_2",
    "clip_path": "clips/01012/01012__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "[00:00 - 00:04] Melancholic background music plays under the narration.",
    "question_type": "Sound Characteristics",
    "question": "What is the mood/texture of the background music at the start?",
    "answer": "Melancholic.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01012.mp4",
    "question_id": "01012_3",
    "clip_path": "clips/01012/01012__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:11 - 00:20",
    "context": "[00:11 - 00:20] Narration is abruptly interrupted by a distinct, moderately loud electronic phone ringing. A hand brings a smartphone into view with an illuminated incoming-call screen; the narrator notes, \"my mother called again.\"",
    "question_type": "Sound Source Identification",
    "question": "What object generated the electronic ringing sound that interrupted the narration?",
    "answer": "A smartphone showing an incoming call.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01012.mp4",
    "question_id": "01012_4",
    "clip_path": "clips/01012/01012__0010500_0020500.mp4"
  },
  {
    "timestamp": "00:11 - 00:20",
    "context": "[00:11 - 00:20] The ringing is described as originating directly in front of the camera while the smartphone is held up.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the ringing originate relative to the camera?",
    "answer": "Directly in front of the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01012.mp4",
    "question_id": "01012_5",
    "clip_path": "clips/01012/01012__0010500_0020500.mp4"
  },
  {
    "timestamp": "00:11 - 00:20",
    "context": "[00:11 - 00:20] Narration is abruptly interrupted by a distinct phone ringing; the narrator explains it is a call from his mother.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why was the narration abruptly interrupted during this segment?",
    "answer": "An incoming phone call began ringing—the mother was calling again.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01012.mp4",
    "question_id": "01012_6",
    "clip_path": "clips/01012/01012__0010500_0020500.mp4"
  },
  {
    "timestamp": "00:20 - 00:28",
    "context": "[00:20 - 00:28] The user answers and a live conversation starts. The user's voice is clear; the mother's voice on the phone is muffled and unintelligible, typical of a phone speaker.",
    "question_type": "Sound Characteristics",
    "question": "How is the mother’s voice characterized during the call?",
    "answer": "Muffled and unintelligible, typical of a phone speaker.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01012.mp4",
    "question_id": "01012_7",
    "clip_path": "clips/01012/01012__0019500_0028500.mp4"
  },
  {
    "timestamp": "00:20 - 00:28",
    "context": "[00:20 - 00:28] The user's voice during the call is described as clear and coming from directly in front of the camera.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where does the user’s speaking voice originate relative to the camera during the call?",
    "answer": "Directly in front of the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01012.mp4",
    "question_id": "01012_8",
    "clip_path": "clips/01012/01012__0019500_0028500.mp4"
  },
  {
    "timestamp": "00:28 - 00:33",
    "context": "[00:28 - 00:33] After ending the call, the narrator explains that if it weren't for the other people in the car, he might have started crying, highlighting the emotional weight of his mother's concern.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why does the narrator say he might have started crying if not for the others in the car?",
    "answer": "His mother’s concern affected him emotionally, but he held back tears because other people were present.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01012.mp4",
    "question_id": "01012_9",
    "clip_path": "clips/01012/01012__0027500_0033500.mp4"
  },
  {
    "timestamp": "00:36 - 00:44",
    "context": "[00:36 - 00:44] The user gives directions to the driver: \"...then drive to that intersection... just stop there... Right here, right here, right here.\"",
    "question_type": "Counting",
    "question": "How many times does the user say the phrase \"Right here\"?",
    "answer": "Three times.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01012.mp4",
    "question_id": "01012_10",
    "clip_path": "clips/01012/01012__0035500_0044500.mp4"
  },
  {
    "timestamp": "00:36 - 00:44",
    "context": "[00:36 - 00:44] As the user gives directions including \"Right here, right here, right here,\" the car slows down in response.",
    "question_type": "Cross-Modal Reasoning",
    "question": "What visual change follows the user’s verbal directions to stop?",
    "answer": "The car slows down.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01012.mp4",
    "question_id": "01012_11",
    "clip_path": "clips/01012/01012__0035500_0044500.mp4"
  },
  {
    "timestamp": "00:55 - 00:58",
    "context": "[00:55 - 00:58] The user's hand reaches for the door handle, producing a distinct mechanical click. The interior soundscape immediately changes as ambient city street noise enters.",
    "question_type": "Sound Characteristics",
    "question": "What is the quality of the sound made when the door handle is operated?",
    "answer": "A distinct mechanical click.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01012.mp4",
    "question_id": "01012_12",
    "clip_path": "clips/01012/01012__0054500_0058500.mp4"
  },
  {
    "timestamp": "00:55 - 00:58",
    "context": "[00:55 - 00:58] After the door handle click, the interior soundscape immediately changes as ambient city street noise enters the car.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why does the interior soundscape change immediately after the click?",
    "answer": "Opening the car door allows ambient city street noise to enter.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01012.mp4",
    "question_id": "01012_13",
    "clip_path": "clips/01012/01012__0054500_0058500.mp4"
  },
  {
    "timestamp": "00:58 - 01:03",
    "context": "[00:58 - 01:03] After stepping out, the user closes the car door, which produces a loud, solid thud.",
    "question_type": "Sound Source Identification",
    "question": "What produced the loud, solid thud heard after the user steps out?",
    "answer": "The car door closing.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01012.mp4",
    "question_id": "01012_14",
    "clip_path": "clips/01012/01012__0057500_0062508.mp4"
  },
  {
    "timestamp": "00:00 - 00:10",
    "context": "A continuous male voiceover in Chinese narrates the scene, explaining their situation: \"在服务区里, 由于我们的眼睛不方便...带着我们走\".",
    "question_type": "Temporal Information",
    "question": "When did the male voiceover narration occur, and was it continuous?",
    "answer": "It ran from 00:00 to 00:10 and was continuous.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01015.mp4",
    "question_id": "01015_1",
    "clip_path": "clips/01015/01015__0000000_0010286.mp4"
  },
  {
    "timestamp": "00:00 - 00:10",
    "context": "A continuous male voiceover in Chinese narrates the scene.",
    "question_type": "Sound Source Identification",
    "question": "What was the source of the narration heard between 00:00 and 00:10?",
    "answer": "A male voiceover speaking Chinese.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01015.mp4",
    "question_id": "01015_2",
    "clip_path": "clips/01015/01015__0000000_0010286.mp4"
  },
  {
    "timestamp": "00:04 - 00:06",
    "context": "As the narration continues, a female voice from the front-left says \"走吧?\" (\"Let's go?\"). The caption notes this serves as a prompt for the group to depart.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the female speaker say \"走吧?\" between 00:04 and 00:06?",
    "answer": "To prompt the group to depart from the entrance area where they were standing.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01015.mp4",
    "question_id": "01015_3",
    "clip_path": "clips/01015/01015__0003500_0006500.mp4"
  },
  {
    "timestamp": "00:04 - 00:06",
    "context": "A female voice from the front-left speaks in a clear, mid-volume tone, asking \"走吧?\"",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From what direction relative to the camera did the female voice originate?",
    "answer": "From the front-left.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01015.mp4",
    "question_id": "01015_4",
    "clip_path": "clips/01015/01015__0003500_0006500.mp4"
  },
  {
    "timestamp": "00:04 - 00:06",
    "context": "A female voice speaks in a clear, mid-volume tone, asking \"走吧?\"",
    "question_type": "Sound Characteristics",
    "question": "What was the clarity and volume of the female speaker's \"走吧?\"?",
    "answer": "It was clear and mid-volume.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01015.mp4",
    "question_id": "01015_5",
    "clip_path": "clips/01015/01015__0003500_0006500.mp4"
  },
  {
    "timestamp": "00:04 - 00:06",
    "context": "As the narration continues, a female voice says \"走吧?\"",
    "question_type": "Temporal Information",
    "question": "When did the female voice speak relative to the ongoing narration?",
    "answer": "Between 00:04 and 00:06, while the male narration continued.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01015.mp4",
    "question_id": "01015_6",
    "clip_path": "clips/01015/01015__0003500_0006500.mp4"
  },
  {
    "timestamp": "00:04 - 00:06",
    "context": "As the narration continues, a female voice from the front-left speaks.",
    "question_type": "Counting",
    "question": "How many distinct speakers are heard between 00:04 and 00:06?",
    "answer": "Two—the male narrator and a female speaker.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01015.mp4",
    "question_id": "01015_7",
    "clip_path": "clips/01015/01015__0003500_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:10",
    "context": "The male voiceover narrates: \"在服务区里, 由于我们的眼睛不方便...小丁兄弟都是让我抓着他胳膊, 带着我们走\" and explains their situation.",
    "question_type": "Inferential & Contextual Causality",
    "question": "What was the purpose of the male voiceover during 00:00–00:10?",
    "answer": "To explain their situation in the service area, noting their visual inconvenience and that Ding guides them by letting the speaker hold his arm.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01015.mp4",
    "question_id": "01015_8",
    "clip_path": "clips/01015/01015__0000000_0010286.mp4"
  },
  {
    "timestamp": "00:00 - 00:14",
    "context": "[00:00 - 00:14] The user delivers a continuous monologue directly to the camera, saying: \"This is a service area in Suzhou, Anhui... My friend Liu Zheyuan suggested buying some food and drinks. Another friend, Xiao Ding, was very polite and went to the supermarket for us.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Based on the monologue, why did Xiao Ding go to the supermarket?",
    "answer": "To buy food and drinks for the group, following Liu Zheyuan’s suggestion while they were taking a break.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01016.mp4",
    "question_id": "01016_1",
    "clip_path": "clips/01016/01016__0000000_0014500.mp4"
  },
  {
    "timestamp": "00:00 - 00:14",
    "context": "[00:00 - 00:14] The user is speaking directly to the camera from point-blank range.",
    "question_type": "Sound Source Identification",
    "question": "Who produced the continuous monologue heard during this interval?",
    "answer": "The user speaking directly to the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01016.mp4",
    "question_id": "01016_2",
    "clip_path": "clips/01016/01016__0000000_0014500.mp4"
  },
  {
    "timestamp": "00:00 - 00:14",
    "context": "[00:00 - 00:14] His voice is described as clear and at a conversational volume.",
    "question_type": "Sound Characteristics",
    "question": "How is the speaker’s voice characterized in terms of clarity and volume?",
    "answer": "It is clear and at a conversational volume.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01016.mp4",
    "question_id": "01016_3",
    "clip_path": "clips/01016/01016__0000000_0014500.mp4"
  },
  {
    "timestamp": "00:00 - 00:14",
    "context": "[00:00 - 00:14] The monologue is delivered to the camera from point-blank range.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the speech originate relative to the camera?",
    "answer": "From point-blank range, directly in front of the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01016.mp4",
    "question_id": "01016_4",
    "clip_path": "clips/01016/01016__0000000_0014500.mp4"
  },
  {
    "timestamp": "00:00 - 00:14",
    "context": "[00:00 - 00:14] The user delivers a continuous monologue narrating their situation.",
    "question_type": "Temporal Information",
    "question": "Is the monologue brief or continuous during this interval, and across what time span?",
    "answer": "It is continuous throughout 00:00 to 00:14.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01016.mp4",
    "question_id": "01016_5",
    "clip_path": "clips/01016/01016__0000000_0014500.mp4"
  },
  {
    "timestamp": "00:00 - 00:14",
    "context": "[00:00 - 00:14] The speaker names two friends in his monologue: Liu Zheyuan and Xiao Ding.",
    "question_type": "Counting",
    "question": "How many friends are mentioned by name in the monologue?",
    "answer": "Two—Liu Zheyuan and Xiao Ding.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01016.mp4",
    "question_id": "01016_6",
    "clip_path": "clips/01016/01016__0000000_0014500.mp4"
  },
  {
    "timestamp": "00:00 - 00:34",
    "context": "[00:00 - 00:34] A continuous male voice-over in Chinese explains the group's situation, accompanied by soft background music.",
    "question_type": "Temporal Information",
    "question": "Over what time span is the male voice-over heard, and is it continuous or intermittent?",
    "answer": "It is heard continuously throughout 00:00–00:34.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01017.mp4",
    "question_id": "01017_1",
    "clip_path": "clips/01017/01017__0000000_0033553.mp4"
  },
  {
    "timestamp": "00:00 - 00:34",
    "context": "[00:00 - 00:34] Soft background music accompanies the narration.",
    "question_type": "Sound Characteristics",
    "question": "What is the volume/character of the background music accompanying the narration?",
    "answer": "It is soft background music.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01017.mp4",
    "question_id": "01017_2",
    "clip_path": "clips/01017/01017__0000000_0033553.mp4"
  },
  {
    "timestamp": "00:00 - 00:34",
    "context": "[00:00 - 00:34] A continuous male voice-over in Chinese explains the group's situation.",
    "question_type": "Sound Source Identification",
    "question": "What is the source of the continuous speech heard during the clip?",
    "answer": "A male narrator's voice-over in Chinese.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01017.mp4",
    "question_id": "01017_3",
    "clip_path": "clips/01017/01017__0000000_0033553.mp4"
  },
  {
    "timestamp": "00:00 - 00:34",
    "context": "[00:00 - 00:34] The narration is a voice-over rather than on-screen dialogue.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where does the male voice-over seem to originate relative to the camera?",
    "answer": "It is non-diegetic and centrally mixed, with no specific direction or distance.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01017.mp4",
    "question_id": "01017_4",
    "clip_path": "clips/01017/01017__0000000_0033553.mp4"
  },
  {
    "timestamp": "00:00 - 00:34",
    "context": "[00:00 - 00:34] Soft background music accompanies the narration throughout the segment.",
    "question_type": "Temporal Information",
    "question": "Is the background music brief or continuous during this segment?",
    "answer": "It is continuous throughout 00:00–00:34.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01017.mp4",
    "question_id": "01017_5",
    "clip_path": "clips/01017/01017__0000000_0033553.mp4"
  },
  {
    "timestamp": "00:00 - 00:34",
    "context": "[Narration] \"They discovered that the highway ahead was extremely congested. So, they came up with a plan...\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the group decide to alternate between highway and national roads?",
    "answer": "Because the highway ahead was extremely congested.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01017.mp4",
    "question_id": "01017_6",
    "clip_path": "clips/01017/01017__0000000_0033553.mp4"
  },
  {
    "timestamp": "00:00 - 00:34",
    "context": "[Narration] \"My schoolmate and I were standing by the roadside smoking.\" [Narration] \"Around 12:00 noon, we arrived at a service area and pulled over to rest.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why were the narrator and his schoolmate standing by the roadside smoking?",
    "answer": "They had arrived at a service area and pulled over to rest.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01017.mp4",
    "question_id": "01017_7",
    "clip_path": "clips/01017/01017__0000000_0033553.mp4"
  },
  {
    "timestamp": "00:00 - 00:34",
    "context": "[Visual] Two men stand next to a silver car, intently looking at a phone. [Audio] The narrator says the hitchhiker and the driver were using navigation to check road conditions ahead.",
    "question_type": "Cross-Modal Reasoning",
    "question": "When two men are seen looking at a phone by the silver car, what are they likely doing according to the narration?",
    "answer": "Using the navigation to check the road conditions ahead.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01017.mp4",
    "question_id": "01017_8",
    "clip_path": "clips/01017/01017__0000000_0033553.mp4"
  },
  {
    "timestamp": "00:00 - 00:34",
    "context": "[00:00 - 00:34] Only a single narrator is mentioned speaking; no other voices are described.",
    "question_type": "Counting",
    "question": "How many distinct speakers are heard during this segment?",
    "answer": "One male narrator.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01017.mp4",
    "question_id": "01017_9",
    "clip_path": "clips/01017/01017__0000000_0033553.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "The camera wearer and their white Samoyed dog step onto an upward-moving escalator. To guide the dog, the person gives a command in a clear, calm voice, saying, \"Shàng. Escalator.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the person say, \"Shàng. Escalator.\" at the start of the scene?",
    "answer": "To guide the dog onto and up the escalator.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01021.mp4",
    "question_id": "01021_1",
    "clip_path": "clips/01021/01021__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "An upward-moving escalator emits a continuous, low-pitched mechanical hum and a series of soft clicks.",
    "question_type": "Sound Source Identification",
    "question": "What generated the continuous low-pitched hum and the series of soft clicks?",
    "answer": "The upward-moving escalator.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01021.mp4",
    "question_id": "01021_2",
    "clip_path": "clips/01021/01021__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "The escalator emits a continuous, low-pitched mechanical hum and a series of soft clicks.",
    "question_type": "Sound Characteristics",
    "question": "How are the escalator's sounds described?",
    "answer": "A continuous, low-pitched mechanical hum accompanied by a series of soft clicks.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01021.mp4",
    "question_id": "01021_3",
    "clip_path": "clips/01021/01021__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "To guide the dog, the person gives a command in a clear, calm voice: \"Shàng. Escalator.\"",
    "question_type": "Sound Characteristics",
    "question": "What is the vocal quality of the command, \"Shàng. Escalator.\"?",
    "answer": "It is delivered in a clear, calm voice.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01021.mp4",
    "question_id": "01021_4",
    "clip_path": "clips/01021/01021__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "The escalator emits a continuous, low-pitched mechanical hum as they ride up.",
    "question_type": "Temporal Information",
    "question": "Is the escalator's hum intermittent or continuous during this interval?",
    "answer": "It is continuous.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01021.mp4",
    "question_id": "01021_5",
    "clip_path": "clips/01021/01021__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "The escalator produces a low-pitched mechanical hum and a series of soft clicks.",
    "question_type": "Counting",
    "question": "How many distinct types of escalator sounds are described?",
    "answer": "Two: a mechanical hum and soft clicks.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01021.mp4",
    "question_id": "01021_6",
    "clip_path": "clips/01021/01021__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "As they ride up, the person's left hand gently pats the dog's back in a reassuring gesture. Following this, the person offers praise, saying, \"Good.\"",
    "question_type": "Cross-Modal Reasoning",
    "question": "After the visual action of patting the dog's back, what sound event follows?",
    "answer": "The person says, \"Good.\" as praise.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01021.mp4",
    "question_id": "01021_7",
    "clip_path": "clips/01021/01021__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:00 - 00:08",
    "context": "At a bustling car show, a female staff member standing in front of a white SUV introduces it, her voice clear and coming from the front. As she speaks, a man exits the vehicle, and the car door closes with a soft thud.",
    "question_type": "Sound Characteristics",
    "question": "What was the acoustic quality of the car door closing sound at 00:00–00:08?",
    "answer": "It was a soft thud.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01026.mp4",
    "question_id": "01026_1",
    "clip_path": "clips/01026/01026__0000000_0008500.mp4"
  },
  {
    "timestamp": "00:00 - 00:08",
    "context": "As she speaks, a man exits the vehicle, and the car door closes with a soft thud.",
    "question_type": "Sound Source Identification",
    "question": "What produced the soft thud heard at 00:00–00:08?",
    "answer": "The car door closing as the man exited the vehicle.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01026.mp4",
    "question_id": "01026_2",
    "clip_path": "clips/01026/01026__0000000_0008500.mp4"
  },
  {
    "timestamp": "00:00 - 00:08",
    "context": "A female staff member standing in front of a white SUV introduces it, her voice clear and coming from the front.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From which direction relative to the camera did the staff member's voice originate at the start?",
    "answer": "From the front.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01026.mp4",
    "question_id": "01026_3",
    "clip_path": "clips/01026/01026__0000000_0008500.mp4"
  },
  {
    "timestamp": "00:08 - 00:14",
    "context": "The user asks, \"Can the dog get in?\" In response, a different woman standing by the rear passenger door opens it with a distinct click.",
    "question_type": "Sound Source Identification",
    "question": "What generated the distinct click heard between 00:08 and 00:14?",
    "answer": "The rear passenger door being opened.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01026.mp4",
    "question_id": "01026_4",
    "clip_path": "clips/01026/01026__0007500_0014500.mp4"
  },
  {
    "timestamp": "00:08 - 00:14",
    "context": "The user asks, \"Can the dog get in?\" In response, a different woman standing by the rear passenger door opens it with a distinct click.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the woman open the rear passenger door at 00:08–00:14?",
    "answer": "She opened it in response to the user's question about letting the dog get in.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01026.mp4",
    "question_id": "01026_5",
    "clip_path": "clips/01026/01026__0007500_0014500.mp4"
  },
  {
    "timestamp": "00:14 - 00:23",
    "context": "The dog jumps into the car, and its paws make a soft scuffling sound on the car's floor and seat.",
    "question_type": "Sound Characteristics",
    "question": "How are the dog's paw sounds described as it jumps into the car?",
    "answer": "They are soft scuffling sounds.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01026.mp4",
    "question_id": "01026_6",
    "clip_path": "clips/01026/01026__0013500_0023500.mp4"
  },
  {
    "timestamp": "00:14 - 00:23",
    "context": "The metal parts of its harness jingle faintly.",
    "question_type": "Sound Source Identification",
    "question": "What produced the faint jingling sound when the dog entered?",
    "answer": "The metal parts of the dog's harness.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01026.mp4",
    "question_id": "01026_7",
    "clip_path": "clips/01026/01026__0013500_0023500.mp4"
  },
  {
    "timestamp": "00:14 - 00:23",
    "context": "The staff member guides the dog to the open door, instructing it, \"Get inside. Yuki, up.\" The user echoes the command, \"Up.\"",
    "question_type": "Counting",
    "question": "How many times was the command \"Up\" spoken in this interval?",
    "answer": "Twice—once by the staff member and once by the user.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01026.mp4",
    "question_id": "01026_8",
    "clip_path": "clips/01026/01026__0013500_0023500.mp4"
  },
  {
    "timestamp": "00:23 - 00:33",
    "context": "She then hands the cane back to the user inside the car, which makes a soft tapping sound.",
    "question_type": "Sound Source Identification",
    "question": "What produced the soft tapping sound at 00:23–00:33?",
    "answer": "The user's white cane when it was handed back.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01026.mp4",
    "question_id": "01026_9",
    "clip_path": "clips/01026/01026__0022500_0033500.mp4"
  },
  {
    "timestamp": "00:23 - 00:33",
    "context": "The user gets into the car, causing fabric rustling and shuffling sounds. The staff member outside offers, \"Yuki is in... I'll hold this for you,\" referring to the user's white cane.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the staff member offer to hold the user's white cane?",
    "answer": "To assist the user while they were getting into the car.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01026.mp4",
    "question_id": "01026_10",
    "clip_path": "clips/01026/01026__0022500_0033500.mp4"
  },
  {
    "timestamp": "00:33 - 01:01",
    "context": "While seated in the car's second row with the door still open, the user listens to the staff member who stands just outside. Her voice comes clearly from the left as she describes the vehicle's features.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From which direction did the staff member's voice come during the feature explanation?",
    "answer": "From the left, just outside the car.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01026.mp4",
    "question_id": "01026_11",
    "clip_path": "clips/01026/01026__0032500_0060930.mp4"
  },
  {
    "timestamp": "00:33 - 01:01",
    "context": "Her voice comes clearly from the left as she describes the vehicle's features.",
    "question_type": "Temporal Information",
    "question": "Over what time span did the staff member describe the vehicle's features?",
    "answer": "From 00:33 to 01:01, about 28 seconds.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01026.mp4",
    "question_id": "01026_12",
    "clip_path": "clips/01026/01026__0032500_0060930.mp4"
  },
  {
    "timestamp": "00:00 - 00:14",
    "context": "A car door closes with a soft thud (00:00–00:08). Later, a rear passenger door opens with a distinct click (00:08–00:14).",
    "question_type": "Counting",
    "question": "How many door-related sound events occurred between 00:00 and 00:14?",
    "answer": "Two: one door closing thud and one door opening click.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01026.mp4",
    "question_id": "01026_13",
    "clip_path": "clips/01026/01026__0000000_0014500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "[00:00 - 00:07] From inside a blue-canopied, three-wheeled vehicle, a low, continuous rumbling sound is heard as it slowly reverses into a snowy courtyard.",
    "question_type": "Sound Source Identification",
    "question": "What generated the low, continuous rumbling heard at the beginning?",
    "answer": "The engine of the three-wheeled vehicle reversing into the courtyard.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01031.mp4",
    "question_id": "01031_1",
    "clip_path": "clips/01031/01031__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:07 - 00:16",
    "context": "[00:07 - 00:16] A small black dog appears on the left and begins to bark loudly, aggressively, and continuously. The high-pitched barking is a reaction to the vehicle's arrival.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the small black dog start barking at 00:07–00:16?",
    "answer": "It was reacting to the vehicle's arrival in the courtyard.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01031.mp4",
    "question_id": "01031_2",
    "clip_path": "clips/01031/01031__0006500_0016500.mp4"
  },
  {
    "timestamp": "00:07 - 00:16",
    "context": "[00:07 - 00:16] The dog barks loudly, aggressively, and continuously in a high-pitched tone.",
    "question_type": "Sound Characteristics",
    "question": "What were the acoustic qualities of the dog's barking during this period?",
    "answer": "High-pitched, loud, aggressive, and continuous.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01031.mp4",
    "question_id": "01031_3",
    "clip_path": "clips/01031/01031__0006500_0016500.mp4"
  },
  {
    "timestamp": "00:07 - 00:16",
    "context": "[00:07 - 00:16] The dog is on the left and barks from about 3–4 meters away.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where was the dog's barking coming from relative to the camera?",
    "answer": "From the left, about 3–4 meters away.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01031.mp4",
    "question_id": "01031_4",
    "clip_path": "clips/01031/01031__0006500_0016500.mp4"
  },
  {
    "timestamp": "00:00 - 00:16",
    "context": "[00:00 - 00:16] A low, continuous rumbling is heard as the vehicle reverses; [00:16] The vehicle comes to a stop, ending the rumbling engine sound.",
    "question_type": "Temporal Information",
    "question": "When did the engine rumble end, and over what interval was it heard?",
    "answer": "It was heard continuously from 00:00 until it ended at 00:16 when the vehicle stopped.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01031.mp4",
    "question_id": "01031_5",
    "clip_path": "clips/01031/01031__0000000_0016500.mp4"
  },
  {
    "timestamp": "00:07 - 00:16",
    "context": "[00:07 - 00:16] Another, more distant male voice calls out something unintelligible, likely trying to quiet the dog.",
    "question_type": "Inferential & Contextual Causality",
    "question": "What was the likely reason the distant male voice called out during the barking?",
    "answer": "He was likely trying to quiet the dog.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01031.mp4",
    "question_id": "01031_6",
    "clip_path": "clips/01031/01031__0006500_0016500.mp4"
  },
  {
    "timestamp": "00:16 - 00:23",
    "context": "[00:16 - 00:23] The dog's barking subsides to a few intermittent yaps. The speaker sighs and says, \"after half a year, it doesn't even recognize me anymore.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the speaker sigh and express disappointment after the barking subsided?",
    "answer": "Because after half a year, the dog no longer recognized him.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01031.mp4",
    "question_id": "01031_7",
    "clip_path": "clips/01031/01031__0015500_0023500.mp4"
  },
  {
    "timestamp": "00:16 - 00:23",
    "context": "[00:16 - 00:23] The vehicle stops, and the dog's barking subsides to a few intermittent yaps.",
    "question_type": "Temporal Information",
    "question": "How did the dog's barking change once the vehicle stopped?",
    "answer": "It decreased to a few intermittent yaps.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01031.mp4",
    "question_id": "01031_8",
    "clip_path": "clips/01031/01031__0015500_0023500.mp4"
  },
  {
    "timestamp": "00:23 - 00:28",
    "context": "[00:23 - 00:28] The older man continues talking from off-screen, his voice muffled.",
    "question_type": "Sound Characteristics",
    "question": "How is the older man's off-screen speech acoustically described?",
    "answer": "Muffled.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01031.mp4",
    "question_id": "01031_9",
    "clip_path": "clips/01031/01031__0022500_0028352.mp4"
  },
  {
    "timestamp": "00:07 - 00:16",
    "context": "[00:07 - 00:16] The speaker says, \"barking like crazy, barking like crazy.\"",
    "question_type": "Counting",
    "question": "How many times did the speaker repeat the phrase \"barking like crazy\"?",
    "answer": "Twice.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01031.mp4",
    "question_id": "01031_10",
    "clip_path": "clips/01031/01031__0006500_0016500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "[00:00 - 00:07] A male speaker, originating from the camera's perspective, narrates the event.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From where did the primary narrator's voice originate relative to the camera?",
    "answer": "From the camera's perspective, i.e., directly at the camera's position.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01031.mp4",
    "question_id": "01031_11",
    "clip_path": "clips/01031/01031__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:07 - 00:16",
    "context": "[00:07 - 00:16] The dog barks continuously until the vehicle stops.",
    "question_type": "Temporal Information",
    "question": "Approximately how long did the dog's continuous barking persist before subsiding?",
    "answer": "About 9 seconds, from 00:07 to 00:16.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01031.mp4",
    "question_id": "01031_12",
    "clip_path": "clips/01031/01031__0006500_0016500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "The individual (camera wearer) speaks clearly and close-range: \"So, I used that button-style input device to enter the verification code.\" Their hands are over a tablet and a physical keypad on the counter.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the individual state that they used the button-style input device at the start?",
    "answer": "To explain that they had entered the verification code using the hardware keypad.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01037.mp4",
    "question_id": "01037_1",
    "clip_path": "clips/01037/01037__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "A clear, close-range monologue is heard from the person whose perspective the video is from.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the monologue originate relative to the camera?",
    "answer": "From very close to the camera, spoken by the person whose perspective the video represents.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01037.mp4",
    "question_id": "01037_2",
    "clip_path": "clips/01037/01037__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:07 - 00:09",
    "context": "The bank teller speaks via intercom from the front: \"Oh, okay, okay, okay, it's done, it's done.\"",
    "question_type": "Counting",
    "question": "How many times does the teller say the word \"okay\" in her confirmation?",
    "answer": "Three times.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01037.mp4",
    "question_id": "01037_3",
    "clip_path": "clips/01037/01037__0006500_0009500.mp4"
  },
  {
    "timestamp": "00:07 - 00:09",
    "context": "The teller confirms through an intercom with: \"Oh, okay, okay, okay, it's done, it's done.\"",
    "question_type": "Sound Characteristics",
    "question": "What was the tone of the teller’s confirmation?",
    "answer": "Friendly.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01037.mp4",
    "question_id": "01037_4",
    "clip_path": "clips/01037/01037__0006500_0009500.mp4"
  },
  {
    "timestamp": "00:07 - 00:09",
    "context": "The teller’s voice is heard coming from the front through an intercom system.",
    "question_type": "Sound Source Identification",
    "question": "What system carried the teller’s voice to the listener?",
    "answer": "An intercom system.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01037.mp4",
    "question_id": "01037_5",
    "clip_path": "clips/01037/01037__0006500_0009500.mp4"
  },
  {
    "timestamp": "00:10 - 00:12",
    "context": "An automated female voice prompt plays: \"Please check the agreement information and confirm.\"",
    "question_type": "Sound Source Identification",
    "question": "What generated the instruction to check and confirm the agreement information?",
    "answer": "An automated female voice from the bank’s system.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01037.mp4",
    "question_id": "01037_6",
    "clip_path": "clips/01037/01037__0009500_0012500.mp4"
  },
  {
    "timestamp": "00:10 - 00:12",
    "context": "An automated prompt is broadcast between 00:10 and 00:12.",
    "question_type": "Temporal Information",
    "question": "When did the automated prompt occur and approximately how long did it last?",
    "answer": "Between 00:10 and 00:12, lasting about 2 seconds.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01037.mp4",
    "question_id": "01037_7",
    "clip_path": "clips/01037/01037__0009500_0012500.mp4"
  },
  {
    "timestamp": "00:13 - 00:30",
    "context": "While critiquing the design, the user presses several buttons on the white physical keypad, producing a series of electronic beeps.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why were electronic beeps heard during the user's critique?",
    "answer": "Because the user was pressing buttons on the physical keypad attempting to confirm via the hardware device.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01037.mp4",
    "question_id": "01037_8",
    "clip_path": "clips/01037/01037__0012500_0030500.mp4"
  },
  {
    "timestamp": "00:13 - 00:30",
    "context": "Pressing the keypad produces a series of soft, distinct electronic beeps.",
    "question_type": "Sound Characteristics",
    "question": "How are the keypad beeps described?",
    "answer": "As a series of soft, distinct electronic beeps.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01037.mp4",
    "question_id": "01037_9",
    "clip_path": "clips/01037/01037__0012500_0030500.mp4"
  },
  {
    "timestamp": "00:13 - 00:30",
    "context": "The user's left hand presses buttons on the white physical keypad on the counter while speaking.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the beeps originate relative to the camera?",
    "answer": "From the physical keypad on the counter directly in front of the user.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01037.mp4",
    "question_id": "01037_10",
    "clip_path": "clips/01037/01037__0012500_0030500.mp4"
  },
  {
    "timestamp": "00:30 - 00:40",
    "context": "The teller interjects from the front: \"Some can, some can't,\" and instructs, \"Press the confirmation key,\" while the user struggles to confirm.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the teller instruct, \"Press the confirmation key\"?",
    "answer": "To help resolve the user's difficulty confirming via the keypad after their attempt was not successful.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01037.mp4",
    "question_id": "01037_11",
    "clip_path": "clips/01037/01037__0029500_0040500.mp4"
  },
  {
    "timestamp": "00:41 - 00:47",
    "context": "The user says a staff member was called and clicked the on-screen confirmation; visually, the user's hands remain idle on the counter.",
    "question_type": "Cross-Modal Reasoning",
    "question": "After the user explains that staff clicked the on-screen confirmation, what does the visual state of the hands indicate about who performed the confirmation?",
    "answer": "The hands remain idle on the counter, indicating the staff member used the touchscreen rather than the user confirming via the keypad.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01037.mp4",
    "question_id": "01037_12",
    "clip_path": "clips/01037/01037__0040500_0047500.mp4"
  },
  {
    "timestamp": "00:00 - 00:17",
    "context": "[00:00 - 00:17] A distinct, rhythmic, moderately loud creaking from the boat's single oar, with gentle water splashing, prompts a conversation between two observers very close to the camera. Person 1 asks, “Did you hear the sound of the water?”",
    "question_type": "Inferential & Contextual Causality",
    "question": "What prompted the two observers to start conversing at the beginning of the video?",
    "answer": "The distinct, rhythmic creaking of the single oar and the accompanying gentle water splashing prompted their conversation.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01047.mp4",
    "question_id": "01047_1",
    "clip_path": "clips/01047/01047__0000000_0017500.mp4"
  },
  {
    "timestamp": "00:00 - 00:17",
    "context": "[00:00 - 00:17] A distinct, rhythmic, moderately loud creaking sound emanates from the boat's single oar as the boatman propels it forward.",
    "question_type": "Sound Source Identification",
    "question": "What generated the distinct rhythmic creaking heard early in the video?",
    "answer": "The boat’s single oar.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01047.mp4",
    "question_id": "01047_2",
    "clip_path": "clips/01047/01047__0000000_0017500.mp4"
  },
  {
    "timestamp": "00:00 - 00:17",
    "context": "[00:00 - 00:17] The boat’s single oar produces a distinct, rhythmic, moderately loud creaking, accompanied by gentle splashing of water.",
    "question_type": "Sound Characteristics",
    "question": "How is the oar’s sound described in terms of rhythm and loudness?",
    "answer": "It is distinct, rhythmic, and moderately loud.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01047.mp4",
    "question_id": "01047_3",
    "clip_path": "clips/01047/01047__0000000_0017500.mp4"
  },
  {
    "timestamp": "00:00 - 00:17",
    "context": "[00:00 - 00:17] The sound prompts a conversation between two observers, who are located very close to the camera.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where are the speakers located relative to the camera?",
    "answer": "Very close to the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01047.mp4",
    "question_id": "01047_4",
    "clip_path": "clips/01047/01047__0000000_0017500.mp4"
  },
  {
    "timestamp": "00:22 - 00:27",
    "context": "[00:22 - 00:27] As the boat continues to glide down the canal on the right, the rhythmic creaking of the oar remains the dominant environmental sound.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From which side relative to the camera does the dominant creaking originate during this interval?",
    "answer": "From the right side, where the boat is gliding down the canal.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01047.mp4",
    "question_id": "01047_5",
    "clip_path": "clips/01047/01047__0021500_0027500.mp4"
  },
  {
    "timestamp": "00:22 - 00:27",
    "context": "[00:22 - 00:27] The rhythmic creaking of the oar remains the dominant environmental sound as the boat continues.",
    "question_type": "Temporal Information",
    "question": "During 00:22–00:27, is the rhythmic creaking brief or ongoing?",
    "answer": "It is ongoing and remains dominant.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01047.mp4",
    "question_id": "01047_6",
    "clip_path": "clips/01047/01047__0021500_0027500.mp4"
  },
  {
    "timestamp": "00:22 - 00:27",
    "context": "[00:22 - 00:27] Person 2 remarks on the sound's quality: “Look, it’s very rhythmic.”",
    "question_type": "Temporal Information",
    "question": "When does Person 2 remark that the sound is very rhythmic?",
    "answer": "Between 00:22 and 00:27.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01047.mp4",
    "question_id": "01047_7",
    "clip_path": "clips/01047/01047__0021500_0027500.mp4"
  },
  {
    "timestamp": "00:00 - 00:17",
    "context": "[00:00 - 00:17] The oar’s creaking is accompanied by the gentle splashing of water.",
    "question_type": "Counting",
    "question": "How many distinct non-speech environmental sounds are mentioned at the start?",
    "answer": "Two: the oar’s rhythmic creaking and the gentle splashing of water.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01047.mp4",
    "question_id": "01047_8",
    "clip_path": "clips/01047/01047__0000000_0017500.mp4"
  },
  {
    "timestamp": "00:00 - 00:17",
    "context": "[00:00 - 00:17] Person 2 explains the mechanics: “It’s like a scull... He uses just one to shake... using the tail to control the direction,” while the creaking and splashing are heard as the boatman propels the boat.",
    "question_type": "Cross-Modal Reasoning",
    "question": "Based on the audio and Person 2’s explanation, what is the boatman doing?",
    "answer": "Rowing the traditional wooden boat with a single oar like a scull and using the tail to control direction.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01047.mp4",
    "question_id": "01047_9",
    "clip_path": "clips/01047/01047__0000000_0017500.mp4"
  },
  {
    "timestamp": "00:22 - 00:27",
    "context": "[00:22 - 00:27] Person 2 says, “Look, it’s very rhythmic.” Person 1 agrees and adds, “Yes, even from far away, I could hear the sound of the boat being rowed.”",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did Person 1 mention they could hear the boat from far away?",
    "answer": "To agree with Person 2’s comment about the sound’s rhythmic quality and emphasize how well it carries over distance.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01047.mp4",
    "question_id": "01047_10",
    "clip_path": "clips/01047/01047__0021500_0027500.mp4"
  },
  {
    "timestamp": "00:00 - 00:12",
    "context": "A conversation begins nearby. Female: “两边哈，这边都是桑树。桑树是一年四季的乔木…它那个是一年四季都是绿的.” Male responds with interest: “哦 (Oh).”",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the male speaker say “哦 (Oh)” during 00:00 - 00:12?",
    "answer": "He was showing interest and acknowledgement in response to the female’s explanation that the mulberry trees are green all year.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01049.mp4",
    "question_id": "01049_1",
    "clip_path": "clips/01049/01049__0000000_0012500.mp4"
  },
  {
    "timestamp": "00:00 - 00:12",
    "context": "Female speaker explains in Mandarin about the trees: “两边哈，这边都是桑树…它那个是一年四季都是绿的.”",
    "question_type": "Sound Source Identification",
    "question": "Who stated that the trees on both sides are mulberry trees and green all four seasons?",
    "answer": "The female speaker.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01049.mp4",
    "question_id": "01049_2",
    "clip_path": "clips/01049/01049__0000000_0012500.mp4"
  },
  {
    "timestamp": "00:00 - 00:12",
    "context": "As the camera pans across the canal, “a conversation begins between a man and a woman who are nearby.”",
    "question_type": "Temporal Information",
    "question": "When did the conversation between the two speakers begin?",
    "answer": "At the start of the video, around 00:00.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01049.mp4",
    "question_id": "01049_3",
    "clip_path": "clips/01049/01049__0000000_0012500.mp4"
  },
  {
    "timestamp": "00:00 - 00:12",
    "context": "“A conversation begins between a man and a woman who are nearby.”",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "What was the spatial distance of the speakers relative to the camera when the conversation began?",
    "answer": "They were nearby the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01049.mp4",
    "question_id": "01049_4",
    "clip_path": "clips/01049/01049__0000000_0012500.mp4"
  },
  {
    "timestamp": "00:00 - 00:18",
    "context": "Male responses: [00:00 - 00:12] “哦 (Oh).” [00:13 - 00:18] “对 (Right).”",
    "question_type": "Counting",
    "question": "How many brief verbal acknowledgements did the male speaker make between 00:00 and 00:18?",
    "answer": "Two: “哦” and “对.”",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01049.mp4",
    "question_id": "01049_5",
    "clip_path": "clips/01049/01049__0000000_0018500.mp4"
  },
  {
    "timestamp": "00:13 - 00:18",
    "context": "Female: “它的河道也是专门有人管理的…清理呀什么东西都有人.” Male confirms: “对 (Right).”",
    "question_type": "Sound Source Identification",
    "question": "Who said “对 (Right)” during the discussion of river management?",
    "answer": "The male speaker.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01049.mp4",
    "question_id": "01049_6",
    "clip_path": "clips/01049/01049__0012500_0018500.mp4"
  },
  {
    "timestamp": "00:13 - 00:23",
    "context": "[00:13 - 00:18] Female explains the river is specially managed and cleaned. [00:18 - 00:23] Male agrees and adds: “它这种河道维护的还是不错的 (The maintenance of this kind of waterway is quite good).”",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the male speaker comment that the waterway maintenance is quite good at 00:18 - 00:23?",
    "answer": "He was agreeing with and reinforcing the female’s prior observation that the river is well managed and cleaned.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01049.mp4",
    "question_id": "01049_7",
    "clip_path": "clips/01049/01049__0012500_0023500.mp4"
  },
  {
    "timestamp": "00:18 - 00:23",
    "context": "“The male speaker agrees with his companion's observation about the upkeep. He comments in a moderate tone: ‘它这种河道维护的还是不错的.’”",
    "question_type": "Sound Characteristics",
    "question": "What was the tone or volume characteristic of the male speaker’s maintenance comment?",
    "answer": "He spoke in a moderate tone.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01049.mp4",
    "question_id": "01049_8",
    "clip_path": "clips/01049/01049__0017500_0023500.mp4"
  },
  {
    "timestamp": "00:18 - 00:23",
    "context": "As the camera frames the canal, the male says: “它这种河道维护的还是不错的.”",
    "question_type": "Temporal Information",
    "question": "When did the male speaker say that the waterway maintenance is quite good?",
    "answer": "Between 00:18 and 00:23.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01049.mp4",
    "question_id": "01049_9",
    "clip_path": "clips/01049/01049__0017500_0023500.mp4"
  },
  {
    "timestamp": "00:13 - 00:18",
    "context": "Female continues: “它的河道也是专门有人管理的…清理呀什么东西都有人.”",
    "question_type": "Temporal Information",
    "question": "During which time interval did the female continue her explanation about river management and cleaning?",
    "answer": "00:13 - 00:18.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01049.mp4",
    "question_id": "01049_10",
    "clip_path": "clips/01049/01049__0012500_0018500.mp4"
  },
  {
    "timestamp": "00:00 - 00:05",
    "context": "As the camera moves through a crowded, traditional-style street, a continuous, faint, high-pitched chiming is heard from a shop on the right. A male identifies it as wind chimes and a female immediately agrees.",
    "question_type": "Sound Characteristics",
    "question": "What were the acoustic characteristics (volume and pitch) of the chiming sound heard at the start?",
    "answer": "It was continuous, faint, and high-pitched.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01050.mp4",
    "question_id": "01050_1",
    "clip_path": "clips/01050/01050__0000000_0005500.mp4"
  },
  {
    "timestamp": "00:00 - 00:05",
    "context": "A male speaker identifies the chiming sound as wind chimes, and a female speaker immediately agrees.",
    "question_type": "Sound Source Identification",
    "question": "What specific object generated the chiming sound?",
    "answer": "Wind chimes from the shop on the right.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01050.mp4",
    "question_id": "01050_2",
    "clip_path": "clips/01050/01050__0000000_0005500.mp4"
  },
  {
    "timestamp": "00:00 - 00:05",
    "context": "The chiming is heard while approaching a shop on the right. The male then says it's right there and the female confirms the shop as the source.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the chiming originate relative to the camera?",
    "answer": "From the shop on the right side as they approached it.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01050.mp4",
    "question_id": "01050_3",
    "clip_path": "clips/01050/01050__0000000_0005500.mp4"
  },
  {
    "timestamp": "00:00 - 00:05",
    "context": "A continuous chiming persists throughout the movement along the street.",
    "question_type": "Temporal Information",
    "question": "Was the chiming brief or continuous during this interval?",
    "answer": "It was continuous across 00:00–00:05.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01050.mp4",
    "question_id": "01050_4",
    "clip_path": "clips/01050/01050__0000000_0005500.mp4"
  },
  {
    "timestamp": "00:00 - 00:05",
    "context": "After identifying the chiming as wind chimes, the male speaker says it's right there, and the female confirms the shop as the source.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the male speaker say it's right there?",
    "answer": "He visually located the source of the wind chime sound and pointed out the specific shop on the right they were approaching.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01050.mp4",
    "question_id": "01050_5",
    "clip_path": "clips/01050/01050__0000000_0005500.mp4"
  },
  {
    "timestamp": "00:00 - 00:05",
    "context": "A male identifies the sound as wind chimes, and a female immediately agrees.",
    "question_type": "Counting",
    "question": "How many speakers verbally commented on or confirmed the chiming sound in this segment?",
    "answer": "Two speakers: one male and one female.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01050.mp4",
    "question_id": "01050_6",
    "clip_path": "clips/01050/01050__0000000_0005500.mp4"
  },
  {
    "timestamp": "00:06 - 00:09",
    "context": "While walking past the shop entrance on the right, the female reads the sign with auspicious phrases that identify the shop as selling lucky charms and correspond to the earlier wind chime sounds.",
    "question_type": "Cross-Modal Reasoning",
    "question": "What does the sign the female reads suggest about the kind of shop producing the earlier wind chime sounds?",
    "answer": "It indicates a lucky charms shop, aligning with the auspicious phrases and the wind chimes heard from that shop.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01050.mp4",
    "question_id": "01050_7",
    "clip_path": "clips/01050/01050__0005500_0009233.mp4"
  },
  {
    "timestamp": "00:06 - 00:09",
    "context": "The female reads the large characters on the sign while walking past the shop entrance on the right.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where was the speech event of reading the sign located relative to the camera?",
    "answer": "At the shop entrance on the right as they walked past it.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01050.mp4",
    "question_id": "01050_8",
    "clip_path": "clips/01050/01050__0005500_0009233.mp4"
  },
  {
    "timestamp": "00:06 - 00:09",
    "context": "The female reads the sign with auspicious phrases, which serves to identify the type of shop.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the female read the large characters on the sign aloud?",
    "answer": "To identify the type of shop, which sells lucky charms.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01050.mp4",
    "question_id": "01050_9",
    "clip_path": "clips/01050/01050__0005500_0009233.mp4"
  },
  {
    "timestamp": "00:00 - 00:05",
    "context": "The user holds and manipulates a clear plastic zip-lock bag, producing a continuous, sharp crinkling sound directly in front of the camera. Simultaneously, a clear, close-up male voice in Mandarin says: \"Alpha, bring you toothpaste... toothbrush. He's responsible for eating, and I'm responsible for brushing his teeth every day.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Based on the spoken explanation, why is the user handling the items in the zip-lock bag?",
    "answer": "He is preparing dental hygiene items for Alpha and indicates he will brush Alpha’s teeth every day.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01061.mp4",
    "question_id": "01061_1",
    "clip_path": "clips/01061/01061__0000000_0005267.mp4"
  },
  {
    "timestamp": "00:00 - 00:05",
    "context": "The user manipulates a clear plastic zip-lock bag, which produces a continuous, sharp crinkling sound directly in front of the camera.",
    "question_type": "Sound Source Identification",
    "question": "What generated the continuous, sharp crinkling sound?",
    "answer": "The clear plastic zip-lock bag being held and manipulated by the user.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01061.mp4",
    "question_id": "01061_2",
    "clip_path": "clips/01061/01061__0000000_0005267.mp4"
  },
  {
    "timestamp": "00:00 - 00:05",
    "context": "Manipulation of the plastic bag produces a continuous, sharp crinkling sound.",
    "question_type": "Sound Characteristics",
    "question": "How is the crinkling sound characterized?",
    "answer": "It is continuous and sharp.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01061.mp4",
    "question_id": "01061_3",
    "clip_path": "clips/01061/01061__0000000_0005267.mp4"
  },
  {
    "timestamp": "00:00 - 00:05",
    "context": "The crinkling from the plastic zip-lock bag is heard directly in front of the camera.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the crinkling sound originate relative to the camera?",
    "answer": "Directly in front of the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01061.mp4",
    "question_id": "01061_4",
    "clip_path": "clips/01061/01061__0000000_0005267.mp4"
  },
  {
    "timestamp": "00:00 - 00:05",
    "context": "A continuous crinkling sound persists while the bag is manipulated during the 5-second interval.",
    "question_type": "Temporal Information",
    "question": "Is the crinkling sound brief or continuous over this interval?",
    "answer": "It is continuous throughout 00:00–00:05.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01061.mp4",
    "question_id": "01061_5",
    "clip_path": "clips/01061/01061__0000000_0005267.mp4"
  },
  {
    "timestamp": "00:00 - 00:05",
    "context": "Simultaneously with the crinkling, a clear, close-up male voice speaks in Mandarin.",
    "question_type": "Sound Characteristics",
    "question": "How is the speaker’s voice described?",
    "answer": "A clear, close-up male voice.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01061.mp4",
    "question_id": "01061_6",
    "clip_path": "clips/01061/01061__0000000_0005267.mp4"
  },
  {
    "timestamp": "00:00 - 00:05",
    "context": "The voice speaking in Mandarin occurs while the crinkling sound from the bag is heard.",
    "question_type": "Temporal Information",
    "question": "When does the speech occur relative to the crinkling sound?",
    "answer": "It occurs simultaneously during 00:00–00:05.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01061.mp4",
    "question_id": "01061_7",
    "clip_path": "clips/01061/01061__0000000_0005267.mp4"
  },
  {
    "timestamp": "00:00 - 00:05",
    "context": "The speaker says: \"Alpha, bring you toothpaste... toothbrush.\"",
    "question_type": "Counting",
    "question": "How many dental hygiene items are explicitly mentioned in the speech?",
    "answer": "Two: toothpaste and toothbrush.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01061.mp4",
    "question_id": "01061_8",
    "clip_path": "clips/01061/01061__0000000_0005267.mp4"
  },
  {
    "timestamp": "00:00 - 00:05",
    "context": "Two audio elements are present: the bag’s crinkling and the male speech.",
    "question_type": "Counting",
    "question": "How many distinct sound sources are heard simultaneously?",
    "answer": "Two: the plastic bag crinkling and the male voice.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01061.mp4",
    "question_id": "01061_9",
    "clip_path": "clips/01061/01061__0000000_0005267.mp4"
  },
  {
    "timestamp": "00:00 - 00:05",
    "context": "Visually, the user holds a zip-lock bag with a light blue object. Audibly, the speaker mentions toothpaste and a toothbrush for Alpha.",
    "question_type": "Cross-Modal Reasoning",
    "question": "Based on the speech, what is the likely purpose or contents of the zip-lock bag the user is manipulating?",
    "answer": "It likely contains dental hygiene items being prepared for Alpha, such as toothpaste and/or a toothbrush.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01061.mp4",
    "question_id": "01061_10",
    "clip_path": "clips/01061/01061__0000000_0005267.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "[00:00 - 00:03] The camera holder carries a large, clear plastic bag filled with snacks, producing a constant, low-volume rustling directly in front of the camera. A calm instrumental track plays. A pre-recorded male voiceover says: “This is Baoge's first-person perspective as a completely blind person. Please forgive the poor filming.”",
    "question_type": "Sound Source Identification",
    "question": "What generated the constant rustling sound at the start of the video?",
    "answer": "The large, clear plastic bag filled with snacks being held in front of the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01062.mp4",
    "question_id": "01062_1",
    "clip_path": "clips/01062/01062__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "[00:00 - 00:03] The snack bag produces a constant, low-volume rustling directly in front of the camera.",
    "question_type": "Sound Characteristics",
    "question": "How is the rustling from the snack bag characterized in terms of volume and continuity?",
    "answer": "It is constant and low-volume.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01062.mp4",
    "question_id": "01062_2",
    "clip_path": "clips/01062/01062__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "[00:00 - 00:03] The rustling from the snack bag originates directly in front of the camera.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where does the rustling from the snack bag originate relative to the camera?",
    "answer": "Directly in front of the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01062.mp4",
    "question_id": "01062_3",
    "clip_path": "clips/01062/01062__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "[00:00 - 00:03] A pre-recorded male voiceover says: “This is Baoge's first-person perspective as a completely blind person. Please forgive the poor filming,” providing context for the video.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why does the male voiceover ask viewers to forgive the poor filming?",
    "answer": "Because the user is completely blind and is attempting to film from a first-person perspective.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01062.mp4",
    "question_id": "01062_4",
    "clip_path": "clips/01062/01062__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:00 - 00:05",
    "context": "[00:00 - 00:03] A pre-recorded male voiceover gives context. [00:03 - 00:05] A second male voiceover asks for help evaluating the packing.",
    "question_type": "Counting",
    "question": "How many male voiceovers are heard between 00:00 and 00:05?",
    "answer": "Two.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01062.mp4",
    "question_id": "01062_5",
    "clip_path": "clips/01062/01062__0000000_0005500.mp4"
  },
  {
    "timestamp": "00:03 - 00:05",
    "context": "[00:03 - 00:05] The user kneels at an open gray suitcase. Touching the mesh divider with the left hand produces a soft thud followed by fabric rustling.",
    "question_type": "Sound Source Identification",
    "question": "What action produced the soft thud followed by rustling during 00:03 - 00:05?",
    "answer": "The user's left hand touching the mesh divider inside the suitcase.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01062.mp4",
    "question_id": "01062_6",
    "clip_path": "clips/01062/01062__0002500_0005500.mp4"
  },
  {
    "timestamp": "00:03 - 00:05",
    "context": "[00:03 - 00:05] Touching the mesh divider yields a soft thud, then fabric rustling.",
    "question_type": "Sound Characteristics",
    "question": "What are the qualities of the sounds made when the mesh divider was touched?",
    "answer": "A soft thud followed by rustling from the fabric.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01062.mp4",
    "question_id": "01062_7",
    "clip_path": "clips/01062/01062__0002500_0005500.mp4"
  },
  {
    "timestamp": "00:03 - 00:05",
    "context": "[00:03 - 00:05] Touching the mesh divider produces a soft thud followed by rustling.",
    "question_type": "Temporal Information",
    "question": "In what order did the sounds occur when the mesh divider was touched?",
    "answer": "A soft thud first, followed by rustling.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01062.mp4",
    "question_id": "01062_8",
    "clip_path": "clips/01062/01062__0002500_0005500.mp4"
  },
  {
    "timestamp": "00:03 - 00:05",
    "context": "[00:03 - 00:05] A second male voiceover asks: “Everyone, help me see if this is packed okay,” indicating the purpose of the action.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the second male voiceover ask for help to see if it was packed okay?",
    "answer": "Because the user was packing the suitcase and seeking feedback on the packing.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01062.mp4",
    "question_id": "01062_9",
    "clip_path": "clips/01062/01062__0002500_0005500.mp4"
  },
  {
    "timestamp": "00:05 - 00:08",
    "context": "[00:05 - 00:08] The user places a bag of snacks and a green fabric bag into the right compartment of the suitcase, generating a loud, distinct series of crinkling and rustling sounds.",
    "question_type": "Counting",
    "question": "How many items were placed into the suitcase that caused the loud crinkling and rustling?",
    "answer": "Two items: a bag of snacks and a green fabric bag.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01062.mp4",
    "question_id": "01062_10",
    "clip_path": "clips/01062/01062__0004500_0008500.mp4"
  },
  {
    "timestamp": "00:05 - 00:08",
    "context": "[00:05 - 00:08] Placing the items produces a loud, distinct, sharp series of crinkling and rustling directly in front of the camera.",
    "question_type": "Sound Characteristics",
    "question": "How are the sounds described when the items were pushed into place?",
    "answer": "Loud, distinct, and sharp crinkling and rustling.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01062.mp4",
    "question_id": "01062_11",
    "clip_path": "clips/01062/01062__0004500_0008500.mp4"
  },
  {
    "timestamp": "00:05 - 00:08",
    "context": "[00:05 - 00:08] The sound originates from the user's hands interacting with the objects directly in front of the camera.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the crinkling and rustling originate relative to the camera?",
    "answer": "From the user's hands directly in front of the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01062.mp4",
    "question_id": "01062_12",
    "clip_path": "clips/01062/01062__0004500_0008500.mp4"
  },
  {
    "timestamp": "00:08 - 00:09",
    "context": "[00:08 - 00:09] After placing the items, the user stands up and walks away. The primary sound is continuous background music, with faint rustling of clothes.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why is faint rustling of clothes heard at 00:08 - 00:09?",
    "answer": "Because the user stands up and walks away.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01062.mp4",
    "question_id": "01062_13",
    "clip_path": "clips/01062/01062__0007500_0008533.mp4"
  },
  {
    "timestamp": "00:08 - 00:09",
    "context": "[00:08 - 00:09] The primary sound is continuous background music, accompanied by faint clothes rustling.",
    "question_type": "Counting",
    "question": "How many types of sounds are audible after the user stands up and walks away?",
    "answer": "Two: continuous background music and faint rustling of clothes.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01062.mp4",
    "question_id": "01062_14",
    "clip_path": "clips/01062/01062__0007500_0008533.mp4"
  },
  {
    "timestamp": "00:00 - 00:02",
    "context": "[00:00 - 00:02] The user handles a yellow plastic snack package, producing a sharp, crinkling sound from the front. A Chinese voiceover explains, \"Dog treats are carried with you, no need to pack them in the suitcase.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Based on the voiceover, why were the dog treats not packed into the suitcase at 00:00 - 00:02?",
    "answer": "Because the dog treats are to be carried with the user, so there's no need to pack them in the suitcase.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01063.mp4",
    "question_id": "01063_1",
    "clip_path": "clips/01063/01063__0000000_0002500.mp4"
  },
  {
    "timestamp": "00:00 - 00:02",
    "context": "[00:00 - 00:02] The user handles a yellow plastic snack package, producing a sharp, crinkling sound from the front.",
    "question_type": "Sound Source Identification",
    "question": "What object generated the sharp, crinkling sound at 00:00 - 00:02?",
    "answer": "A yellow plastic snack package being handled.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01063.mp4",
    "question_id": "01063_2",
    "clip_path": "clips/01063/01063__0000000_0002500.mp4"
  },
  {
    "timestamp": "00:00 - 00:02",
    "context": "[00:00 - 00:02] The user handles a yellow plastic snack package, producing a sharp, crinkling sound from the front.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where relative to the camera did the crinkling sound originate at 00:00 - 00:02?",
    "answer": "From the front of the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01063.mp4",
    "question_id": "01063_3",
    "clip_path": "clips/01063/01063__0000000_0002500.mp4"
  },
  {
    "timestamp": "00:03 - 00:09",
    "context": "[00:03 - 00:09] Standing over an open suitcase with a white dog watching from the front, the user places a blue collapsible bowl into a green fabric bag, creating a soft, continuous rustling sound. The user says, \"Let's put the dog food bowl together with the dog food.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "What was the stated purpose for placing the blue collapsible bowl into the green fabric bag at 00:03 - 00:09?",
    "answer": "To keep the dog food bowl together with the dog food.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01063.mp4",
    "question_id": "01063_4",
    "clip_path": "clips/01063/01063__0002500_0009500.mp4"
  },
  {
    "timestamp": "00:03 - 00:09",
    "context": "[00:03 - 00:09] The user places a blue collapsible bowl into a green fabric bag, creating a soft, continuous rustling sound.",
    "question_type": "Sound Characteristics",
    "question": "How is the rustling sound described when the bowl is placed into the bag at 00:03 - 00:09?",
    "answer": "It is a soft, continuous rustling sound.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01063.mp4",
    "question_id": "01063_5",
    "clip_path": "clips/01063/01063__0002500_0009500.mp4"
  },
  {
    "timestamp": "00:11 - 00:12",
    "context": "[00:11 - 00:12] The user fastens a grey strap inside the suitcase; the plastic buckle closing produces a single, sharp, distinct click directly in front of the camera.",
    "question_type": "Sound Characteristics",
    "question": "What are the acoustic qualities of the buckle closing sound at 00:11 - 00:12?",
    "answer": "A single, sharp, distinct click.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01063.mp4",
    "question_id": "01063_6",
    "clip_path": "clips/01063/01063__0010500_0012500.mp4"
  },
  {
    "timestamp": "00:11 - 00:12",
    "context": "[00:11 - 00:12] The plastic buckle closing produces a single, sharp, distinct click directly in front of the camera.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where relative to the camera did the buckle's click originate at 00:11 - 00:12?",
    "answer": "Directly in front of the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01063.mp4",
    "question_id": "01063_7",
    "clip_path": "clips/01063/01063__0010500_0012500.mp4"
  },
  {
    "timestamp": "00:13 - 00:16",
    "context": "[00:13 - 00:16] While explaining the packing organization, the user pats the empty left side of the suitcase, creating two soft thuds, and says, \"This side is for my clothes, and this side is for its things. OK.\"",
    "question_type": "Counting",
    "question": "How many thuds were produced when the user patted the left side of the suitcase at 00:13 - 00:16?",
    "answer": "Two soft thuds.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01063.mp4",
    "question_id": "01063_8",
    "clip_path": "clips/01063/01063__0012500_0016500.mp4"
  },
  {
    "timestamp": "00:13 - 00:16",
    "context": "[00:13 - 00:16] While explaining the packing organization, the user pats the empty left side of the suitcase and says, \"This side is for my clothes, and this side is for its things. OK.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the user pat the empty left side of the suitcase at 00:13 - 00:16?",
    "answer": "To illustrate the packing organization—showing which side is for their clothes and which is for the dog's things.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01063.mp4",
    "question_id": "01063_9",
    "clip_path": "clips/01063/01063__0012500_0016500.mp4"
  },
  {
    "timestamp": "00:16 - 00:18",
    "context": "[00:16 - 00:18] The user closes the suitcase, causing a loud, hollow thud as the two hard-shell halves connect.",
    "question_type": "Sound Characteristics",
    "question": "How is the sound described when the suitcase halves connect at 00:16 - 00:18?",
    "answer": "A loud, hollow thud.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01063.mp4",
    "question_id": "01063_10",
    "clip_path": "clips/01063/01063__0015500_0018500.mp4"
  },
  {
    "timestamp": "00:16 - 00:18",
    "context": "[00:16 - 00:18] A voiceover says, \"Close the lid,\" followed immediately by a rapid series of sharp zipping sounds as the user pulls the zipper to shut the suitcase.",
    "question_type": "Temporal Information",
    "question": "What sound occurred immediately after the voiceover said \"Close the lid\" at 00:16 - 00:18?",
    "answer": "A rapid series of sharp zipping sounds as the user pulled the zipper to shut the suitcase.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01063.mp4",
    "question_id": "01063_11",
    "clip_path": "clips/01063/01063__0015500_0018500.mp4"
  },
  {
    "timestamp": "00:16 - 00:18",
    "context": "[00:16 - 00:18] Followed immediately by a rapid series of sharp zipping sounds as the user pulls the zipper to shut the suitcase.",
    "question_type": "Counting",
    "question": "Were the zipping sounds at 00:16 - 00:18 a single zip or multiple?",
    "answer": "Multiple—described as a rapid series of sharp zipping sounds.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01063.mp4",
    "question_id": "01063_12",
    "clip_path": "clips/01063/01063__0015500_0018500.mp4"
  },
  {
    "timestamp": "00:00 - 00:02",
    "context": "[00:00 - 00:02] The user searches a cluttered desk, pushes a yellow dog vest aside causing a soft rustling. The user, directly in front, asks: “我的腰包去哪了” (Where did my fanny pack go?).",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the user ask “Where did my fanny pack go?” at the start?",
    "answer": "Because they were searching the cluttered desk and couldn’t find the fanny pack.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01064.mp4",
    "question_id": "01064_1",
    "clip_path": "clips/01064/01064__0000000_0002500.mp4"
  },
  {
    "timestamp": "00:00 - 00:02",
    "context": "[00:00 - 00:02] The user’s right hand pushes a yellow dog vest aside, creating a soft rustling sound.",
    "question_type": "Sound Source Identification",
    "question": "What generated the soft rustling sound at the beginning?",
    "answer": "The yellow dog vest being pushed aside by the user’s right hand.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01064.mp4",
    "question_id": "01064_2",
    "clip_path": "clips/01064/01064__0000000_0002500.mp4"
  },
  {
    "timestamp": "00:00 - 00:02",
    "context": "[00:00 - 00:02] While searching, the user speaks aloud in a questioning tone from directly in front.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the user’s speech originate relative to the camera at the start?",
    "answer": "Directly in front of the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01064.mp4",
    "question_id": "01064_3",
    "clip_path": "clips/01064/01064__0000000_0002500.mp4"
  },
  {
    "timestamp": "00:03 - 00:08",
    "context": "[00:03 - 00:08] The user moves a black backpack off a white cabinet, producing fabric rustling and a soft thud as it’s shifted.",
    "question_type": "Counting",
    "question": "How many distinct sound types occurred when the backpack was moved off the cabinet?",
    "answer": "Two sounds: fabric rustling and a soft thud.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01064.mp4",
    "question_id": "01064_4",
    "clip_path": "clips/01064/01064__0002500_0008500.mp4"
  },
  {
    "timestamp": "00:03 - 00:08",
    "context": "[00:03 - 00:08] The user picks up a leather dog harness; its metal buckles create a series of quiet, metallic clinks as it’s moved and placed on the cabinet.",
    "question_type": "Sound Characteristics",
    "question": "What were the volume and quality of the buckle sounds from the leather dog harness?",
    "answer": "They were a series of quiet, metallic clinks.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01064.mp4",
    "question_id": "01064_5",
    "clip_path": "clips/01064/01064__0002500_0008500.mp4"
  },
  {
    "timestamp": "00:03 - 00:08",
    "context": "[00:03 - 00:08] Handling the leather dog harness produces multiple metallic clinks from its buckles.",
    "question_type": "Sound Source Identification",
    "question": "What produced the series of quiet metallic clinks during 00:03–00:08?",
    "answer": "The metal buckles of the leather dog harness as it was moved and placed.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01064.mp4",
    "question_id": "01064_6",
    "clip_path": "clips/01064/01064__0002500_0008500.mp4"
  },
  {
    "timestamp": "00:03 - 00:08",
    "context": "[00:03 - 00:08] During the search, the user says: “如果东西没放到固定的位置, 找起来还是挺麻烦的” (If things are not put in a fixed position, it’s still quite troublesome to find them).",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the user remark that not putting things in fixed positions makes them hard to find?",
    "answer": "To explain their difficulty while searching for the fanny pack.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01064.mp4",
    "question_id": "01064_7",
    "clip_path": "clips/01064/01064__0002500_0008500.mp4"
  },
  {
    "timestamp": "00:09 - 00:12",
    "context": "[00:09 - 00:12] The user shifts the leather harness (light clinking) and pushes a black keyboard aside, making a dull, hollow thud against the desk.",
    "question_type": "Sound Source Identification",
    "question": "What object produced the dull, hollow thud in this interval?",
    "answer": "The black keyboard when it was pushed aside against the desk.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01064.mp4",
    "question_id": "01064_8",
    "clip_path": "clips/01064/01064__0008500_0012500.mp4"
  },
  {
    "timestamp": "00:09 - 00:12",
    "context": "[00:09 - 00:12] Pushing the keyboard aside makes a dull, hollow thud.",
    "question_type": "Sound Characteristics",
    "question": "How is the thud sound from the keyboard described?",
    "answer": "It is a dull, hollow thud.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01064.mp4",
    "question_id": "01064_9",
    "clip_path": "clips/01064/01064__0008500_0012500.mp4"
  },
  {
    "timestamp": "00:09 - 00:12",
    "context": "[00:09 - 00:12] Light clinking from the harness occurs, followed by a dull, hollow thud from the keyboard.",
    "question_type": "Counting",
    "question": "How many distinct non-speech sound types are heard during the frantic desk search here?",
    "answer": "Two: light clinking and a dull, hollow thud.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01064.mp4",
    "question_id": "01064_10",
    "clip_path": "clips/01064/01064__0008500_0012500.mp4"
  },
  {
    "timestamp": "00:09 - 00:12",
    "context": "[00:09 - 00:12] Frustrated by the unsuccessful search, the user asks again: “我的腰包呢” (Where’s my fanny pack?).",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the user repeat the question about the fanny pack?",
    "answer": "Because the search was still unsuccessful, leading to frustration.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01064.mp4",
    "question_id": "01064_11",
    "clip_path": "clips/01064/01064__0008500_0012500.mp4"
  },
  {
    "timestamp": "00:15 - 00:18",
    "context": "[00:15 - 00:18] The user looks down toward the floor beside the chair and exclaims in a relieved tone: “哦, 在这, 掉到这了” (Oh, it’s here, it fell here).",
    "question_type": "Inferential & Contextual Causality",
    "question": "What prompted the relieved exclamation at 00:15–00:18?",
    "answer": "Discovering the missing fanny pack on the floor beside the chair.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01064.mp4",
    "question_id": "01064_12",
    "clip_path": "clips/01064/01064__0014500_0018500.mp4"
  },
  {
    "timestamp": "00:15 - 00:18",
    "context": "[00:15 - 00:18] The user picks up the black fanny pack, creating a distinct, close-range rustling as the fabric is handled.",
    "question_type": "Cross-Modal Reasoning",
    "question": "After the distinct, close-range rustling from handling fabric, what object was being picked up?",
    "answer": "The black fanny pack.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01064.mp4",
    "question_id": "01064_13",
    "clip_path": "clips/01064/01064__0014500_0018500.mp4"
  },
  {
    "timestamp": "00:15 - 00:18",
    "context": "[00:15 - 00:18] Picking up the fanny pack produces a distinct, close-range rustling sound.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "What was the relative distance of the rustling sound when the fanny pack was picked up?",
    "answer": "Close-range.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01064.mp4",
    "question_id": "01064_14",
    "clip_path": "clips/01064/01064__0014500_0018500.mp4"
  },
  {
    "timestamp": "00:15 - 00:18",
    "context": "[00:15 - 00:18] A distinct, close-range rustling occurs as the fanny pack is picked up.",
    "question_type": "Temporal Information",
    "question": "When did the distinct close-range rustling occur?",
    "answer": "Between 00:15 and 00:18, as the fanny pack was picked up.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01064.mp4",
    "question_id": "01064_15",
    "clip_path": "clips/01064/01064__0014500_0018500.mp4"
  },
  {
    "timestamp": "00:00 - 00:12",
    "context": "[00:00 - 00:02] The user asks “我的腰包去哪了”. [00:09 - 00:12] The user asks again “我的腰包呢”.",
    "question_type": "Counting",
    "question": "Across the clip before the item is found, how many times did the user ask about the fanny pack?",
    "answer": "Twice.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01064.mp4",
    "question_id": "01064_16",
    "clip_path": "clips/01064/01064__0000000_0012500.mp4"
  },
  {
    "timestamp": "00:00 - 00:09",
    "context": "[00:00 - 00:09] A woman's voice, front-right and close, speaks in an amused tone while soft, intermittent clinking of chopsticks against a paper bowl is heard. She says: \"That Alpha took over Annie's bed. Annie is sitting on the edge, and Alpha is lying in the middle.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why is the woman speaking in an amused tone during 00:00 - 00:09?",
    "answer": "She finds it humorous that Alpha took over Annie's bed, leaving Annie on the edge while Alpha lies in the middle.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01066.mp4",
    "question_id": "01066_1",
    "clip_path": "clips/01066/01066__0000000_0009500.mp4"
  },
  {
    "timestamp": "00:09 - 00:15",
    "context": "[00:09 - 00:15] The camera holder, a man, laughs softly and speaks to the dogs in a playful, scolding tone: \"Alpha, why did you take over Annie's bed? ... Can you just take over a girl's bed like that?\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why does the man address Alpha in a playful, scolding tone at 00:09 - 00:15?",
    "answer": "Because Alpha has taken over Annie's bed.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01066.mp4",
    "question_id": "01066_2",
    "clip_path": "clips/01066/01066__0008500_0015441.mp4"
  },
  {
    "timestamp": "00:00 - 00:09",
    "context": "[00:00 - 00:09] Soft, intermittent clinking of chopsticks against a paper bowl is heard nearby.",
    "question_type": "Sound Source Identification",
    "question": "What generated the clinking sound heard at 00:00 - 00:09?",
    "answer": "Chopsticks striking a paper bowl.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01066.mp4",
    "question_id": "01066_3",
    "clip_path": "clips/01066/01066__0000000_0009500.mp4"
  },
  {
    "timestamp": "00:00 - 00:09",
    "context": "[00:00 - 00:09] The clinking is described as soft and intermittent.",
    "question_type": "Sound Characteristics",
    "question": "What are the volume and pattern characteristics of the clinking sound?",
    "answer": "It is soft and intermittent.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01066.mp4",
    "question_id": "01066_4",
    "clip_path": "clips/01066/01066__0000000_0009500.mp4"
  },
  {
    "timestamp": "00:00 - 00:09",
    "context": "[00:00 - 00:09] A woman's voice originates from the front-right at a close distance.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where does the woman's speech originate relative to the camera?",
    "answer": "From the front-right at a close distance.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01066.mp4",
    "question_id": "01066_5",
    "clip_path": "clips/01066/01066__0000000_0009500.mp4"
  },
  {
    "timestamp": "00:09 - 00:15",
    "context": "[00:09 - 00:15] The man's voice is very clear and originates from the camera's position.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From where does the man's speech originate during 00:09 - 00:15?",
    "answer": "Directly from the camera's position.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01066.mp4",
    "question_id": "01066_6",
    "clip_path": "clips/01066/01066__0008500_0015441.mp4"
  },
  {
    "timestamp": "00:00 - 00:09",
    "context": "[00:00 - 00:09] The clinking of chopsticks against a paper bowl is described as intermittent.",
    "question_type": "Temporal Information",
    "question": "Is the clinking continuous or intermittent during 00:00 - 00:09?",
    "answer": "Intermittent.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01066.mp4",
    "question_id": "01066_7",
    "clip_path": "clips/01066/01066__0000000_0009500.mp4"
  },
  {
    "timestamp": "00:00 - 00:09",
    "context": "[00:00 - 00:09] While she speaks, the camera holder turns to the right, revealing the two dogs in the corner.",
    "question_type": "Counting",
    "question": "How many dogs are revealed when the camera turns to the right?",
    "answer": "Two.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01066.mp4",
    "question_id": "01066_8",
    "clip_path": "clips/01066/01066__0000000_0009500.mp4"
  },
  {
    "timestamp": "00:00 - 00:15",
    "context": "[00:00 - 00:09] A woman speaks. [00:09 - 00:15] The camera-holding man laughs and speaks.",
    "question_type": "Counting",
    "question": "How many distinct speakers are heard in the clip?",
    "answer": "Two: a woman and the camera-holding man.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01066.mp4",
    "question_id": "01066_9",
    "clip_path": "clips/01066/01066__0000000_0015441.mp4"
  },
  {
    "timestamp": "00:00 - 00:09",
    "context": "[00:00 - 00:09] Soft clinking of chopsticks against a paper bowl is heard as the woman shares food, using her chopsticks to transfer items into the camera holder's bowl.",
    "question_type": "Cross-Modal Reasoning",
    "question": "When the clinking is heard, what is the woman doing visually?",
    "answer": "She is using her chopsticks to transfer food into the camera holder's bowl.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01066.mp4",
    "question_id": "01066_10",
    "clip_path": "clips/01066/01066__0000000_0009500.mp4"
  },
  {
    "timestamp": "00:00 - 00:09",
    "context": "[00:00 - 00:09] She narrates the dogs' situation; the camera turns right, revealing the two dogs in the corner.",
    "question_type": "Cross-Modal Reasoning",
    "question": "After her narration about the dogs, what does the camera reveal?",
    "answer": "It reveals the two dogs in the corner.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01066.mp4",
    "question_id": "01066_11",
    "clip_path": "clips/01066/01066__0000000_0009500.mp4"
  },
  {
    "timestamp": "00:09 - 00:15",
    "context": "[00:09 - 00:15] The man playfully scolds Alpha for taking Annie's bed. The visual confirms: the white dog (Alpha) lies comfortably on the mat, while the golden dog (Annie) is squeezed onto the edge.",
    "question_type": "Cross-Modal Reasoning",
    "question": "Following the man's playful scolding, what does the visual confirm about the dogs' positions?",
    "answer": "Alpha, the white dog, is lying comfortably on the mat, while Annie, the golden dog, is squeezed onto the edge.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01066.mp4",
    "question_id": "01066_12",
    "clip_path": "clips/01066/01066__0008500_0015441.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "[00:00 - 00:06] As people disembark from a small boat, the camera holder speaks in a clear, forward-facing male voice to the guide dog: \"Alpha, we are now on the island.\" Another man in a grey jacket begins to give instructions.",
    "question_type": "Sound Source Identification",
    "question": "Who says, \"Alpha, we are now on the island.\" at the start?",
    "answer": "The camera holder, speaking in a clear, forward-facing male voice.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01068.mp4",
    "question_id": "01068_1",
    "clip_path": "clips/01068/01068__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:06 - 00:13",
    "context": "[00:06 - 00:13] The man in the grey jacket provides calm guidance from the front: \"There are two steps here. Come up slowly.\" He continues, \"Slowly, good... slowly... good... one step, good, two steps, good.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why does the man in the grey jacket tell them, \"There are two steps here. Come up slowly\"?",
    "answer": "To ensure the user and the guide dog get off the boat safely onto the pier.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01068.mp4",
    "question_id": "01068_2",
    "clip_path": "clips/01068/01068__0005500_0013500.mp4"
  },
  {
    "timestamp": "00:06 - 00:13",
    "context": "[00:06 - 00:13] \"Slowly, good... one step, good, two steps, good.\"",
    "question_type": "Counting",
    "question": "How many steps does the man in the grey jacket count out?",
    "answer": "Two steps.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01068.mp4",
    "question_id": "01068_3",
    "clip_path": "clips/01068/01068__0005500_0013500.mp4"
  },
  {
    "timestamp": "00:06 - 00:13",
    "context": "[00:06 - 00:13] The man gives calm, instructional guidance and counts the steps in a reassuring tone.",
    "question_type": "Sound Characteristics",
    "question": "What is the tone of the man's step-counting guidance?",
    "answer": "Calm and reassuring.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01068.mp4",
    "question_id": "01068_4",
    "clip_path": "clips/01068/01068__0005500_0013500.mp4"
  },
  {
    "timestamp": "00:06 - 00:13",
    "context": "[00:06 - 00:13] The man in the grey jacket provides guidance from the front as the dog climbs onto the pier.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From where relative to the user is the guidance delivered during the step counting?",
    "answer": "From the front.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01068.mp4",
    "question_id": "01068_5",
    "clip_path": "clips/01068/01068__0005500_0013500.mp4"
  },
  {
    "timestamp": "00:06 - 00:13",
    "context": "[00:06 - 00:13] The man counts, \"one step... two steps,\" as the dog climbs from the boat onto the pier.",
    "question_type": "Temporal Information",
    "question": "When does the step counting occur during the disembarkation?",
    "answer": "Between 00:06 and 00:13, as the dog climbs onto the pier.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01068.mp4",
    "question_id": "01068_6",
    "clip_path": "clips/01068/01068__0005500_0013500.mp4"
  },
  {
    "timestamp": "00:13 - 00:18",
    "context": "[00:13 - 00:18] The user commands, \"Alpha, let's go. We're on the island, Alpha.\" The man in the grey jacket continues to guide from a close distance: \"Good, come up slowly... come slowly, good.\"",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "During this segment, is the guiding voice near or far from the user?",
    "answer": "Near; it is from a close distance.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01068.mp4",
    "question_id": "01068_7",
    "clip_path": "clips/01068/01068__0012500_0018500.mp4"
  },
  {
    "timestamp": "00:18 - 00:21",
    "context": "[00:18 - 00:21] Another man advises: \"A bigger step.\" followed by, \"Be careful, be careful, be careful.\" The man in the grey jacket confirms, \"Good, good.\"",
    "question_type": "Counting",
    "question": "How many times is the phrase \"Be careful\" repeated?",
    "answer": "Three times.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01068.mp4",
    "question_id": "01068_8",
    "clip_path": "clips/01068/01068__0017500_0021500.mp4"
  },
  {
    "timestamp": "00:18 - 00:21",
    "context": "[00:18 - 00:21] After the cautionary advice, the man in the grey jacket says, \"Good, good.\"",
    "question_type": "Sound Source Identification",
    "question": "Who says \"Good, good\" to confirm their safety?",
    "answer": "The man in the grey jacket.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01068.mp4",
    "question_id": "01068_9",
    "clip_path": "clips/01068/01068__0017500_0021500.mp4"
  },
  {
    "timestamp": "00:18 - 00:21",
    "context": "[00:18 - 00:21] Another man urges, \"A bigger step... Be careful, be careful, be careful.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why does another man urge \"A bigger step\" and repeat \"Be careful\"?",
    "answer": "To help the user and guide dog complete the movement safely as they finish getting onto the pier.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01068.mp4",
    "question_id": "01068_10",
    "clip_path": "clips/01068/01068__0017500_0021500.mp4"
  },
  {
    "timestamp": "00:21 - 00:25",
    "context": "[00:21 - 00:25] The group begins to walk. The man in the grey jacket gives directions from the front-left: \"Alright, we'll stay to your left side... here for a bit...\"",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From what direction relative to the user are the navigational directions given at this point?",
    "answer": "From the front-left.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01068.mp4",
    "question_id": "01068_11",
    "clip_path": "clips/01068/01068__0020500_0025101.mp4"
  },
  {
    "timestamp": "00:21 - 00:25",
    "context": "[00:21 - 00:25] The user commands, \"Alpha, find the way,\" instructing the dog to begin leading.",
    "question_type": "Cross-Modal Reasoning",
    "question": "After the user says, \"Alpha, find the way,\" what action is expected to happen?",
    "answer": "The guide dog is expected to begin leading the group.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01068.mp4",
    "question_id": "01068_12",
    "clip_path": "clips/01068/01068__0020500_0025101.mp4"
  },
  {
    "timestamp": "00:21 - 00:25",
    "context": "[00:21 - 00:25] Now safely on the pier, the group begins to walk.",
    "question_type": "Temporal Information",
    "question": "When does the group begin to walk?",
    "answer": "Between 00:21 and 00:25, after they are safely on the pier.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01068.mp4",
    "question_id": "01068_13",
    "clip_path": "clips/01068/01068__0020500_0025101.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "[00:00] On a moving boat, a continuous, low-frequency engine rumble and gentle water sounds are heard. A man reaches forward and gently pets a yellow Labrador guide dog. As he does this, the camera holder speaks from the immediate foreground: \"Alpha, we're almost there. We're almost there, okay?\" The caption states the purpose is to comfort the dog as they approach their destination.",
    "question_type": "Inferential & Contextual Causality",
    "question": "What was the purpose of the petting and reassuring speech at the start?",
    "answer": "To comfort the dog, Alpha, as they approached their destination.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01069.mp4",
    "question_id": "01069_1",
    "clip_path": "clips/01069/01069__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "The video begins on a moving boat, indicated by a continuous, low-frequency rumble and gentle water sounds.",
    "question_type": "Sound Source Identification",
    "question": "What produced the continuous, low-frequency rumble heard at the beginning?",
    "answer": "The boat's engine.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01069.mp4",
    "question_id": "01069_2",
    "clip_path": "clips/01069/01069__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "The caption notes \"the gentle sound of water\" alongside the engine rumble.",
    "question_type": "Sound Characteristics",
    "question": "How is the water sound described at the start?",
    "answer": "Gentle.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01069.mp4",
    "question_id": "01069_3",
    "clip_path": "clips/01069/01069__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "The engine noise is described as a continuous, low-frequency rumble.",
    "question_type": "Temporal Information",
    "question": "During 00:00–00:03, is the engine rumble intermittent or continuous?",
    "answer": "Continuous.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01069.mp4",
    "question_id": "01069_4",
    "clip_path": "clips/01069/01069__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:04 - 00:06",
    "context": "[00:04] After the man retracts his hand, the camera holder continues to engage the dog and asks from the foreground: \"Is it fun, Alpha?\"",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From where relative to the camera did the question \"Is it fun, Alpha?\" originate?",
    "answer": "From the immediate foreground, close to the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01069.mp4",
    "question_id": "01069_5",
    "clip_path": "clips/01069/01069__0003500_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "As the man gently pets the dog's head, the camera holder speaks: \"Alpha, we're almost there. We're almost there, okay?\"",
    "question_type": "Cross-Modal Reasoning",
    "question": "While the man is seen petting the dog's head, what reassuring words are heard?",
    "answer": "\"Alpha, we're almost there. We're almost there, okay?\"",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01069.mp4",
    "question_id": "01069_6",
    "clip_path": "clips/01069/01069__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:07 - 00:10",
    "context": "[00:07] The camera holder jokes: \"Alpha says, 'I could probably swim over faster than this.'\" [00:08] In response, a man's voice from the left lets out a short, quiet laugh.",
    "question_type": "Inferential & Contextual Causality",
    "question": "What prompted the man's laugh from the left?",
    "answer": "The camera holder's joke about Alpha being able to swim faster than the boat.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01069.mp4",
    "question_id": "01069_7",
    "clip_path": "clips/01069/01069__0006500_0010500.mp4"
  },
  {
    "timestamp": "00:07 - 00:10",
    "context": "After the joke, a man's voice from the left lets out a laugh.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From which direction relative to the camera did the laugh occur?",
    "answer": "From the left side of the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01069.mp4",
    "question_id": "01069_8",
    "clip_path": "clips/01069/01069__0006500_0010500.mp4"
  },
  {
    "timestamp": "00:07 - 00:10",
    "context": "In response to the joke, a man's voice from the left laughs briefly, and the speaker concludes with a cheerful \"Hahaha, okay.\"",
    "question_type": "Counting",
    "question": "How many distinct laugh occurrences are heard after the joke?",
    "answer": "Two: one short laugh from the man on the left and the speaker’s own cheerful \"Hahaha.\"",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01069.mp4",
    "question_id": "01069_9",
    "clip_path": "clips/01069/01069__0006500_0010500.mp4"
  },
  {
    "timestamp": "00:07 - 00:10",
    "context": "A man's voice from the left lets out a short, quiet laugh.",
    "question_type": "Sound Characteristics",
    "question": "What are the volume and duration characteristics of the laugh from the left?",
    "answer": "It is short and quiet.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01069.mp4",
    "question_id": "01069_10",
    "clip_path": "clips/01069/01069__0006500_0010500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "A man says: '往下往下' ... '这里有点危险' ... '你要不要先把那个拉布拉多拖一下过去' (indicating concern for the guide dog's safety while boarding).",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the first man suggest getting the Labrador across first?",
    "answer": "To prioritize the guide dog's safety while boarding the small boat due to the dangerous situation.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01070.mp4",
    "question_id": "01070_1",
    "clip_path": "clips/01070/01070__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "From the front-left, a man's voice is heard giving verbal guidance: '往下往下'.",
    "question_type": "Sound Source Identification",
    "question": "Who generated the '往下往下' verbal guidance at the start?",
    "answer": "A man positioned at the front-left of the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01070.mp4",
    "question_id": "01070_2",
    "clip_path": "clips/01070/01070__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "From the front-left, a man's voice is heard...",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the initial guidance voice originate relative to the camera?",
    "answer": "From the front-left.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01070.mp4",
    "question_id": "01070_3",
    "clip_path": "clips/01070/01070__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:07 - 00:15",
    "context": "He responds with a thoughtful tone: '哎, 有点...有点难, 我看看要怎么带它好'.",
    "question_type": "Sound Characteristics",
    "question": "What was the tone of the man's response explaining the difficulty?",
    "answer": "A thoughtful tone.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01070.mp4",
    "question_id": "01070_4",
    "clip_path": "clips/01070/01070__0006500_0015500.mp4"
  },
  {
    "timestamp": "00:07 - 00:15",
    "context": "The camera person asks: '这不好走是吧'.",
    "question_type": "Sound Source Identification",
    "question": "Who asked, '这不好走是吧'?",
    "answer": "The camera person.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01070.mp4",
    "question_id": "01070_5",
    "clip_path": "clips/01070/01070__0006500_0015500.mp4"
  },
  {
    "timestamp": "00:07 - 00:15",
    "context": "He points out the hazard: '有个缺口' and clarifies the path: '这样的'.",
    "question_type": "Temporal Information",
    "question": "During which time segment did the man point out '有个缺口' and explain the intended path?",
    "answer": "Between 00:07 and 00:15.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01070.mp4",
    "question_id": "01070_6",
    "clip_path": "clips/01070/01070__0006500_0015500.mp4"
  },
  {
    "timestamp": "00:15 - 00:23",
    "context": "The camera person gives a clear, firm command: '阿尔法走, find the way'.",
    "question_type": "Sound Characteristics",
    "question": "What was the quality of the camera person's command to the guide dog?",
    "answer": "It was a clear, firm command.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01070.mp4",
    "question_id": "01070_7",
    "clip_path": "clips/01070/01070__0014500_0023500.mp4"
  },
  {
    "timestamp": "00:15 - 00:23",
    "context": "The assisting man gives a warning from nearby: '小心, 这里有个缺口哦'.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "What was the relative location of the man's warning '小心, 这里有个缺口哦'?",
    "answer": "From nearby, after he boarded the boat to assist.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01070.mp4",
    "question_id": "01070_8",
    "clip_path": "clips/01070/01070__0014500_0023500.mp4"
  },
  {
    "timestamp": "00:15 - 00:23",
    "context": "The command combines Chinese and English: '阿尔法走, find the way'.",
    "question_type": "Counting",
    "question": "How many languages were used in the command to the guide dog?",
    "answer": "Two: Chinese and English.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01070.mp4",
    "question_id": "01070_9",
    "clip_path": "clips/01070/01070__0014500_0023500.mp4"
  },
  {
    "timestamp": "00:23 - 00:28",
    "context": "Now on the boat, the man provides cues from a few feet ahead: '好好, 小心前面'.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where was the man relative to the camera when he said '小心前面'?",
    "answer": "A few feet ahead of the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01070.mp4",
    "question_id": "01070_10",
    "clip_path": "clips/01070/01070__0022500_0028500.mp4"
  },
  {
    "timestamp": "00:23 - 00:28",
    "context": "He instructs: '上船, 对'. The camera person repeats: '上船'.",
    "question_type": "Counting",
    "question": "How many times was '上船' spoken in this segment?",
    "answer": "Twice—once by the man and once by the camera person.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01070.mp4",
    "question_id": "01070_11",
    "clip_path": "clips/01070/01070__0022500_0028500.mp4"
  },
  {
    "timestamp": "00:23 - 00:28",
    "context": "'好好, 小心前面' is said as they move further in on the boat.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the man say '小心前面' at this moment?",
    "answer": "To caution about immediate obstacles ahead as they continued onto the boat.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01070.mp4",
    "question_id": "01070_12",
    "clip_path": "clips/01070/01070__0022500_0028500.mp4"
  },
  {
    "timestamp": "00:28 - 00:39",
    "context": "He warns: '这里还有一个比较大的一个楼梯口' and repeats: '下比较大的楼梯口, 小心啊'. He adds: '这还有一个'.",
    "question_type": "Counting",
    "question": "How many step openings did the man mention during 00:28–00:39?",
    "answer": "Two: one relatively large step and then another one.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01070.mp4",
    "question_id": "01070_13",
    "clip_path": "clips/01070/01070__0027500_0039500.mp4"
  },
  {
    "timestamp": "00:28 - 00:39",
    "context": "After '好, OK', the man repeats the warning for emphasis: '下比较大的楼梯口, 小心啊'.",
    "question_type": "Temporal Information",
    "question": "When was the repeated warning about the large step given relative to the acknowledgment?",
    "answer": "Immediately after the '好, OK' acknowledgment within 00:28–00:39.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01070.mp4",
    "question_id": "01070_14",
    "clip_path": "clips/01070/01070__0027500_0039500.mp4"
  },
  {
    "timestamp": "00:28 - 00:39",
    "context": "The man issues another clear, preventative warning about the step.",
    "question_type": "Sound Characteristics",
    "question": "How is the man's warning about the large step described acoustically?",
    "answer": "As a clear, preventative warning.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01070.mp4",
    "question_id": "01070_15",
    "clip_path": "clips/01070/01070__0027500_0039500.mp4"
  },
  {
    "timestamp": "00:06 - 00:08",
    "context": "[00:06] Camera pulls back to show the dog comfortably settled in its bed. [00:07] A synthesized male voice says: \"Bro, you don't have to worry about me, you can go get busy.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why does the synthesized male voice say \"you can go get busy\" at this moment?",
    "answer": "Because the dog is comfortably settled in the new bed, implying it doesn’t need further attention.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01072.mp4",
    "question_id": "01072_1",
    "clip_path": "clips/01072/01072__0005500_0008500.mp4"
  },
  {
    "timestamp": "00:19 - 00:22",
    "context": "[00:19] The person says: \"You're not even going to thank me.\" [00:21] The synthesized voice replies: \"Thank you, now go get busy.\"",
    "question_type": "Sound Source Identification",
    "question": "Who delivers the sarcastic reply, \"Thank you, now go get busy\"?",
    "answer": "A synthesized male 'dog voice' representing the dog's thoughts.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01072.mp4",
    "question_id": "01072_2",
    "clip_path": "clips/01072/01072__0018500_0022500.mp4"
  },
  {
    "timestamp": "00:35 - 00:46",
    "context": "[00:35] The dog is visibly asleep, curled tightly in a ball. [00:36 - 00:46] A faint, low-pitched snoring sound emerges from the dog.",
    "question_type": "Sound Characteristics",
    "question": "What are the acoustic characteristics of the snoring heard while the dog sleeps?",
    "answer": "It is faint and low-pitched.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01072.mp4",
    "question_id": "01072_3",
    "clip_path": "clips/01072/01072__0034500_0046500.mp4"
  },
  {
    "timestamp": "00:35 - 00:46",
    "context": "[00:35 - 00:46] The dog sleeps curled in a ball as a faint, low-pitched snoring sound is heard.",
    "question_type": "Temporal Information",
    "question": "During what time interval is the snoring audible?",
    "answer": "From 00:35 to 00:46.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01072.mp4",
    "question_id": "01072_4",
    "clip_path": "clips/01072/01072__0034500_0046500.mp4"
  },
  {
    "timestamp": "00:22 - 00:30",
    "context": "[00:22] The camera moves closer as the person's hands gently pet the dog's head and neck. [00:23 - 00:30] A soft rustling sound of hands stroking fur is audible over the music.",
    "question_type": "Sound Source Identification",
    "question": "What action generates the soft rustling sound heard over the music?",
    "answer": "The person's hands stroking the dog's fur on its head and neck.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01072.mp4",
    "question_id": "01072_5",
    "clip_path": "clips/01072/01072__0021500_0030500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "[00:00 - 00:06] The interaction is accompanied by soft, uplifting background music while the person pets the dog.",
    "question_type": "Sound Characteristics",
    "question": "How is the background music characterized during the initial interaction?",
    "answer": "It is soft and uplifting.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01072.mp4",
    "question_id": "01072_6",
    "clip_path": "clips/01072/01072__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:09 - 00:18",
    "context": "[00:09 - 00:18] The person says: \"Alfa, come down... Alfa, come down...\" while the dog remains lying in the bed.",
    "question_type": "Counting",
    "question": "How many times does the person say \"Alfa, come down\"?",
    "answer": "Twice.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01072.mp4",
    "question_id": "01072_7",
    "clip_path": "clips/01072/01072__0008500_0018500.mp4"
  },
  {
    "timestamp": "00:09 - 00:18",
    "context": "[00:09 - 00:18] The person tries to coax the dog out, but it remains lying down. The person says: \"You don't want to move, do you? Is it because it's too warm and comfortable?\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why does the dog refuse to move when coaxed?",
    "answer": "Because the new bed is warm and comfortable, making the dog reluctant to leave.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01072.mp4",
    "question_id": "01072_8",
    "clip_path": "clips/01072/01072__0008500_0018500.mp4"
  },
  {
    "timestamp": "00:22 - 00:30",
    "context": "[00:22] The person's hands re-enter to pet the dog's head and neck; a soft rustling is audible. [00:29] The dog curls up further in response to the petting.",
    "question_type": "Cross-Modal Reasoning",
    "question": "After the gentle petting sounds are heard, what visual response does the dog show?",
    "answer": "The dog curls up further in the bed.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01072.mp4",
    "question_id": "01072_9",
    "clip_path": "clips/01072/01072__0021500_0030500.mp4"
  },
  {
    "timestamp": "00:35 - 00:46",
    "context": "[00:35] The dog is visibly asleep, curled tightly in a ball. [00:36 - 00:46] A faint, low-pitched snoring sound emerges.",
    "question_type": "Cross-Modal Reasoning",
    "question": "When the dog is visibly asleep and curled in a ball, what sound is heard?",
    "answer": "A faint, low-pitched snoring.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01072.mp4",
    "question_id": "01072_10",
    "clip_path": "clips/01072/01072__0034500_0046500.mp4"
  },
  {
    "timestamp": "00:22 - 00:30",
    "context": "[00:22 - 00:30] As the camera moves closer and the person pets the dog, a soft rustling of fur is audible over the music.",
    "question_type": "Temporal Information",
    "question": "During which interval is the soft rustling of fur audible?",
    "answer": "Between 00:22 and 00:30.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01072.mp4",
    "question_id": "01072_11",
    "clip_path": "clips/01072/01072__0021500_0030500.mp4"
  },
  {
    "timestamp": "00:30 - 00:35",
    "context": "[00:30 - 00:35] The person says: \"Okay, sleep in your new nest. Sleep.\"",
    "question_type": "Counting",
    "question": "How many times does the person use the word \"sleep\" while instructing the dog to rest?",
    "answer": "Twice.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01072.mp4",
    "question_id": "01072_12",
    "clip_path": "clips/01072/01072__0029500_0035500.mp4"
  },
  {
    "timestamp": "00:00 - 00:12",
    "context": "A person scoops white rice from a large plastic container into a small metal bowl while speaking. They say Alpha had gastroenteritis and were advised to switch to plain white rice if its stool remains soft.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Based on the speaker's explanation, why is the person switching to plain white rice?",
    "answer": "Because Alpha had gastroenteritis and, if its stool is still soft, the trainer advised switching its dog food to plain white rice.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01079.mp4",
    "question_id": "01079_1",
    "clip_path": "clips/01079/01079__0000000_0011967.mp4"
  },
  {
    "timestamp": "00:00 - 00:12",
    "context": "The sound of rice grains rustling can be heard as they are transferred from a large plastic container into a small metal bowl.",
    "question_type": "Sound Source Identification",
    "question": "What generated the rustling sound heard during the transfer?",
    "answer": "The rice grains being scooped and poured from the plastic container into the metal bowl.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01079.mp4",
    "question_id": "01079_2",
    "clip_path": "clips/01079/01079__0000000_0011967.mp4"
  },
  {
    "timestamp": "00:00 - 00:12",
    "context": "The speaker talks while scooping rice, described as speaking in a clear, conversational tone.",
    "question_type": "Sound Characteristics",
    "question": "What is the quality of the speaker’s voice during the scene?",
    "answer": "It is clear and conversational.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01079.mp4",
    "question_id": "01079_3",
    "clip_path": "clips/01079/01079__0000000_0011967.mp4"
  },
  {
    "timestamp": "00:00 - 00:12",
    "context": "The speaker’s voice originates from the camera's position.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where does the speech originate relative to the camera?",
    "answer": "Directly from the camera’s position.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01079.mp4",
    "question_id": "01079_4",
    "clip_path": "clips/01079/01079__0000000_0011967.mp4"
  },
  {
    "timestamp": "00:10 - 00:12",
    "context": "As the monologue concludes, the person carries the bowl of rice to the kitchen sink, intending to wash it.",
    "question_type": "Temporal Information",
    "question": "What action occurs immediately as the monologue concludes?",
    "answer": "The person carries the bowl of rice to the kitchen sink intending to wash it.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01079.mp4",
    "question_id": "01079_5",
    "clip_path": "clips/01079/01079__0009500_0011967.mp4"
  },
  {
    "timestamp": "00:00 - 00:12",
    "context": "Audible elements include the speaker’s voice and the rustling of rice grains during transfer.",
    "question_type": "Counting",
    "question": "How many distinct types of sounds are explicitly described during this segment?",
    "answer": "Two: the speaker’s voice and the rustling of rice grains.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01079.mp4",
    "question_id": "01079_6",
    "clip_path": "clips/01079/01079__0000000_0011967.mp4"
  },
  {
    "timestamp": "00:00 - 00:12",
    "context": "The person is seen scooping rice while explaining that Alpha had gastroenteritis and may need plain white rice if its stool remains soft.",
    "question_type": "Cross-Modal Reasoning",
    "question": "How does the speech explain the visual action of scooping rice?",
    "answer": "The spoken explanation provides the purpose: preparing plain white rice for Alpha as advised due to soft stools after gastroenteritis.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01079.mp4",
    "question_id": "01079_7",
    "clip_path": "clips/01079/01079__0000000_0011967.mp4"
  },
  {
    "timestamp": "00:00 - 00:02",
    "context": "[00:00 - 00:02] The user places an egg into a light blue electric cooker, creating a single, soft clinking sound as the egg shell touches the metal interior. The user narrates very close from the front.",
    "question_type": "Sound Source Identification",
    "question": "What generated the soft clinking sound at the start?",
    "answer": "The egg shell touching the cooker's metal interior.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01080.mp4",
    "question_id": "01080_1",
    "clip_path": "clips/01080/01080__0000000_0002500.mp4"
  },
  {
    "timestamp": "00:00 - 00:02",
    "context": "[00:00 - 00:02] The user narrates the action from the front and very close (less than 0.5m).",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the user's narration originate relative to the camera?",
    "answer": "From the front, very close—less than 0.5 meters.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01080.mp4",
    "question_id": "01080_2",
    "clip_path": "clips/01080/01080__0000000_0002500.mp4"
  },
  {
    "timestamp": "00:02 - 00:06",
    "context": "[00:02 - 00:06] The lid is placed on the cooker, producing a dull thud.",
    "question_type": "Sound Characteristics",
    "question": "How is the sound characterized when the lid is placed on?",
    "answer": "A dull thud.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01080.mp4",
    "question_id": "01080_3",
    "clip_path": "clips/01080/01080__0001500_0006500.mp4"
  },
  {
    "timestamp": "00:02 - 00:06",
    "context": "[00:02 - 00:06] Placing a steamer rack makes a soft metallic clatter; placing the lid produces a dull thud.",
    "question_type": "Counting",
    "question": "How many distinct assembly sounds are described in this segment?",
    "answer": "Two—the steamer rack's soft metallic clatter and the lid's dull thud.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01080.mp4",
    "question_id": "01080_4",
    "clip_path": "clips/01080/01080__0001500_0006500.mp4"
  },
  {
    "timestamp": "00:07 - 00:12",
    "context": "[00:07 - 00:12] Plugging in the cooker produces a soft scraping as the plug contacts the socket, followed by a distinct, sharp click when fully inserted.",
    "question_type": "Temporal Information",
    "question": "What is the sequence of sounds when the power cord is plugged in?",
    "answer": "A soft scraping sound followed by a distinct, sharp click.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01080.mp4",
    "question_id": "01080_5",
    "clip_path": "clips/01080/01080__0006500_0012500.mp4"
  },
  {
    "timestamp": "00:07 - 00:12",
    "context": "[00:07 - 00:12] The user explains, 'feel for the hole to plug it in, this way it won't be dangerous for me.'",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why does the user feel for the socket hole before plugging in?",
    "answer": "For safety—to avoid putting themselves in danger.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01080.mp4",
    "question_id": "01080_6",
    "clip_path": "clips/01080/01080__0006500_0012500.mp4"
  },
  {
    "timestamp": "00:12 - 00:15",
    "context": "[00:12 - 00:15] The cooker emits three short, high-pitched electronic beeps from in front of the camera.",
    "question_type": "Counting",
    "question": "How many beeps does the cooker emit to signal completion?",
    "answer": "Three short beeps.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01080.mp4",
    "question_id": "01080_7",
    "clip_path": "clips/01080/01080__0011500_0015500.mp4"
  },
  {
    "timestamp": "00:12 - 00:15",
    "context": "[00:12 - 00:15] The beeps signal the cooking cycle has finished; the user says, '米饭煮好了' ('The rice is cooked').",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why does the cooker emit the beeps at this moment?",
    "answer": "To signal that the cooking cycle has finished.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01080.mp4",
    "question_id": "01080_8",
    "clip_path": "clips/01080/01080__0011500_0015500.mp4"
  },
  {
    "timestamp": "00:16 - 00:17",
    "context": "[00:16 - 00:17] The user says, '先把电关上' ('First turn off the power') and unplugs the cooker.",
    "question_type": "Inferential & Contextual Causality",
    "question": "What is the reason for unplugging the cooker before handling it?",
    "answer": "To safely handle the appliance.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01080.mp4",
    "question_id": "01080_9",
    "clip_path": "clips/01080/01080__0015500_0017500.mp4"
  },
  {
    "timestamp": "00:17 - 00:19",
    "context": "[00:17 - 00:19] The user opens the cooker by lifting the outer casing.",
    "question_type": "Cross-Modal Reasoning",
    "question": "When the user lifts the outer casing to open the cooker, what sound is heard?",
    "answer": "A soft metallic clank as it separates from the inner components.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01080.mp4",
    "question_id": "01080_10",
    "clip_path": "clips/01080/01080__0016500_0019500.mp4"
  },
  {
    "timestamp": "00:19 - 00:22",
    "context": "[00:19 - 00:22] Lifting the hot metal bowl creates a faint scraping sound; setting it on the counter makes a soft clink.",
    "question_type": "Temporal Information",
    "question": "What sounds occur during the bowl removal and in what order?",
    "answer": "First a faint scraping as the bowl is lifted, then a soft clink when it is set on the counter.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01080.mp4",
    "question_id": "01080_11",
    "clip_path": "clips/01080/01080__0018500_0021800.mp4"
  },
  {
    "timestamp": "00:19 - 00:22",
    "context": "[00:19 - 00:22] The user says, '哦有点烫' ('Oh, it's a bit hot') while lifting the hot metal bowl of rice.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why does the user say 'it's a bit hot' at this moment?",
    "answer": "Because the metal bowl of rice is hot while being lifted.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01080.mp4",
    "question_id": "01080_12",
    "clip_path": "clips/01080/01080__0018500_0021800.mp4"
  },
  {
    "timestamp": "00:00 - 00:02",
    "context": "[00:00 - 00:02] A woman pushes a stroller past them, its wheels making a low, rumbling sound.",
    "question_type": "Sound Source Identification",
    "question": "What generated the low, rumbling sound at the start?",
    "answer": "The stroller's wheels.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01082.mp4",
    "question_id": "01082_1",
    "clip_path": "clips/01082/01082__0000000_0002500.mp4"
  },
  {
    "timestamp": "00:00 - 00:14",
    "context": "[00:00 - 00:02] The scene is filled with the rhythmic, sharp tapping sounds of their white canes hitting the concrete. [00:02 - 00:14] The alternating tapping of their two canes creates a continuous, percussive rhythm against ambient street noise.",
    "question_type": "Sound Characteristics",
    "question": "What are the acoustic qualities of the white cane tapping heard while they walk?",
    "answer": "Rhythmic, sharp taps forming a continuous, percussive rhythm.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01082.mp4",
    "question_id": "01082_2",
    "clip_path": "clips/01082/01082__0000000_0014500.mp4"
  },
  {
    "timestamp": "00:02 - 00:14",
    "context": "[00:02 - 00:14] Companion: “他们之前美团买菜联系我…如果用了有啥不方便可以跟我说。” Her statement reveals the purpose of their interaction: to provide feedback on the app's accessibility for visually impaired users.",
    "question_type": "Inferential & Contextual Causality",
    "question": "What is the purpose of their interaction as revealed by the companion’s explanation?",
    "answer": "To gather and relay feedback on the shopping app’s accessibility for visually impaired users.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01082.mp4",
    "question_id": "01082_3",
    "clip_path": "clips/01082/01082__0001500_0014500.mp4"
  },
  {
    "timestamp": "00:14 - 00:24",
    "context": "[00:14 - 00:24] The sound of all three white canes tapping the ground now forms a more complex, layered rhythm.",
    "question_type": "Counting",
    "question": "How many white canes can be heard tapping during this segment?",
    "answer": "Three.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01082.mp4",
    "question_id": "01082_4",
    "clip_path": "clips/01082/01082__0013500_0024500.mp4"
  },
  {
    "timestamp": "00:24 - 00:36",
    "context": "[00:24 - 00:36] Companion: “因为他们之前，我拍了个视频…然后好多人就艾特他们…然后他们后来就来找我了。”",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the company later reach out to the companion?",
    "answer": "Her video prompted many people to tag the company to improve accessibility, leading the company to contact her.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01082.mp4",
    "question_id": "01082_5",
    "clip_path": "clips/01082/01082__0023500_0036500.mp4"
  },
  {
    "timestamp": "00:36 - 00:38",
    "context": "[00:36 - 00:38] Companion: “他们把弹窗和搜索框改了。” The tapping of their canes continues unabated.",
    "question_type": "Temporal Information",
    "question": "Is the cane tapping continuous or interrupted during 00:36–00:38?",
    "answer": "Continuous; it continues unabated.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01082.mp4",
    "question_id": "01082_6",
    "clip_path": "clips/01082/01082__0035500_0038500.mp4"
  },
  {
    "timestamp": "00:38 - 00:47",
    "context": "[00:38 - 00:47] Man: “…我就听右边有一个打桌球的。” Corroborating his statement, the faint, distinct, and rhythmic sound of a ping pong ball being hit can be heard from the right side.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From which direction relative to the camera is the ping pong sound heard?",
    "answer": "From the right side.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01082.mp4",
    "question_id": "01082_7",
    "clip_path": "clips/01082/01082__0037500_0047500.mp4"
  },
  {
    "timestamp": "00:38 - 00:47",
    "context": "[00:38 - 00:47] The faint, distinct, and rhythmic sound of a ping pong ball being hit can be heard.",
    "question_type": "Sound Source Identification",
    "question": "What is the source of the faint, rhythmic sound heard during this segment?",
    "answer": "A ping pong ball being hit.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01082.mp4",
    "question_id": "01082_8",
    "clip_path": "clips/01082/01082__0037500_0047500.mp4"
  },
  {
    "timestamp": "00:38 - 00:47",
    "context": "[00:38 - 00:47] The ping pong ball sound is audible while he describes his navigation strategy.",
    "question_type": "Temporal Information",
    "question": "During which time interval is the ping pong sound heard?",
    "answer": "Between 00:38 and 00:47.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01082.mp4",
    "question_id": "01082_9",
    "clip_path": "clips/01082/01082__0037500_0047500.mp4"
  },
  {
    "timestamp": "00:38 - 00:47",
    "context": "[00:38 - 00:47] Man: “…听右边有一个打桌球的。过了它再往左拐就到了。”",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why does the man listen for the table tennis place on the right?",
    "answer": "He uses it as an auditory landmark; after passing it, turning left leads to the destination.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01082.mp4",
    "question_id": "01082_10",
    "clip_path": "clips/01082/01082__0037500_0047500.mp4"
  },
  {
    "timestamp": "00:38 - 00:47",
    "context": "[00:38 - 00:47] He explains the route and the ping pong sound is audible from the right.",
    "question_type": "Cross-Modal Reasoning",
    "question": "Based on his explanation and the heard ping pong sound, what action should be taken after passing that sound source?",
    "answer": "Turn left to reach the destination.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01082.mp4",
    "question_id": "01082_11",
    "clip_path": "clips/01082/01082__0037500_0047500.mp4"
  },
  {
    "timestamp": "00:47 - 00:52",
    "context": "[00:47 - 00:52] Companion: “现在还是有阳光的。你感觉到太阳晒在你身上了吗？” The user quietly affirms. The video ends with the continuous sound of their canes tapping.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the companion ask if the user could feel the sun shining on them?",
    "answer": "To check perception of sunlight via touch/temperature, emphasizing non-visual environmental awareness.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01082.mp4",
    "question_id": "01082_12",
    "clip_path": "clips/01082/01082__0046500_0052500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "A man on a three-wheeled motorcycle waits for them to pass, its engine idling with a low rumble.",
    "question_type": "Sound Source Identification",
    "question": "What generated the low rumble heard in the background at the beginning?",
    "answer": "The idling engine of a three-wheeled motorcycle waiting in the background.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01084.mp4",
    "question_id": "01084_1",
    "clip_path": "clips/01084/01084__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "The male companion, walking towards her from about 2 meters away, suggests, '再往前一个吧应该 (Let's go one more forward, I think).'",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "What was the approximate distance of the male companion’s speech relative to the camera when he suggested moving forward?",
    "answer": "About 2 meters away.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01084.mp4",
    "question_id": "01084_2",
    "clip_path": "clips/01084/01084__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "The female voice exclaims '哦,我知道 (Oh, I know).' Almost simultaneously, the male companion suggests, '再往前一个吧应该 (Let's go one more forward, I think).'",
    "question_type": "Temporal Information",
    "question": "What was the timing relationship between the woman's exclamation and the man's suggestion?",
    "answer": "They occurred almost simultaneously.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01084.mp4",
    "question_id": "01084_3",
    "clip_path": "clips/01084/01084__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:03 - 00:09",
    "context": "The female asks, '咦,那个超市是不是走过了? (Huh, did we walk past that supermarket?)' The man confirms, '走过了 (We walked past it).' The female then says, '我们往后退一点 (Let's back up a little).'",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the woman propose backing up a little?",
    "answer": "Because they realized they had walked past the supermarket.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01084.mp4",
    "question_id": "01084_4",
    "clip_path": "clips/01084/01084__0002500_0009500.mp4"
  },
  {
    "timestamp": "00:03 - 00:09",
    "context": "The man, now slightly in front and to the right, confirms, '走过了 (We walked past it).'",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From where relative to the camera did the man confirm they had walked past the supermarket?",
    "answer": "Slightly in front and to the right.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01084.mp4",
    "question_id": "01084_5",
    "clip_path": "clips/01084/01084__0002500_0009500.mp4"
  },
  {
    "timestamp": "00:09 - 00:12",
    "context": "As they turn to retrace their steps, the man says, '刚才那个音乐搞不好就是了 (The music just now might have been it),' providing a reason for their error.",
    "question_type": "Inferential & Contextual Causality",
    "question": "According to the man, what was the likely reason they missed the supermarket?",
    "answer": "They likely overlooked the music cue that indicated the supermarket.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01084.mp4",
    "question_id": "01084_6",
    "clip_path": "clips/01084/01084__0008500_0012500.mp4"
  },
  {
    "timestamp": "00:09 - 00:12",
    "context": "The man remarks, '刚才那个音乐搞不好就是了 (The music just now might have been it),' suggesting they use auditory cues from shops.",
    "question_type": "Sound Source Identification",
    "question": "What was the likely source of the 'music just now' mentioned by the man?",
    "answer": "The supermarket they were looking for.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01084.mp4",
    "question_id": "01084_7",
    "clip_path": "clips/01084/01084__0008500_0012500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "A three-wheeled motorcycle waits nearby, its engine idling with a low rumble.",
    "question_type": "Sound Characteristics",
    "question": "How is the motorcycle engine’s sound described while it waits?",
    "answer": "A low rumble while idling.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01084.mp4",
    "question_id": "01084_8",
    "clip_path": "clips/01084/01084__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:00 - 00:09",
    "context": "Two visually impaired individuals, a man and the person recording, are conversing on a busy street to navigate.",
    "question_type": "Counting",
    "question": "How many people are conversing during this segment?",
    "answer": "Two.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01084.mp4",
    "question_id": "01084_9",
    "clip_path": "clips/01084/01084__0000000_0009500.mp4"
  },
  {
    "timestamp": "00:00 - 00:09",
    "context": "[00:00 - 00:09] A visually impaired woman says she wants to buy green peppers and asks for help. Another woman asks if she wants to touch them and places the guide's hand over her hand, saying, \"This is green pepper.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the guide place her hand over the visually impaired woman's hand and guide it to the peppers at 00:00 - 00:09?",
    "answer": "To help her tactilely identify the green peppers she wanted to buy.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01087.mp4",
    "question_id": "01087_1",
    "clip_path": "clips/01087/01087__0000000_0009500.mp4"
  },
  {
    "timestamp": "00:09 - 00:16",
    "context": "[00:09 - 00:16] The guide distinguishes pepper varieties and says, \"The one you touched is spicy. This one is not spicy. Here, feel it.\" The sound of hands gently rubbing the smooth peppers is audible.",
    "question_type": "Sound Source Identification",
    "question": "What produced the gentle rubbing sound heard between 00:09 and 00:16?",
    "answer": "Their hands rubbing against the smooth surface of the peppers.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01087.mp4",
    "question_id": "01087_2",
    "clip_path": "clips/01087/01087__0008500_0016500.mp4"
  },
  {
    "timestamp": "00:20 - 00:30",
    "context": "[00:20 - 00:30] A male guide from the right hands them a plastic bag, which makes a loud, sharp rustling sound as it is opened.",
    "question_type": "Sound Characteristics",
    "question": "How is the plastic bag's opening sound described at 00:20 - 00:30?",
    "answer": "A loud, sharp rustling sound.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01087.mp4",
    "question_id": "01087_3",
    "clip_path": "clips/01087/01087__0019500_0030500.mp4"
  },
  {
    "timestamp": "00:09 - 00:16",
    "context": "[00:09 - 00:16] The guide explains from the left which pepper is spicy and which is not.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From which direction relative to the camera did the guide's explanatory voice originate at 00:09 - 00:16?",
    "answer": "From the left.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01087.mp4",
    "question_id": "01087_4",
    "clip_path": "clips/01087/01087__0008500_0016500.mp4"
  },
  {
    "timestamp": "00:30 - 00:43",
    "context": "[00:30 - 00:43] The woman asks the price and the male guide replies from her right, \"3.98 yuan.\"",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From which direction relative to the camera did the male guide state the price at 00:30 - 00:43?",
    "answer": "From the right.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01087.mp4",
    "question_id": "01087_5",
    "clip_path": "clips/01087/01087__0029500_0043500.mp4"
  },
  {
    "timestamp": "00:55 - 01:11",
    "context": "[00:55 - 01:01] As they begin to walk, a distinct, light tapping sound is heard. [01:01 - 01:11] The tapping remains audible as they walk toward tomatoes.",
    "question_type": "Temporal Information",
    "question": "When did the tapping sound begin and how long did it persist?",
    "answer": "It began as they started walking at 00:55 - 01:01 and continued throughout 01:01 - 01:11.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01087.mp4",
    "question_id": "01087_6",
    "clip_path": "clips/01087/01087__0054500_0071500.mp4"
  },
  {
    "timestamp": "00:55 - 01:01",
    "context": "[00:55 - 01:01] The visually impaired woman asks to hold the guide’s shoulder; as they start walking, a distinct, light tapping sound is audible.",
    "question_type": "Sound Source Identification",
    "question": "What generated the distinct, light tapping sound at 00:55 - 01:01?",
    "answer": "Her white cane tapping on the supermarket’s smooth floor.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01087.mp4",
    "question_id": "01087_7",
    "clip_path": "clips/01087/01087__0054500_0061500.mp4"
  },
  {
    "timestamp": "00:30 - 00:43",
    "context": "[00:30 - 00:43] While selecting peppers, the woman counts aloud, \"Two, three... I'll buy four. I'll get another big one.\"",
    "question_type": "Counting",
    "question": "How many peppers did the woman decide to buy?",
    "answer": "Four.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01087.mp4",
    "question_id": "01087_8",
    "clip_path": "clips/01087/01087__0029500_0043500.mp4"
  },
  {
    "timestamp": "00:55 - 01:01",
    "context": "[00:55 - 01:01] As they begin to walk, the distinct, light tapping of the white cane becomes audible, signaling their movement away from the vegetable stand.",
    "question_type": "Cross-Modal Reasoning",
    "question": "What does the onset of the cane tapping sound indicate about the group's movement at 00:55 - 01:01?",
    "answer": "They began walking away from the vegetable stand.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01087.mp4",
    "question_id": "01087_9",
    "clip_path": "clips/01087/01087__0054500_0061500.mp4"
  },
  {
    "timestamp": "00:30 - 00:43",
    "context": "[00:30 - 00:43] The male guide replies from the right, \"3.98 yuan,\" confirming the price shown on the sign behind the display.",
    "question_type": "Cross-Modal Reasoning",
    "question": "When the male guide states the price, what visual information does his statement confirm?",
    "answer": "The price shown on the sign behind the display (3.98 yuan).",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01087.mp4",
    "question_id": "01087_10",
    "clip_path": "clips/01087/01087__0029500_0043500.mp4"
  },
  {
    "timestamp": "00:20 - 00:30; 00:30 - 00:43",
    "context": "[00:20 - 00:30] The plastic bag creates loud, sharp rustling as it is opened and continues as she places the first pepper inside. [00:30 - 00:43] After placing the last pepper in the bag, it rustles again.",
    "question_type": "Temporal Information",
    "question": "During which moments is the bag rustling audible, and in relation to what actions?",
    "answer": "When the bag is opened and as the first pepper is placed inside (00:20 - 00:30), and again after the last pepper is placed (00:30 - 00:43).",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01087.mp4",
    "question_id": "01087_11",
    "clip_path": "clips/01087/01087__0019500_0043500.mp4"
  },
  {
    "timestamp": "01:11 - 01:18",
    "context": "[01:11 - 01:18] She hands a brown jacket to the male guide, accompanied by the soft rustling sound of the fabric.",
    "question_type": "Sound Characteristics",
    "question": "What is the acoustic quality of the fabric sound when she hands over the jacket at 01:11 - 01:18?",
    "answer": "A soft rustling sound.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01087.mp4",
    "question_id": "01087_12",
    "clip_path": "clips/01087/01087__0070500_0078500.mp4"
  },
  {
    "timestamp": "01:01 - 01:11",
    "context": "[01:01 - 01:11] Another guide’s voice from ahead gives continuous verbal directions: \"This way, this way... this way...\"",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From which direction relative to the camera did the guiding voice giving directions originate at 01:01 - 01:11?",
    "answer": "From ahead.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01087.mp4",
    "question_id": "01087_13",
    "clip_path": "clips/01087/01087__0060500_0071500.mp4"
  },
  {
    "timestamp": "00:43 - 00:55",
    "context": "[00:43 - 00:55] The male guide points to a different pepper; neither knows if it is spicy. The woman laughs and says, \"I'll just stick with the one I'm sure isn't spicy.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the woman decide to stick with the known non-spicy peppers at 00:43 - 00:55?",
    "answer": "Because they were unsure if the other variety was spicy, so she chose the one she knew was not spicy.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01087.mp4",
    "question_id": "01087_14",
    "clip_path": "clips/01087/01087__0042500_0055500.mp4"
  },
  {
    "timestamp": "00:00 - 00:09",
    "context": "[00:00 - 00:09] The woman speaks in a clear, moderate voice from directly in front of the camera.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the woman's initial speech originate relative to the camera?",
    "answer": "Directly in front of the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01087.mp4",
    "question_id": "01087_15",
    "clip_path": "clips/01087/01087__0000000_0009500.mp4"
  },
  {
    "timestamp": "01:01 - 01:11",
    "context": "[01:01 - 01:11] As they walk, another guide’s voice from ahead gives continuous directions: \"This way, this way... this way...\"",
    "question_type": "Temporal Information",
    "question": "Are the verbal directions during 01:01 - 01:11 brief or continuous?",
    "answer": "They are continuous.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01087.mp4",
    "question_id": "01087_16",
    "clip_path": "clips/01087/01087__0060500_0071500.mp4"
  },
  {
    "timestamp": "00:12 - 00:14",
    "context": "As a man on an electric scooter passes by on the right, emitting a low, continuous hum, the person next to the user adds another warning: \"Step carefully.\" This emphasizes the immediate need for caution due to uneven terrain and nearby traffic.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the person next to the user say \"Step carefully\" at 00:12–00:14?",
    "answer": "Because an electric scooter was passing on the right and the terrain was uneven, creating an immediate need for caution.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01094.mp4",
    "question_id": "01094_1",
    "clip_path": "clips/01094/01094__0011500_0014489.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "The white canes produce distinct, sharp tapping sounds on the concrete floor with each step.",
    "question_type": "Sound Source Identification",
    "question": "What generated the distinct, sharp tapping sounds at the start?",
    "answer": "The white canes striking the concrete floor with each step.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01094.mp4",
    "question_id": "01094_2",
    "clip_path": "clips/01094/01094__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "The white canes produce distinct, sharp tapping sounds on the concrete floor with each step.",
    "question_type": "Sound Characteristics",
    "question": "How are the cane-tapping sounds described?",
    "answer": "Distinct and sharp.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01094.mp4",
    "question_id": "01094_3",
    "clip_path": "clips/01094/01094__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:03 - 00:09",
    "context": "The rhythmic tapping of the canes on the uneven ground remains a constant, clear sound.",
    "question_type": "Temporal Information",
    "question": "Between 00:03 and 00:09, was the cane-tapping rhythm intermittent or continuous?",
    "answer": "It remained a constant, clear sound.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01094.mp4",
    "question_id": "01094_4",
    "clip_path": "clips/01094/01094__0002500_0009500.mp4"
  },
  {
    "timestamp": "00:03 - 00:09",
    "context": "The guide provides further instructions from the front, saying, \"Keep going right, keep going right.\"",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From where relative to the camera did the guiding instructions originate during 00:03–00:09?",
    "answer": "From the front.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01094.mp4",
    "question_id": "01094_5",
    "clip_path": "clips/01094/01094__0002500_0009500.mp4"
  },
  {
    "timestamp": "00:09 - 00:12",
    "context": "Approaching a more difficult section, the user's companion extends their telescopic white cane, producing a series of soft metallic clicks and a brief scraping sound as it locks into place.",
    "question_type": "Sound Source Identification",
    "question": "What produced the series of soft metallic clicks and the brief scraping sound at 00:09–00:12?",
    "answer": "The companion’s telescopic white cane extending and locking into place.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01094.mp4",
    "question_id": "01094_6",
    "clip_path": "clips/01094/01094__0008500_0012500.mp4"
  },
  {
    "timestamp": "00:09 - 00:12",
    "context": "The telescopic white cane produces a series of soft metallic clicks and a brief scraping sound as it locks into place.",
    "question_type": "Counting",
    "question": "How many distinct sounds associated with the telescopic cane’s adjustment are mentioned?",
    "answer": "Two: soft metallic clicks and a brief scraping sound.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01094.mp4",
    "question_id": "01094_7",
    "clip_path": "clips/01094/01094__0008500_0012500.mp4"
  },
  {
    "timestamp": "00:12 - 00:14",
    "context": "A man on an electric scooter passes by on the right, emitting a low, continuous hum.",
    "question_type": "Sound Characteristics",
    "question": "What are the acoustic characteristics of the electric scooter’s sound at 00:12–00:14?",
    "answer": "A low, continuous hum.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01094.mp4",
    "question_id": "01094_8",
    "clip_path": "clips/01094/01094__0011500_0014489.mp4"
  },
  {
    "timestamp": "00:12 - 00:14",
    "context": "A man on an electric scooter passes by on the right.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "On which side relative to the camera did the electric scooter pass?",
    "answer": "On the right.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01094.mp4",
    "question_id": "01094_9",
    "clip_path": "clips/01094/01094__0011500_0014489.mp4"
  },
  {
    "timestamp": "00:09 - 00:12",
    "context": "The cane produces soft metallic clicks and a brief scraping sound as it locks into place.",
    "question_type": "Cross-Modal Reasoning",
    "question": "After the clicks and brief scraping, what action was likely completed?",
    "answer": "The telescopic white cane finished extending and locked into place.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01094.mp4",
    "question_id": "01094_10",
    "clip_path": "clips/01094/01094__0008500_0012500.mp4"
  },
  {
    "timestamp": "00:03 - 00:09",
    "context": "As they continue walking, the guide provides further instructions. The user and their companion say \"Thank you\" multiple times, and the guide responds, \"Okay, thank you,\" confirming their cooperation.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the user and their companion say \"Thank you\" multiple times between 00:03 and 00:09?",
    "answer": "They were expressing gratitude for the guide’s ongoing instructions and assistance.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01094.mp4",
    "question_id": "01094_11",
    "clip_path": "clips/01094/01094__0002500_0009500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "A visually impaired person and their companion, both using white canes. The white canes produce distinct, sharp tapping sounds with each step.",
    "question_type": "Counting",
    "question": "How many white canes are producing the tapping sounds at the start?",
    "answer": "Two, one used by the camera user and one by their companion.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01094.mp4",
    "question_id": "01094_12",
    "clip_path": "clips/01094/01094__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "[00:00 - 00:06] As the user walks into a museum hall, a companion speaks nearby: \"Just now we were outside... bronze statues, right? There is a special exhibition for them inside.\" The speech provides a reason for entering the building.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Based on the companion's explanation, why are they entering the building at this moment?",
    "answer": "To see the special exhibition of bronze statues inside.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01096.mp4",
    "question_id": "01096_1",
    "clip_path": "clips/01096/01096__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "[00:00 - 00:06] A companion speaks in a clear, conversational tone from nearby, explaining the bronze statue exhibition.",
    "question_type": "Sound Source Identification",
    "question": "Who is the source of the spoken explanation heard as they enter the hall?",
    "answer": "The user's companion.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01096.mp4",
    "question_id": "01096_2",
    "clip_path": "clips/01096/01096__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "[00:00 - 00:06] The companion speaks in a clear, conversational tone from nearby.",
    "question_type": "Sound Characteristics",
    "question": "How is the companion's speech characterized in terms of tone and clarity?",
    "answer": "It is clear and conversational.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01096.mp4",
    "question_id": "01096_3",
    "clip_path": "clips/01096/01096__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "[00:00 - 00:06] The companion speaks from nearby as the user enters the hall.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the companion's speech originate relative to the camera?",
    "answer": "From nearby.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01096.mp4",
    "question_id": "01096_4",
    "clip_path": "clips/01096/01096__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "[00:00 - 00:06] As the user walks through the doorway, the companion delivers a short explanation about the exhibition.",
    "question_type": "Temporal Information",
    "question": "When did the companion deliver the explanation, and was it brief or extended?",
    "answer": "During 00:00–00:06 as they walked through the doorway; it was brief.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01096.mp4",
    "question_id": "01096_5",
    "clip_path": "clips/01096/01096__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "[00:00 - 00:06] Only the companion's speech is explicitly described as an audible event.",
    "question_type": "Counting",
    "question": "How many distinct sound sources are explicitly mentioned in this segment?",
    "answer": "One—the companion speaking.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01096.mp4",
    "question_id": "01096_6",
    "clip_path": "clips/01096/01096__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:05 - 00:08",
    "context": "[00:05 - 00:08] Amidst a constant, low-volume murmur of a surrounding crowd, a woman's clear voice from the immediate left exclaims, “我天呐我找你半天” (Oh my god, I've been looking for you for ages). She approaches wearing a fluffy cream-colored coat and holds out a smartphone, implying she had been trying to make contact while searching in the busy public space.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Based on the context, why does the woman speak in a tone of exasperated relief and hold out a smartphone?",
    "answer": "Because she had been searching for the camera wearer in the busy public space and had been trying to contact them by phone, finally finding them.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01100.mp4",
    "question_id": "01100_1",
    "clip_path": "clips/01100/01100__0004500_0008500.mp4"
  },
  {
    "timestamp": "00:05 - 00:08",
    "context": "[00:05 - 00:08] A woman's clear voice emerges from the immediate left of the camera as she approaches wearing a fluffy cream-colored coat.",
    "question_type": "Sound Source Identification",
    "question": "Who generated the clear voice heard in this segment?",
    "answer": "A woman wearing a fluffy cream-colored coat approaching the camera wearer.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01100.mp4",
    "question_id": "01100_2",
    "clip_path": "clips/01100/01100__0004500_0008500.mp4"
  },
  {
    "timestamp": "00:05 - 00:08",
    "context": "[00:05 - 00:08] There is a constant, low-volume murmur from a surrounding crowd.",
    "question_type": "Sound Characteristics",
    "question": "What is the volume and nature of the surrounding crowd sound during this interval?",
    "answer": "It is a constant, low-volume murmur.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01100.mp4",
    "question_id": "01100_3",
    "clip_path": "clips/01100/01100__0004500_0008500.mp4"
  },
  {
    "timestamp": "00:05 - 00:08",
    "context": "[00:05 - 00:08] The woman exclaims, “我天呐我找你半天,” in a tone described as exasperated relief.",
    "question_type": "Sound Characteristics",
    "question": "What is the emotional tone of the woman’s exclamation?",
    "answer": "Exasperated relief.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01100.mp4",
    "question_id": "01100_4",
    "clip_path": "clips/01100/01100__0004500_0008500.mp4"
  },
  {
    "timestamp": "00:05 - 00:08",
    "context": "[00:05 - 00:08] The woman's voice emerges from the immediate left of the camera.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From which direction relative to the camera does the woman's voice originate?",
    "answer": "From the immediate left of the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01100.mp4",
    "question_id": "01100_5",
    "clip_path": "clips/01100/01100__0004500_0008500.mp4"
  },
  {
    "timestamp": "00:05 - 00:08",
    "context": "[00:05 - 00:08] A constant, low-volume murmur of a surrounding crowd is audible throughout the segment.",
    "question_type": "Temporal Information",
    "question": "Is the crowd murmur brief or continuous during 00:05–00:08?",
    "answer": "It is continuous throughout the interval.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01100.mp4",
    "question_id": "01100_6",
    "clip_path": "clips/01100/01100__0004500_0008500.mp4"
  },
  {
    "timestamp": "00:05 - 00:08",
    "context": "[00:05 - 00:08] The audio features a surrounding crowd murmur and a woman's clear exclamation.",
    "question_type": "Counting",
    "question": "How many distinct sound sources are mentioned during this segment?",
    "answer": "Two: the surrounding crowd murmur and the woman's exclamation.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01100.mp4",
    "question_id": "01100_7",
    "clip_path": "clips/01100/01100__0004500_0008500.mp4"
  },
  {
    "timestamp": "00:00 - 00:18",
    "context": "[00:00 - 00:18] The surrounding environment is filled with the low, distant hum of city traffic.",
    "question_type": "Temporal Information",
    "question": "Is the city traffic hum continuous or intermittent during 00:00–00:18?",
    "answer": "It is continuous throughout the segment.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01104.mp4",
    "question_id": "01104_1",
    "clip_path": "clips/01104/01104__0000000_0018500.mp4"
  },
  {
    "timestamp": "00:00 - 00:18",
    "context": "[00:00 - 00:18] The user stands on a city sidewalk next to a busy street; the environment is filled with the low, distant hum of city traffic.",
    "question_type": "Sound Source Identification",
    "question": "What generated the low, distant hum heard in the background?",
    "answer": "City traffic from the busy street.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01104.mp4",
    "question_id": "01104_2",
    "clip_path": "clips/01104/01104__0000000_0018500.mp4"
  },
  {
    "timestamp": "00:00 - 00:18",
    "context": "[00:00 - 00:18] The user says: \"Right now, I'm calling a car and waiting for it, and then I'll meet up with my friend to go to the museum.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Based on the monologue, why is the user calling a car at this moment?",
    "answer": "To meet up with a friend and go to the museum.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01104.mp4",
    "question_id": "01104_3",
    "clip_path": "clips/01104/01104__0000000_0018500.mp4"
  },
  {
    "timestamp": "00:00 - 00:18",
    "context": "[00:00 - 00:18] The user says: \"Today I'm going to the museum to see a very, very special exhibition.\"",
    "question_type": "Counting",
    "question": "How many times does the user say the word \"very\" when describing the exhibition?",
    "answer": "Twice.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01104.mp4",
    "question_id": "01104_4",
    "clip_path": "clips/01104/01104__0000000_0018500.mp4"
  },
  {
    "timestamp": "00:00 - 00:18",
    "context": "[00:00 - 00:18] The user delivers a clear monologue directly to the camera.",
    "question_type": "Sound Characteristics",
    "question": "How is the clarity of the user's monologue?",
    "answer": "Clear.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01104.mp4",
    "question_id": "01104_5",
    "clip_path": "clips/01104/01104__0000000_0018500.mp4"
  },
  {
    "timestamp": "00:00 - 00:18",
    "context": "[00:00 - 00:18] The surrounding environment is filled with the low, distant hum of city traffic.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Is the background traffic noise near or distant relative to the camera?",
    "answer": "Distant.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01104.mp4",
    "question_id": "01104_6",
    "clip_path": "clips/01104/01104__0000000_0018500.mp4"
  },
  {
    "timestamp": "00:00 - 00:18",
    "context": "[00:00 - 00:18] The surrounding environment is filled with the low, distant hum of city traffic.",
    "question_type": "Sound Characteristics",
    "question": "What are the volume and texture characteristics of the background traffic sound?",
    "answer": "It is a low, distant hum.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01104.mp4",
    "question_id": "01104_7",
    "clip_path": "clips/01104/01104__0000000_0018500.mp4"
  },
  {
    "timestamp": "00:00 - 00:18",
    "context": "[00:00 - 00:18] The user speaks while a background traffic hum persists.",
    "question_type": "Counting",
    "question": "How many distinct environmental sounds (excluding speech) are audible in this segment?",
    "answer": "One—the ambient hum of city traffic.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01104.mp4",
    "question_id": "01104_8",
    "clip_path": "clips/01104/01104__0000000_0018500.mp4"
  },
  {
    "timestamp": "00:00 - 00:08",
    "context": "A hand in a white sleeve reaches out and touches a large, rough-textured sculpture; a quiet conversation begins. The person asks, “是人物吗?” while the companion stands on the right holding a rustling paper map.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the person ask “是人物吗?” (Is it a person?) when the conversation began?",
    "answer": "Because she was inferring the sculpture’s form by touch and wanted to confirm whether it depicted a person.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01109.mp4",
    "question_id": "01109_1",
    "clip_path": "clips/01109/01109__0000000_0008500.mp4"
  },
  {
    "timestamp": "00:00 - 00:08",
    "context": "The companion standing close by on the right holds a paper map, which produces a soft rustling sound as it is moved.",
    "question_type": "Sound Source Identification",
    "question": "What produced the soft rustling sound during 00:00–00:08?",
    "answer": "The paper map held by the companion.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01109.mp4",
    "question_id": "01109_2",
    "clip_path": "clips/01109/01109__0000000_0008500.mp4"
  },
  {
    "timestamp": "00:00 - 00:08",
    "context": "The companion is positioned on the right side, close to the person touching the sculpture, and the map rustles softly.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From where relative to the camera did the rustling map sound originate?",
    "answer": "From the right side, close by.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01109.mp4",
    "question_id": "01109_3",
    "clip_path": "clips/01109/01109__0000000_0008500.mp4"
  },
  {
    "timestamp": "00:13 - 00:15",
    "context": "While sliding their hand over the vast surface of the sculpture, a soft, continuous rubbing sound is audible across the coarse material.",
    "question_type": "Sound Characteristics",
    "question": "What are the acoustic qualities of the rubbing sound heard between 00:13 and 00:15?",
    "answer": "It is soft and continuous, produced by contact with a coarse surface.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01109.mp4",
    "question_id": "01109_4",
    "clip_path": "clips/01109/01109__0012500_0015500.mp4"
  },
  {
    "timestamp": "00:13 - 00:15",
    "context": "The person continues to slide their hand over the sculpture, producing a rubbing sound.",
    "question_type": "Sound Source Identification",
    "question": "What generated the soft, continuous rubbing sound at 00:13–00:15?",
    "answer": "The person’s hand moving across the rough-textured surface of the sculpture.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01109.mp4",
    "question_id": "01109_5",
    "clip_path": "clips/01109/01109__0012500_0015500.mp4"
  },
  {
    "timestamp": "00:00 - 00:08",
    "context": "The person asks, “是人物吗?” and the companion immediately confirms, “对, 特别大, 所以说你可能摸不出来.”",
    "question_type": "Temporal Information",
    "question": "How quickly did the companion respond after the person asked “是人物吗?”",
    "answer": "Immediately.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01109.mp4",
    "question_id": "01109_6",
    "clip_path": "clips/01109/01109__0000000_0008500.mp4"
  },
  {
    "timestamp": "00:00 - 00:08",
    "context": "The first person asks two questions: “是人物吗?” and “这是他的什么?”",
    "question_type": "Counting",
    "question": "How many questions did the first person ask during 00:00–00:08?",
    "answer": "Two.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01109.mp4",
    "question_id": "01109_7",
    "clip_path": "clips/01109/01109__0000000_0008500.mp4"
  },
  {
    "timestamp": "00:13 - 00:15",
    "context": "As the person slides their hand over the sculpture, they exclaim, “好大呀, 我都摸不完他.” The visual context includes the companion holding a white mobility cane, suggesting the person relies on touch.",
    "question_type": "Cross-Modal Reasoning",
    "question": "Why did the person exclaim, “好大呀, 我都摸不完他” (It’s so big, I can’t even feel all of it)?",
    "answer": "Because while exploring the sculpture by touch they realized its large scale; the visual cue of a mobility cane suggests they rely on touch to perceive it.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01109.mp4",
    "question_id": "01109_8",
    "clip_path": "clips/01109/01109__0012500_0015500.mp4"
  },
  {
    "timestamp": "00:00 - 00:08",
    "context": "As the hand touches the sculpture, a quiet conversation begins between the two people.",
    "question_type": "Temporal Information",
    "question": "When did the conversation begin relative to the touching of the sculpture?",
    "answer": "It began as the hand reached out and touched the sculpture.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01109.mp4",
    "question_id": "01109_9",
    "clip_path": "clips/01109/01109__0000000_0008500.mp4"
  },
  {
    "timestamp": "00:00 - 00:08",
    "context": "The companion speaks and a paper map rustles softly; the companion likely reads, replying with “雕塑馆.”",
    "question_type": "Cross-Modal Reasoning",
    "question": "What item was the companion likely referencing while replying “雕塑馆,” as indicated by the accompanying sound?",
    "answer": "A paper map, evidenced by the soft rustling as it was handled.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01109.mp4",
    "question_id": "01109_10",
    "clip_path": "clips/01109/01109__0000000_0008500.mp4"
  },
  {
    "timestamp": "00:00 - 00:08",
    "context": "The person initiates the conversation in a clear, female voice at close range.",
    "question_type": "Sound Characteristics",
    "question": "How is the quality of the initial speaker’s voice described when she asks her first question?",
    "answer": "Clear and close-range.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01109.mp4",
    "question_id": "01109_11",
    "clip_path": "clips/01109/01109__0000000_0008500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "[00:00 - 00:03] A continuous, rapid series of moderate-volume mechanical thuds emanates from a four-legged robot dog running toward the camera. The sound is caused by the robot's legs striking the pavement.",
    "question_type": "Sound Source Identification",
    "question": "What generated the rapid mechanical thuds heard between 00:00 and 00:03?",
    "answer": "The robot dog's legs striking the stone pavement as it ran.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01115.mp4",
    "question_id": "01115_1",
    "clip_path": "clips/01115/01115__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "[00:00 - 00:03] The robot runs directly toward the camera user, with the sound growing louder as it approaches from about 5 meters away.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From what direction and approximate starting distance did the thuds approach the camera?",
    "answer": "From directly ahead, approaching from about 5 meters away.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01115.mp4",
    "question_id": "01115_2",
    "clip_path": "clips/01115/01115__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "[00:00 - 00:03] A continuous, rapid series of moderate-volume mechanical thuds is heard as the robot runs.",
    "question_type": "Sound Characteristics",
    "question": "How are the thuds characterized in terms of continuity, rate, and volume?",
    "answer": "They are a continuous, rapid series of moderate-volume mechanical thuds.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01115.mp4",
    "question_id": "01115_3",
    "clip_path": "clips/01115/01115__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "[00:00 - 00:03] The thuds grow progressively louder as the robot approaches.",
    "question_type": "Temporal Information",
    "question": "How did the volume of the thuds change during 00:00–00:03?",
    "answer": "It progressively grew louder as the robot approached.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01115.mp4",
    "question_id": "01115_4",
    "clip_path": "clips/01115/01115__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:03 - 00:08",
    "context": "[00:03 - 00:08] The robot stops directly in front. The user's left hand pats the top of the robot, creating a soft tapping sound. This action triggers a synthesized male voice announcing: “当前电量为58%”.",
    "question_type": "Cross-Modal Reasoning",
    "question": "What action triggered the robot's synthesized voice announcement at 00:03–00:08?",
    "answer": "The user's pat on the top of the robot.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01115.mp4",
    "question_id": "01115_5",
    "clip_path": "clips/01115/01115__0002500_0008500.mp4"
  },
  {
    "timestamp": "00:03 - 00:08",
    "context": "[00:03 - 00:08] The user's pat creates a soft tapping sound on the robot.",
    "question_type": "Sound Characteristics",
    "question": "What was the acoustic quality of the patting sound when the user touched the robot?",
    "answer": "A soft tapping sound.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01115.mp4",
    "question_id": "01115_6",
    "clip_path": "clips/01115/01115__0002500_0008500.mp4"
  },
  {
    "timestamp": "00:03 - 00:08",
    "context": "[00:03 - 00:08] A clear, synthesized male voice from the robot states: “当前电量为58%”.",
    "question_type": "Sound Characteristics",
    "question": "How is the robot's spoken announcement characterized acoustically?",
    "answer": "It is a clear, synthesized male voice.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01115.mp4",
    "question_id": "01115_7",
    "clip_path": "clips/01115/01115__0002500_0008500.mp4"
  },
  {
    "timestamp": "00:03 - 00:08",
    "context": "[00:03 - 00:08] The robot is directly in front of the user when the synthesized voice speaks.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the synthesized voice originate relative to the camera?",
    "answer": "Directly in front, from the robot halted before the user.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01115.mp4",
    "question_id": "01115_8",
    "clip_path": "clips/01115/01115__0002500_0008500.mp4"
  },
  {
    "timestamp": "00:03 - 00:08",
    "context": "[00:03 - 00:08] A bystander encourages the interaction, saying “摸摸” (Touch it, touch it).",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the bystander say “摸摸” during 00:03–00:08?",
    "answer": "To encourage the user to touch/pat the robot.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01115.mp4",
    "question_id": "01115_9",
    "clip_path": "clips/01115/01115__0002500_0008500.mp4"
  },
  {
    "timestamp": "00:08 - 00:13",
    "context": "[00:08 - 00:13] The user says, “好搞笑在跑步” (So funny, it's running), followed by a lighthearted laugh: “哈哈哈”.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the user laugh between 00:08 and 00:13?",
    "answer": "Because she found the robot's running amusing.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01115.mp4",
    "question_id": "01115_10",
    "clip_path": "clips/01115/01115__0007500_0013500.mp4"
  },
  {
    "timestamp": "00:13 - 00:18",
    "context": "[00:13 - 00:18] The robot turns around and runs away, producing mechanical thuds whose volume decreases as it moves further away.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the mechanical thuds decrease in volume during 00:13–00:18?",
    "answer": "Because the robot ran away from the user, increasing its distance.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01115.mp4",
    "question_id": "01115_11",
    "clip_path": "clips/01115/01115__0012500_0018500.mp4"
  },
  {
    "timestamp": "00:18 - 00:21",
    "context": "[00:18 - 00:21] A man asks, “这个叫什么小哥?” Another man replies from the side, “叫铁蛋”.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From what direction relative to the camera did the reply revealing the robot's name come?",
    "answer": "From the side.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01115.mp4",
    "question_id": "01115_12",
    "clip_path": "clips/01115/01115__0017500_0021500.mp4"
  },
  {
    "timestamp": "00:21 - 00:26",
    "context": "[00:03 - 00:08 and 00:21 - 00:26] After pats, the robot's synthesized voice states: “当前电量为58%” both times.",
    "question_type": "Counting",
    "question": "How many times did the robot announce its battery level across the scene, and when?",
    "answer": "Twice—at 00:03–00:08 and again at 00:21–00:26, both stating “当前电量为58%”.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01115.mp4",
    "question_id": "01115_13",
    "clip_path": "clips/01115/01115__0020500_0026500.mp4"
  },
  {
    "timestamp": "00:21 - 00:26",
    "context": "[00:21 - 00:26] The user says “铁蛋…摸头” and pats the robot, producing a soft patting sound. The robot immediately responds with the same synthesized voice stating: “当前电量为58%”.",
    "question_type": "Cross-Modal Reasoning",
    "question": "After the user said “摸头” and patted the robot again, what sound followed and what did it indicate?",
    "answer": "The robot's synthesized voice repeated, stating its battery level: “当前电量为58%”.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01115.mp4",
    "question_id": "01115_14",
    "clip_path": "clips/01115/01115__0020500_0026500.mp4"
  },
  {
    "timestamp": "00:00 - 00:08",
    "context": "A male voice, coming from the front, comments on the user's adaptation to the robot, saying, 'Oh, indeed, you can now accept its sudden stops.' Another person chimes in, 'Got used to it.'",
    "question_type": "Inferential & Contextual Causality",
    "question": "Based on the immediate context, why did the male voice make the comment about 'accept its sudden stops'?",
    "answer": "Because the user had adapted to the robot’s behavior and could handle its sudden stops.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01116.mp4",
    "question_id": "01116_1",
    "clip_path": "clips/01116/01116__0000000_0008500.mp4"
  },
  {
    "timestamp": "00:00 - 00:08",
    "context": "A male voice, coming from the front, comments on the user's adaptation to the robot.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From what direction relative to the camera did the male voice delivering the initial comment originate?",
    "answer": "From the front.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01116.mp4",
    "question_id": "01116_2",
    "clip_path": "clips/01116/01116__0000000_0008500.mp4"
  },
  {
    "timestamp": "00:08 - 00:13",
    "context": "A male voice from the front makes a lighthearted joke: 'Challenge successful, time for the award acceptance speech.' The user lets out a short laugh and repeats, 'Award acceptance speech.'",
    "question_type": "Inferential & Contextual Causality",
    "question": "What prompted the user's short laugh during 00:08–00:13?",
    "answer": "The male voice’s lighthearted joke about an 'award acceptance speech.'",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01116.mp4",
    "question_id": "01116_3",
    "clip_path": "clips/01116/01116__0007500_0013500.mp4"
  },
  {
    "timestamp": "00:08 - 00:13",
    "context": "The user lets out a short laugh in response, a soft, high-pitched 'haha' sound.",
    "question_type": "Sound Characteristics",
    "question": "What were the qualities of the user's laugh?",
    "answer": "It was short, soft, and high-pitched— a brief 'haha.'",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01116.mp4",
    "question_id": "01116_4",
    "clip_path": "clips/01116/01116__0007500_0013500.mp4"
  },
  {
    "timestamp": "00:08 - 00:13",
    "context": "A male voice from the front makes a lighthearted joke.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the joking male voice come from relative to the camera?",
    "answer": "From the front.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01116.mp4",
    "question_id": "01116_5",
    "clip_path": "clips/01116/01116__0007500_0013500.mp4"
  },
  {
    "timestamp": "00:13 - 00:23",
    "context": "The user provides their feedback on the experience, speaking in a clear, medium-volume voice.",
    "question_type": "Sound Characteristics",
    "question": "How is the user's feedback monologue delivered in terms of clarity and volume?",
    "answer": "In a clear, medium-volume voice.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01116.mp4",
    "question_id": "01116_6",
    "clip_path": "clips/01116/01116__0012500_0023500.mp4"
  },
  {
    "timestamp": "00:13 - 00:23",
    "context": "The user provides their feedback on the experience in a continuous monologue.",
    "question_type": "Temporal Information",
    "question": "When does the user's feedback monologue occur, and is it continuous over that interval?",
    "answer": "From 00:13 to 00:23, and it is continuous.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01116.mp4",
    "question_id": "01116_7",
    "clip_path": "clips/01116/01116__0012500_0023500.mp4"
  },
  {
    "timestamp": "00:23 - 00:30",
    "context": "Another person says, 'Its strength is actually sufficient.' The user agrees and adds, 'At the beginning, I was very worried that if I propped myself on it, it would fall over.'",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the user express worry that the robot might fall over if they propped themselves on it?",
    "answer": "They were concerned about the robot’s stability when bearing their weight, in response to discussion of its strength.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01116.mp4",
    "question_id": "01116_8",
    "clip_path": "clips/01116/01116__0022500_0030500.mp4"
  },
  {
    "timestamp": "00:30 - 00:41",
    "context": "A male voice asks if the user puts their weight on the robot. The user explains conditions and confirms, 'Yes, a little bit.'",
    "question_type": "Inferential & Contextual Causality",
    "question": "What was the purpose of the male voice asking whether the user puts their weight on the robot?",
    "answer": "To further explore and clarify the robot’s stability under load.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01116.mp4",
    "question_id": "01116_9",
    "clip_path": "clips/01116/01116__0029500_0041500.mp4"
  },
  {
    "timestamp": "00:41 - 00:42",
    "context": "The user brings both hands up and claps them together twice, producing two sharp, distinct clapping sounds.",
    "question_type": "Sound Source Identification",
    "question": "What generated the two sharp clapping sounds at 00:41–00:42?",
    "answer": "The user's hands clapping together.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01116.mp4",
    "question_id": "01116_10",
    "clip_path": "clips/01116/01116__0040500_0042500.mp4"
  },
  {
    "timestamp": "00:41 - 00:42",
    "context": "The user claps their hands together twice, producing two sharp, distinct clapping sounds.",
    "question_type": "Counting",
    "question": "How many claps were produced by the user?",
    "answer": "Two.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01116.mp4",
    "question_id": "01116_11",
    "clip_path": "clips/01116/01116__0040500_0042500.mp4"
  },
  {
    "timestamp": "00:41 - 00:42",
    "context": "The claps are described as 'two sharp, distinct clapping sounds.'",
    "question_type": "Sound Characteristics",
    "question": "How would you describe the acoustic quality of the clapping sounds?",
    "answer": "Sharp and distinct.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01116.mp4",
    "question_id": "01116_12",
    "clip_path": "clips/01116/01116__0040500_0042500.mp4"
  },
  {
    "timestamp": "00:41 - 00:42",
    "context": "While clapping, the user exclaims in a cheerful, medium-volume tone, 'Successful!' to signify the positive outcome of the test.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the user exclaim 'Successful!' while clapping?",
    "answer": "To signal a positive outcome of the test with the robot.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01116.mp4",
    "question_id": "01116_13",
    "clip_path": "clips/01116/01116__0040500_0042500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "[00:00 - 00:06] From the smartphone held by the user, a synthesized female voice says, \"...Activate the take photo button.\" [00:06 - 00:07] In direct response, the user's thumb taps the bottom of the smartphone screen.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the user tap the bottom of the smartphone screen at around 00:06?",
    "answer": "To follow the voice command to activate the take photo button.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01118.mp4",
    "question_id": "01118_1",
    "clip_path": "clips/01118/01118__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:06 - 00:07",
    "context": "[00:06] The user's thumb taps the bottom of the smartphone screen. [00:06 - 00:07] This action immediately triggers a loud, high-pitched digital camera shutter \"click\" sound.",
    "question_type": "Cross-Modal Reasoning",
    "question": "After the user's thumb taps the bottom of the screen, what sound follows?",
    "answer": "A loud, high-pitched digital camera shutter \"click\".",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01118.mp4",
    "question_id": "01118_2",
    "clip_path": "clips/01118/01118__0005500_0007500.mp4"
  },
  {
    "timestamp": "00:06 - 00:07",
    "context": "[00:06 - 00:07] The tap immediately triggers a digital camera shutter \"click\" sound.",
    "question_type": "Sound Characteristics",
    "question": "What are the volume and pitch characteristics of the shutter sound?",
    "answer": "It is loud and high-pitched.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01118.mp4",
    "question_id": "01118_3",
    "clip_path": "clips/01118/01118__0005500_0007500.mp4"
  },
  {
    "timestamp": "00:06 - 00:07",
    "context": "[00:06] Thumb taps the screen. [00:06 - 00:07] The shutter \"click\" occurs immediately.",
    "question_type": "Temporal Information",
    "question": "What is the delay between the screen tap and the shutter click?",
    "answer": "It occurs immediately with no noticeable delay.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01118.mp4",
    "question_id": "01118_4",
    "clip_path": "clips/01118/01118__0005500_0007500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "[00:00 - 00:06] From the smartphone held by the user, a clear, synthesized female voice emanates, narrating the surroundings.",
    "question_type": "Sound Source Identification",
    "question": "What device generated the synthesized female narration?",
    "answer": "The smartphone held by the user.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01118.mp4",
    "question_id": "01118_5",
    "clip_path": "clips/01118/01118__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "[00:00 - 00:06] A clear, synthesized female voice narrates the environment from the smartphone.",
    "question_type": "Sound Characteristics",
    "question": "How is the narration voice described acoustically?",
    "answer": "As a clear, synthesized female voice.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01118.mp4",
    "question_id": "01118_6",
    "clip_path": "clips/01118/01118__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "[00:00 - 00:06] The synthesized voice narrates, including the instruction \"Activate the take photo button.\"",
    "question_type": "Temporal Information",
    "question": "When does the narration occur and how long does it last?",
    "answer": "From 00:00 to 00:06, lasting about 6 seconds.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01118.mp4",
    "question_id": "01118_7",
    "clip_path": "clips/01118/01118__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:06 - 00:07",
    "context": "[00:06 - 00:07] A single loud, high-pitched digital camera shutter \"click\" is heard after the tap.",
    "question_type": "Counting",
    "question": "How many shutter click sounds are heard?",
    "answer": "One.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01118.mp4",
    "question_id": "01118_8",
    "clip_path": "clips/01118/01118__0005500_0007500.mp4"
  },
  {
    "timestamp": "00:06 - 00:07",
    "context": "[00:06 - 00:07] The shutter click is heard. Simultaneously, the phone's screen flashes and shows a newly captured photo of the white dog, confirming the action.",
    "question_type": "Cross-Modal Reasoning",
    "question": "After the shutter click, what visual change occurs and what does it confirm?",
    "answer": "The screen flashes and displays a newly captured photo of the white dog, confirming the photo was taken.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01118.mp4",
    "question_id": "01118_9",
    "clip_path": "clips/01118/01118__0005500_0007500.mp4"
  },
  {
    "timestamp": "00:06 - 00:07",
    "context": "[00:06 - 00:07] The user's screen tap immediately triggers the shutter \"click\" sound.",
    "question_type": "Sound Source Identification",
    "question": "What produced the shutter click sound?",
    "answer": "The smartphone's camera function triggered by the user's screen tap.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01118.mp4",
    "question_id": "01118_10",
    "clip_path": "clips/01118/01118__0005500_0007500.mp4"
  },
  {
    "timestamp": "00:00 - 00:25",
    "context": "From a first-person perspective, the user is sitting on a bed and speaking in a clear, moderate-volume female voice. She explains she will use the 'Be My Eyes' app so a volunteer can describe her new 1-million-subscriber plaque. The monologue serves to explain her visual impairment and her plan to experience unboxing the award with a remote volunteer.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why does the user plan to use the Be My Eyes app during the unboxing of her 1-million-subscriber plaque?",
    "answer": "Because she is visually impaired and wants a remote volunteer to describe the plaque for her in real time while she unboxes it.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01119.mp4",
    "question_id": "01119_1",
    "clip_path": "clips/01119/01119__0000000_0025101.mp4"
  },
  {
    "timestamp": "00:00 - 00:25",
    "context": "The user speaks in a clear, moderate-volume female voice while seated on a bed.",
    "question_type": "Sound Source Identification",
    "question": "Who is the source of the clear, moderate-volume voice heard in this segment?",
    "answer": "The user herself, a female speaker.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01119.mp4",
    "question_id": "01119_2",
    "clip_path": "clips/01119/01119__0000000_0025101.mp4"
  },
  {
    "timestamp": "00:00 - 00:25",
    "context": "The user is speaking in a clear, moderate-volume female voice.",
    "question_type": "Sound Characteristics",
    "question": "What are the clarity and volume characteristics of the user's speech?",
    "answer": "Her speech is clear and moderate in volume.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01119.mp4",
    "question_id": "01119_3",
    "clip_path": "clips/01119/01119__0000000_0025101.mp4"
  },
  {
    "timestamp": "00:00 - 00:25",
    "context": "The user delivers a monologue explaining the context and plan to use Be My Eyes to identify her one-million-subscriber plaque.",
    "question_type": "Temporal Information",
    "question": "When does the explanatory monologue occur within the video?",
    "answer": "During 00:00–00:25.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01119.mp4",
    "question_id": "01119_4",
    "clip_path": "clips/01119/01119__0000000_0025101.mp4"
  },
  {
    "timestamp": "00:00 - 00:25",
    "context": "She says: \"Now, I want to use Be My Eyes to identify my one-million-subscriber plaque... When I received my 100,000-subscriber plaque, I connected with a volunteer to describe it for me.\"",
    "question_type": "Counting",
    "question": "How many distinct subscriber plaques does the speaker mention in her monologue?",
    "answer": "Two: the 100,000-subscriber plaque and the 1-million-subscriber plaque.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01119.mp4",
    "question_id": "01119_5",
    "clip_path": "clips/01119/01119__0000000_0025101.mp4"
  },
  {
    "timestamp": "00:00 - 00:25",
    "context": "She notes her previous experience: \"When I received my 100,000-subscriber plaque, I connected with a volunteer to describe it for me,\" while explaining she will use Be My Eyes again.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why does the speaker mention her prior use of a volunteer for the 100,000-subscriber plaque?",
    "answer": "To provide context and precedent for using Be My Eyes again now, showing that volunteers previously described a plaque for her.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01119.mp4",
    "question_id": "01119_6",
    "clip_path": "clips/01119/01119__0000000_0025101.mp4"
  },
  {
    "timestamp": "00:00 - 00:13",
    "context": "[00:00 - 00:13] A clear, synthesized female voice, originating from the smartphone held in the user's hand, reads a detailed description of the scene.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the synthesized female voice originate from at the start of the video?",
    "answer": "From the smartphone held in the user's hand.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01122.mp4",
    "question_id": "01122_1",
    "clip_path": "clips/01122/01122__0000000_0013500.mp4"
  },
  {
    "timestamp": "00:00 - 00:13",
    "context": "[00:00 - 00:13] A clear, synthesized female voice, originating from the smartphone held in the user's hand, reads a detailed description.",
    "question_type": "Sound Characteristics",
    "question": "How is the AI voice characterized in terms of clarity and nature?",
    "answer": "It is a clear, synthesized female voice.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01122.mp4",
    "question_id": "01122_2",
    "clip_path": "clips/01122/01122__0000000_0013500.mp4"
  },
  {
    "timestamp": "00:13 - 00:15",
    "context": "[00:13 - 00:15] A short, sharp digital click sound is produced as the user's thumb taps the smartphone screen.",
    "question_type": "Sound Characteristics",
    "question": "What are the acoustic qualities of the click produced at 00:13 - 00:15?",
    "answer": "It is a short, sharp digital click.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01122.mp4",
    "question_id": "01122_3",
    "clip_path": "clips/01122/01122__0012500_0015500.mp4"
  },
  {
    "timestamp": "00:13 - 00:15",
    "context": "[00:13 - 00:15] A short, sharp digital click sound is produced as the user's thumb taps the smartphone screen.",
    "question_type": "Sound Source Identification",
    "question": "What action generated the digital click sound between 00:13 and 00:15?",
    "answer": "The user's thumb tapping the smartphone screen.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01122.mp4",
    "question_id": "01122_4",
    "clip_path": "clips/01122/01122__0012500_0015500.mp4"
  },
  {
    "timestamp": "00:13 - 00:15",
    "context": "[00:13 - 00:15] The screen tap transitions the app's interface to showing the generated text in an editable message field, indicating the user's intent to review or modify the AI's description.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the user tap the smartphone screen at 00:13 - 00:15?",
    "answer": "To switch the app to an editable view of the generated text so she could review or modify the AI’s description.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01122.mp4",
    "question_id": "01122_5",
    "clip_path": "clips/01122/01122__0012500_0015500.mp4"
  },
  {
    "timestamp": "00:00 - 00:28",
    "context": "[00:00 - 00:13] The AI voice reads its analysis. [00:15 - 00:28] The synthesized female voice continues reading its analysis.",
    "question_type": "Temporal Information",
    "question": "When does the synthesized voice speak, and what are the durations of each segment?",
    "answer": "It speaks during 00:00–00:13 and 00:15–00:28; both segments last 13 seconds each.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01122.mp4",
    "question_id": "01122_6",
    "clip_path": "clips/01122/01122__0000000_0028500.mp4"
  },
  {
    "timestamp": "00:28 - 00:46",
    "context": "[00:28 - 00:46] The user provides spoken feedback about the AI’s recognition, questioning '30 million' vs '1 million' and discussing possible causes of inaccuracy. The caption notes this reveals the purpose: to test and evaluate the accuracy of the phone's visual recognition software.",
    "question_type": "Inferential & Contextual Causality",
    "question": "What was the purpose of the user's spoken analysis between 00:28 and 00:46?",
    "answer": "To test and evaluate the accuracy of the phone’s visual recognition software on a complex real-world object.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01122.mp4",
    "question_id": "01122_7",
    "clip_path": "clips/01122/01122__0027500_0046500.mp4"
  },
  {
    "timestamp": "00:28 - 00:46",
    "context": "[00:28 - 00:46] The user begins to speak in a calm, analytical tone, giving feedback on the AI's performance.",
    "question_type": "Sound Characteristics",
    "question": "What was the tone of the user's speech during her feedback?",
    "answer": "Calm and analytical.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01122.mp4",
    "question_id": "01122_8",
    "clip_path": "clips/01122/01122__0027500_0046500.mp4"
  },
  {
    "timestamp": "00:28 - 00:46",
    "context": "[00:28 - 00:46] The user suggests several reasons for the possible misrecognition: the recognition could be wrong, the shooting angle could be wrong, or artistic fonts could be affecting recognition.",
    "question_type": "Counting",
    "question": "How many distinct possible reasons does the user give for the recognition inaccuracy?",
    "answer": "Three: incorrect recognition, an incorrect shooting angle, and the use of artistic fonts.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01122.mp4",
    "question_id": "01122_9",
    "clip_path": "clips/01122/01122__0027500_0046500.mp4"
  },
  {
    "timestamp": "00:46 - 00:49",
    "context": "[00:46 - 00:49] As she finishes her analysis, the user lowers the phone slightly, providing a clearer view of the plaque, and says, 'But I think this description is already very powerful.'",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the user lower the phone slightly at the end?",
    "answer": "To provide a clearer view of the plaque.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01122.mp4",
    "question_id": "01122_10",
    "clip_path": "clips/01122/01122__0045500_0049250.mp4"
  },
  {
    "timestamp": "00:01 - 00:12",
    "context": "[00:01] The camera holder approaches a man and a woman and asks, \"Hello, excuse me, how do I get to IKEA?\" Others then provide directions.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the camera holder ask, \"How do I get to IKEA?\"",
    "answer": "They were seeking directions to IKEA.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01124.mp4",
    "question_id": "01124_1",
    "clip_path": "clips/01124/01124__0000500_0012500.mp4"
  },
  {
    "timestamp": "00:06 - 00:10",
    "context": "The man replies, \"Yes, forward, forward, forward. Thank you.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the man repeat the word \"forward\" multiple times?",
    "answer": "To emphasize and clarify the direction to continue straight ahead.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01124.mp4",
    "question_id": "01124_2",
    "clip_path": "clips/01124/01124__0005500_0010500.mp4"
  },
  {
    "timestamp": "00:10 - 00:12",
    "context": "Another person adds, \"Go straight and turn right.\" The camera holder says, \"Okay, okay, thank you,\" concluding the interaction.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the camera holder say, \"Okay, okay, thank you\" at the end?",
    "answer": "To express gratitude after receiving the directions and to conclude the interaction.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01124.mp4",
    "question_id": "01124_3",
    "clip_path": "clips/01124/01124__0009500_0012500.mp4"
  },
  {
    "timestamp": "00:01 - 00:03",
    "context": "A clear, polite voice originates directly from the camera's position: \"Hello, excuse me, how do I get to IKEA?\"",
    "question_type": "Sound Source Identification",
    "question": "Who produced the clear, polite voice asking for directions?",
    "answer": "The camera holder.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01124.mp4",
    "question_id": "01124_4",
    "clip_path": "clips/01124/01124__0000500_0003500.mp4"
  },
  {
    "timestamp": "00:04 - 00:08",
    "context": "The man points forward and says, \"That way, go forward.\"",
    "question_type": "Sound Source Identification",
    "question": "Who said, \"That way, go forward\"?",
    "answer": "The man standing slightly in front and to the right.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01124.mp4",
    "question_id": "01124_5",
    "clip_path": "clips/01124/01124__0003500_0008500.mp4"
  },
  {
    "timestamp": "00:03 - 00:10",
    "context": "The man and woman, standing slightly in front and to the right, respond in a normal conversational tone.",
    "question_type": "Sound Characteristics",
    "question": "What was the tone of the responses from the man and woman?",
    "answer": "A normal conversational tone.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01124.mp4",
    "question_id": "01124_6",
    "clip_path": "clips/01124/01124__0002500_0010500.mp4"
  },
  {
    "timestamp": "00:01 - 00:12",
    "context": "The entire exchange is set against the faint, ambient sound of mall music.",
    "question_type": "Sound Characteristics",
    "question": "What are the volume and nature of the background mall music during the exchange?",
    "answer": "It is faint and ambient.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01124.mp4",
    "question_id": "01124_7",
    "clip_path": "clips/01124/01124__0000500_0012500.mp4"
  },
  {
    "timestamp": "00:03 - 00:08",
    "context": "The man and woman are described as standing slightly in front and to the right while responding.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where were the responding man and woman located relative to the camera?",
    "answer": "Slightly in front and to the right of the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01124.mp4",
    "question_id": "01124_8",
    "clip_path": "clips/01124/01124__0002500_0008500.mp4"
  },
  {
    "timestamp": "00:01 - 00:03",
    "context": "The asking voice \"originates directly from the camera's position.\"",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From what location relative to the camera did the initial question originate?",
    "answer": "Directly from the camera's position.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01124.mp4",
    "question_id": "01124_9",
    "clip_path": "clips/01124/01124__0000500_0003500.mp4"
  },
  {
    "timestamp": "00:01 - 00:12",
    "context": "The entire exchange is set against the faint, ambient sound of mall music.",
    "question_type": "Temporal Information",
    "question": "Was the background mall music brief or continuous during the interaction?",
    "answer": "It was continuous throughout the exchange.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01124.mp4",
    "question_id": "01124_10",
    "clip_path": "clips/01124/01124__0000500_0012500.mp4"
  },
  {
    "timestamp": "00:01 - 00:12",
    "context": "The interaction begins with the camera holder asking for directions and ends with, \"Okay, okay, thank you.\"",
    "question_type": "Temporal Information",
    "question": "When did the brief interaction take place?",
    "answer": "From 00:01 to 00:12.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01124.mp4",
    "question_id": "01124_11",
    "clip_path": "clips/01124/01124__0000500_0012500.mp4"
  },
  {
    "timestamp": "00:06 - 00:10",
    "context": "The man replies, \"Yes, forward, forward, forward.\"",
    "question_type": "Counting",
    "question": "How many times did the man repeat the word \"forward\"?",
    "answer": "Three times.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01124.mp4",
    "question_id": "01124_12",
    "clip_path": "clips/01124/01124__0005500_0010500.mp4"
  },
  {
    "timestamp": "00:02 - 00:05",
    "context": "[00:02 - 00:05] The user, navigating a busy shopping mall with a white cane, stops a middle-aged man and asks from the camera's position, \"您好，您知道宜家在哪儿吗?\" in a clear, polite voice. Ambient chatter and footsteps are audible.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the user ask the man for directions during 00:02 - 00:05?",
    "answer": "She was trying to find IKEA while navigating the busy shopping mall.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01125.mp4",
    "question_id": "01125_1",
    "clip_path": "clips/01125/01125__0001500_0005500.mp4"
  },
  {
    "timestamp": "00:05 - 00:08",
    "context": "[00:05 - 00:08] The man, standing directly in front of the user, replies in a helpful tone: \"宜家这儿呢，你现在先转过来\" and gestures with his head.",
    "question_type": "Sound Source Identification",
    "question": "Who delivered the helpful reply giving initial directions at 00:05 - 00:08?",
    "answer": "The middle-aged man standing directly in front of the user.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01125.mp4",
    "question_id": "01125_2",
    "clip_path": "clips/01125/01125__0004500_0008500.mp4"
  },
  {
    "timestamp": "00:05 - 00:08",
    "context": "[00:05 - 00:08] The man, standing directly in front of the user, replies in a helpful tone.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the man's spoken reply originate relative to the camera?",
    "answer": "Directly in front of the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01125.mp4",
    "question_id": "01125_3",
    "clip_path": "clips/01125/01125__0004500_0008500.mp4"
  },
  {
    "timestamp": "00:02 - 00:05",
    "context": "[00:02 - 00:05] The user asks, in a clear, polite voice: \"您好，您知道宜家在哪儿吗?\"",
    "question_type": "Sound Characteristics",
    "question": "What was the tone and clarity of the user's question at 00:02 - 00:05?",
    "answer": "It was clear and polite.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01125.mp4",
    "question_id": "01125_4",
    "clip_path": "clips/01125/01125__0001500_0005500.mp4"
  },
  {
    "timestamp": "00:08 - 00:16",
    "context": "[00:08 - 00:16] A female narrator's voice-over begins, explaining that many kind people helped and a couple took them directly to IKEA.",
    "question_type": "Temporal Information",
    "question": "When did the female narrator's voice-over begin, and how long does it span in the caption?",
    "answer": "It begins at 00:08 and continues through 00:16, about 8 seconds.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01125.mp4",
    "question_id": "01125_5",
    "clip_path": "clips/01125/01125__0007500_0016500.mp4"
  },
  {
    "timestamp": "00:05 - 00:08",
    "context": "[00:05 - 00:08] Visually, the man gestures with his head to indicate direction while saying, \"宜家这儿呢，你现在先转过来.\"",
    "question_type": "Cross-Modal Reasoning",
    "question": "How does the man's head gesture relate to his spoken instruction at 00:05 - 00:08?",
    "answer": "His head gesture indicates the direction as he tells her to turn around, implying she should rotate to face the indicated corridor.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01125.mp4",
    "question_id": "01125_6",
    "clip_path": "clips/01125/01125__0004500_0008500.mp4"
  },
  {
    "timestamp": "00:02 - 00:16",
    "context": "[00:02 - 00:05] The user speaks to ask for IKEA. [00:05 - 00:08] The man replies. [00:08 - 00:16] A female narrator's voice-over begins.",
    "question_type": "Counting",
    "question": "How many distinct speakers are heard across the clip?",
    "answer": "Three: the user, the man, and a female narrator.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01125.mp4",
    "question_id": "01125_7",
    "clip_path": "clips/01125/01125__0001500_0016500.mp4"
  },
  {
    "timestamp": "00:08 - 00:16",
    "context": "[00:08 - 00:16] The narrator says there were many kind people and that a couple directly took them to IKEA, adding thanks.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why does the narrator express gratitude during 00:08 - 00:16?",
    "answer": "Because many kind people helped, and a couple directly escorted them to IKEA.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01125.mp4",
    "question_id": "01125_8",
    "clip_path": "clips/01125/01125__0007500_0016500.mp4"
  },
  {
    "timestamp": "00:02 - 00:05",
    "context": "[00:02 - 00:05] The user asks for directions from the camera's position.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the user's question originate relative to the camera?",
    "answer": "From the camera's position.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01125.mp4",
    "question_id": "01125_9",
    "clip_path": "clips/01125/01125__0001500_0005500.mp4"
  },
  {
    "timestamp": "00:02 - 00:05",
    "context": "[00:02 - 00:05] Ambient chatter and footsteps of the mall are audible in the background while the user speaks.",
    "question_type": "Sound Source Identification",
    "question": "What is the source of the background chatter and footsteps heard at 00:02 - 00:05?",
    "answer": "The busy shopping mall environment.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01125.mp4",
    "question_id": "01125_10",
    "clip_path": "clips/01125/01125__0001500_0005500.mp4"
  },
  {
    "timestamp": "00:00 - 00:11",
    "context": "In a bustling mall, a clear, close-range dialogue unfolds about guiding a visually impaired person. One woman asks, \"Don't grab my cane, can I hold your arm?\" Another woman adds, \"No, she should be holding my shoulder.\" The camera wearer's companion corrects them and demonstrates the proper technique.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why does the camera wearer's companion step in to correct the others during the conversation?",
    "answer": "To clarify the correct and safe method for guiding a visually impaired person.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01127.mp4",
    "question_id": "01127_1",
    "clip_path": "clips/01127/01127__0000000_0011500.mp4"
  },
  {
    "timestamp": "00:00 - 00:11",
    "context": "One woman asks, \"Don't grab my cane, can I hold your arm?\" Another woman interjects, \"No, she should be holding my shoulder.\"",
    "question_type": "Sound Source Identification",
    "question": "Who asks, \"Don't grab my cane, can I hold your arm?\"",
    "answer": "One of the two women.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01127.mp4",
    "question_id": "01127_2",
    "clip_path": "clips/01127/01127__0000000_0011500.mp4"
  },
  {
    "timestamp": "00:00 - 00:11",
    "context": "One woman asks, \"Don't grab my cane, can I hold your arm?\" Another woman interjects, \"No, she should be holding my shoulder.\"",
    "question_type": "Sound Source Identification",
    "question": "Who says, \"No, she should be holding my shoulder\"?",
    "answer": "The other woman who interjects.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01127.mp4",
    "question_id": "01127_3",
    "clip_path": "clips/01127/01127__0000000_0011500.mp4"
  },
  {
    "timestamp": "00:00 - 00:11",
    "context": "The camera wearer's companion demonstrates the proper technique and says, \"No, no, no. Like how I'm holding her, let her hold you like that.\"",
    "question_type": "Sound Source Identification",
    "question": "Who delivers the corrective explanation beginning with \"No, no, no\"?",
    "answer": "The camera wearer's companion.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01127.mp4",
    "question_id": "01127_4",
    "clip_path": "clips/01127/01127__0000000_0011500.mp4"
  },
  {
    "timestamp": "00:00 - 00:11",
    "context": "The audio captures a clear, close-range dialogue in the bustling shopping mall.",
    "question_type": "Sound Characteristics",
    "question": "How is the dialogue's audio quality described?",
    "answer": "Clear and close-range.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01127.mp4",
    "question_id": "01127_5",
    "clip_path": "clips/01127/01127__0000000_0011500.mp4"
  },
  {
    "timestamp": "00:00 - 00:11",
    "context": "The audio captures a clear, close-range dialogue.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Is the conversation recorded near the camera or from a distance?",
    "answer": "Near the camera (close-range).",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01127.mp4",
    "question_id": "01127_6",
    "clip_path": "clips/01127/01127__0000000_0011500.mp4"
  },
  {
    "timestamp": "00:00 - 00:11",
    "context": "The entire exchange occurs within the 11-second clip.",
    "question_type": "Temporal Information",
    "question": "When does the conversation occur and what is its approximate duration?",
    "answer": "From 00:00 to 00:11, lasting about 11 seconds.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01127.mp4",
    "question_id": "01127_7",
    "clip_path": "clips/01127/01127__0000000_0011500.mp4"
  },
  {
    "timestamp": "00:00 - 00:11",
    "context": "Two women offer different suggestions: one about holding an arm and another about holding a shoulder, before the companion corrects them.",
    "question_type": "Counting",
    "question": "How many different women propose guidance suggestions before the companion intervenes?",
    "answer": "Two women.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01127.mp4",
    "question_id": "01127_8",
    "clip_path": "clips/01127/01127__0000000_0011500.mp4"
  },
  {
    "timestamp": "00:00 - 00:11",
    "context": "The companion says, \"No, no, no. Like how I'm holding her, let her hold you like that.\"",
    "question_type": "Counting",
    "question": "How many times does the companion repeat the word \"No\" at the start of her correction?",
    "answer": "Three times.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01127.mp4",
    "question_id": "01127_9",
    "clip_path": "clips/01127/01127__0000000_0011500.mp4"
  },
  {
    "timestamp": "00:00 - 00:11",
    "context": "Correcting them, the camera wearer's companion demonstrates the proper technique by holding the woman's arm while explaining, \"Like how I'm holding her, let her hold you like that.\"",
    "question_type": "Cross-Modal Reasoning",
    "question": "What visual action accompanies the companion's verbal correction to demonstrate the proper guiding technique?",
    "answer": "She demonstrates by holding the woman's arm to show how the visually impaired person should hold.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01127.mp4",
    "question_id": "01127_10",
    "clip_path": "clips/01127/01127__0000000_0011500.mp4"
  },
  {
    "timestamp": "00:00 - 00:08",
    "context": "The camera holder walks through a crowded IKEA aisle, creating the sound of their own footsteps on the hard floor.",
    "question_type": "Sound Source Identification",
    "question": "What generated the footsteps heard in this segment?",
    "answer": "The camera holder’s own feet on the hard floor.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01129.mp4",
    "question_id": "01129_1",
    "clip_path": "clips/01129/01129__0000000_0008167.mp4"
  },
  {
    "timestamp": "00:00 - 00:08",
    "context": "The surrounding environment is filled with the continuous, low-volume, and diffuse chatter of numerous other shoppers.",
    "question_type": "Sound Characteristics",
    "question": "What are the volume and texture characteristics of the surrounding chatter?",
    "answer": "It is continuous, low-volume, and diffuse.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01129.mp4",
    "question_id": "01129_2",
    "clip_path": "clips/01129/01129__0000000_0008167.mp4"
  },
  {
    "timestamp": "00:00 - 00:08",
    "context": "A female voice, very close to the camera, speaks casually: '我们先瞎走吧哈哈' ('Let's just walk around randomly, haha').",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the female voice originate relative to the camera?",
    "answer": "Very close to the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01129.mp4",
    "question_id": "01129_3",
    "clip_path": "clips/01129/01129__0000000_0008167.mp4"
  },
  {
    "timestamp": "00:00 - 00:08",
    "context": "The surrounding environment is filled with the continuous, low-volume, and diffuse chatter of numerous other shoppers.",
    "question_type": "Temporal Information",
    "question": "Is the background chatter brief or continuous during this interval?",
    "answer": "It is continuous throughout 00:00–00:08.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01129.mp4",
    "question_id": "01129_4",
    "clip_path": "clips/01129/01129__0000000_0008167.mp4"
  },
  {
    "timestamp": "00:00 - 00:08",
    "context": "Female voice: '我们先瞎走吧哈哈' ('Let's just walk around randomly, haha'). Narrator voiceover: '我们一开始其实就是想随便逛逛，也没什么特定的目标，所以就决定自己探索' (They wanted to wander casually without a specific goal and explore on their own).",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the female suggest 'Let's just walk around randomly'?",
    "answer": "Because they had no specific destination or goal and intended to explore on their own.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01129.mp4",
    "question_id": "01129_5",
    "clip_path": "clips/01129/01129__0000000_0008167.mp4"
  },
  {
    "timestamp": "00:00 - 00:08",
    "context": "Immediately following the female’s line, a narrator’s voiceover explains their intention for the visit, contextualizing the scene.",
    "question_type": "Inferential & Contextual Causality",
    "question": "What was the purpose of the narrator’s voiceover explanation?",
    "answer": "To explain their intention for the visit and contextualize the scene.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01129.mp4",
    "question_id": "01129_6",
    "clip_path": "clips/01129/01129__0000000_0008167.mp4"
  },
  {
    "timestamp": "00:00 - 00:08",
    "context": "A female voice speaks near the camera; immediately following, a narrator’s voiceover explains their intention.",
    "question_type": "Temporal Information",
    "question": "When did the narrator’s voiceover occur relative to the female’s speech?",
    "answer": "Immediately after the female’s speech.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01129.mp4",
    "question_id": "01129_7",
    "clip_path": "clips/01129/01129__0000000_0008167.mp4"
  },
  {
    "timestamp": "00:00 - 00:08",
    "context": "A female voice near the camera speaks a line; immediately after, a narrator’s voiceover explains their intention for the visit.",
    "question_type": "Counting",
    "question": "How many distinct speaking voices deliver clear statements in this segment?",
    "answer": "Two: a female voice near the camera and a narrator’s voiceover.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01129.mp4",
    "question_id": "01129_8",
    "clip_path": "clips/01129/01129__0000000_0008167.mp4"
  },
  {
    "timestamp": "00:00 - 00:08",
    "context": "The surrounding environment is filled with chatter of numerous other shoppers.",
    "question_type": "Sound Source Identification",
    "question": "Who generated the background chatter?",
    "answer": "Numerous other shoppers in the store.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01129.mp4",
    "question_id": "01129_9",
    "clip_path": "clips/01129/01129__0000000_0008167.mp4"
  },
  {
    "timestamp": "00:01 - 00:05",
    "context": "The user's hand rummages through a cardboard bin of lint rollers, producing soft, continuous rustling/rattling. A female voice from the user's right identifies the product.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From which direction did the identifying female voice originate?",
    "answer": "From the user's right side.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01130.mp4",
    "question_id": "01130_1",
    "clip_path": "clips/01130/01130__0000500_0005500.mp4"
  },
  {
    "timestamp": "00:01 - 00:05",
    "context": "The user's hand touches and rummages through a large cardboard bin filled with lint rollers, producing soft, continuous rustling and rattling from plastic items shifting against each other.",
    "question_type": "Sound Source Identification",
    "question": "What generated the soft, continuous rustling and rattling at the start?",
    "answer": "Plastic lint rollers shifting against each other as the user's hand rummaged through the bin.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01130.mp4",
    "question_id": "01130_2",
    "clip_path": "clips/01130/01130__0000500_0005500.mp4"
  },
  {
    "timestamp": "00:01 - 00:05",
    "context": "Rummaging in the bin produces soft, continuous rustling and rattling.",
    "question_type": "Sound Characteristics",
    "question": "How are the rustling and rattling sounds described in terms of volume and texture?",
    "answer": "They are soft and continuous, with a rustling/rattling texture.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01130.mp4",
    "question_id": "01130_3",
    "clip_path": "clips/01130/01130__0000500_0005500.mp4"
  },
  {
    "timestamp": "00:01 - 00:05",
    "context": "While the user rummages in the lint roller bin, a female voice from the user's right says, “这是那个粘毛的那个滚子.”",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the female voice identify the product at this moment?",
    "answer": "Because the user was handling items in the bin, she clarified that the items were lint rollers.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01130.mp4",
    "question_id": "01130_4",
    "clip_path": "clips/01130/01130__0000500_0005500.mp4"
  },
  {
    "timestamp": "00:05 - 00:13",
    "context": "Another person in a white jacket picks up and examines a lint roller. The user's hand continues interacting with items, creating more soft plastic rattling. The user then decisively picks up one lint roller, creating a short, distinct plastic clatter.",
    "question_type": "Temporal Information",
    "question": "When did the distinct clatter occur in this interval, and was it brief or extended?",
    "answer": "It occurred when the user decisively picked up one lint roller, and it was brief.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01130.mp4",
    "question_id": "01130_5",
    "clip_path": "clips/01130/01130__0004500_0013500.mp4"
  },
  {
    "timestamp": "00:05 - 00:13",
    "context": "The user decisively picks up one lint roller from the front of the bin, creating a short, distinct plastic clatter.",
    "question_type": "Sound Source Identification",
    "question": "What caused the short, distinct plastic clatter?",
    "answer": "A lint roller being lifted from the pile in the bin.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01130.mp4",
    "question_id": "01130_6",
    "clip_path": "clips/01130/01130__0004500_0013500.mp4"
  },
  {
    "timestamp": "00:05 - 00:13",
    "context": "Picking up one lint roller from the pile creates a short, distinct plastic clatter.",
    "question_type": "Sound Characteristics",
    "question": "How is the clatter described when the lint roller is lifted?",
    "answer": "As a short, distinct plastic clatter.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01130.mp4",
    "question_id": "01130_7",
    "clip_path": "clips/01130/01130__0004500_0013500.mp4"
  },
  {
    "timestamp": "00:01 - 00:13",
    "context": "Spoken lines include: “这是那个粘毛的那个滚子,” “这附近应该都是这一类的东西我感觉嗯嗯,” and “可能都是小东西.”",
    "question_type": "Counting",
    "question": "How many quoted speech lines are present in this span?",
    "answer": "Three.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01130.mp4",
    "question_id": "01130_8",
    "clip_path": "clips/01130/01130__0000500_0013500.mp4"
  },
  {
    "timestamp": "00:05 - 00:13",
    "context": "The two have a conversation about the products in the area.",
    "question_type": "Counting",
    "question": "How many people are engaged in the conversation during this interval?",
    "answer": "Two people.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01130.mp4",
    "question_id": "01130_9",
    "clip_path": "clips/01130/01130__0004500_0013500.mp4"
  },
  {
    "timestamp": "00:05 - 00:13",
    "context": "While another person in a white jacket picks up and examines a lint roller, the user's hand continues interacting with items in the bin, creating more soft plastic rattling.",
    "question_type": "Cross-Modal Reasoning",
    "question": "During the white-jacketed person's examination of a lint roller, what audio cue is heard and what does it indicate about ongoing actions?",
    "answer": "Soft plastic rattling is heard, indicating the user continues interacting with items in the bin.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01130.mp4",
    "question_id": "01130_10",
    "clip_path": "clips/01130/01130__0004500_0013500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "[00:00 - 00:03] While walking under a bus stop shelter, a series of five quiet, high-pitched, metallic clicks are heard from an object being held just below the camera's view. Subsequently, a loud, sharp clattering occurs as a red-and-silver stick-like object is dropped onto the brick pavement directly in front.",
    "question_type": "Counting",
    "question": "How many metallic click sounds were heard before the clattering?",
    "answer": "Five.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01132.mp4",
    "question_id": "01132_1",
    "clip_path": "clips/01132/01132__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "[00:00 - 00:03] A series of five quiet, high-pitched, metallic clicks are heard from an object held just below the camera's view.",
    "question_type": "Sound Characteristics",
    "question": "What were the acoustic qualities (volume and texture) of the series of clicks?",
    "answer": "They were quiet, high-pitched, and metallic.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01132.mp4",
    "question_id": "01132_2",
    "clip_path": "clips/01132/01132__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "[00:00 - 00:03] The clicks originate from an object being held just below the camera's view.",
    "question_type": "Sound Source Identification",
    "question": "What produced the series of five clicks?",
    "answer": "An object being held just below the camera's view.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01132.mp4",
    "question_id": "01132_3",
    "clip_path": "clips/01132/01132__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "[00:00 - 00:03] A loud, sharp clattering sound occurs as a red-and-silver stick-like object is accidentally dropped onto the brick pavement directly in front.",
    "question_type": "Sound Source Identification",
    "question": "What generated the loud, sharp clattering sound?",
    "answer": "The red-and-silver stick-like object hitting the brick pavement after being dropped.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01132.mp4",
    "question_id": "01132_4",
    "clip_path": "clips/01132/01132__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "[00:00 - 00:03] The object is dropped onto the brick pavement directly in front of the person, producing a loud clatter.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the clattering sound originate relative to the camera?",
    "answer": "From the brick pavement directly in front of the person/camera at ground level.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01132.mp4",
    "question_id": "01132_5",
    "clip_path": "clips/01132/01132__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "[00:00 - 00:03] The clicks are heard from an object being held just below the camera's view.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where were the metallic clicks coming from relative to the camera?",
    "answer": "From just below the camera's view (immediately below the camera).",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01132.mp4",
    "question_id": "01132_6",
    "clip_path": "clips/01132/01132__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "[00:00 - 00:03] Five metallic clicks are heard, followed subsequently by a loud clattering as the object is dropped.",
    "question_type": "Temporal Information",
    "question": "What was the sequence of sounds between 00:00 and 00:03?",
    "answer": "First a series of five clicks, followed by a loud, sharp clattering sound.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01132.mp4",
    "question_id": "01132_7",
    "clip_path": "clips/01132/01132__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:03 - 00:06",
    "context": "[00:00 - 00:03] The object is dropped, producing a loud clatter. [00:03 - 00:06] In direct response, the person exclaims, '哎妈呀' ('Oh my goodness!').",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the person exclaim '哎妈呀'?",
    "answer": "Because they had just dropped the object and reacted in surprise.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01132.mp4",
    "question_id": "01132_8",
    "clip_path": "clips/01132/01132__0002500_0006500.mp4"
  },
  {
    "timestamp": "00:03 - 00:06",
    "context": "[00:03 - 00:06] After the object is dropped and clatters, the person exclaims '哎妈呀'.",
    "question_type": "Temporal Information",
    "question": "When did the exclamation occur relative to the dropping sound?",
    "answer": "Immediately after the clattering, between 00:03 and 00:06.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01132.mp4",
    "question_id": "01132_9",
    "clip_path": "clips/01132/01132__0002500_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "A continuous, non-diegetic female narration in Mandarin plays clearly. A low, continuous, diffuse murmur of background chatter from numerous shoppers in a crowded IKEA furniture and home decor section creates a busy ambient soundscape.",
    "question_type": "Inferential & Contextual Causality",
    "question": "What is the likely reason for the low, diffuse murmur of background chatter heard during this segment?",
    "answer": "The IKEA store is crowded with numerous shoppers scattered throughout the furniture and home decor section who are conversing.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01140.mp4",
    "question_id": "01140_1",
    "clip_path": "clips/01140/01140__0000000_0005800.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "A continuous, non-diegetic female narration in Mandarin plays clearly.",
    "question_type": "Sound Source Identification",
    "question": "What is the source of the continuous female voice heard during this segment?",
    "answer": "A non-diegetic female narration in Mandarin.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01140.mp4",
    "question_id": "01140_2",
    "clip_path": "clips/01140/01140__0000000_0005800.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "A low, continuous, diffuse murmur of background chatter from numerous shoppers creates a busy ambient soundscape.",
    "question_type": "Sound Characteristics",
    "question": "How is the background chatter described in terms of volume and texture?",
    "answer": "It is a low, continuous, diffuse murmur.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01140.mp4",
    "question_id": "01140_3",
    "clip_path": "clips/01140/01140__0000000_0005800.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "Background chatter comes from numerous shoppers scattered throughout the furniture and home decor section, producing a diffuse ambient soundscape.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where does the background chatter appear to originate relative to the camera?",
    "answer": "It is diffuse and surrounding, not from a single direction, coming from shoppers scattered throughout the area.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01140.mp4",
    "question_id": "01140_4",
    "clip_path": "clips/01140/01140__0000000_0005800.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "A continuous, non-diegetic female narration in Mandarin plays clearly over the scene.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Does the female narration originate from a specific direction relative to the camera?",
    "answer": "No. It is non-diegetic and not tied to any physical location in the scene.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01140.mp4",
    "question_id": "01140_5",
    "clip_path": "clips/01140/01140__0000000_0005800.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "A continuous, non-diegetic female narration in Mandarin plays clearly throughout the segment.",
    "question_type": "Temporal Information",
    "question": "Is the female narration brief or continuous during 00:00–00:06?",
    "answer": "It is continuous across the entire segment.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01140.mp4",
    "question_id": "01140_6",
    "clip_path": "clips/01140/01140__0000000_0005800.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "A low, continuous, diffuse murmur of background chatter from numerous shoppers persists throughout the segment.",
    "question_type": "Temporal Information",
    "question": "Is the background chatter intermittent or continuous during 00:00–00:06?",
    "answer": "Continuous.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01140.mp4",
    "question_id": "01140_7",
    "clip_path": "clips/01140/01140__0000000_0005800.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "Both a clear, non-diegetic female narration and a diffuse murmur of shopper chatter are audible throughout.",
    "question_type": "Counting",
    "question": "How many distinct speech streams are simultaneously audible during 00:00–00:06?",
    "answer": "Two: the non-diegetic female narration and the diffuse background chatter from numerous shoppers.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01140.mp4",
    "question_id": "01140_8",
    "clip_path": "clips/01140/01140__0000000_0005800.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "[00:00 - 00:04] As the person walks toward a subway station entrance, a continuous, medium-volume mechanical whirring is audible directly ahead. The person says, \"Hearing the sound of this escalator, I know the subway station is here.\"",
    "question_type": "Sound Source Identification",
    "question": "What specific object generated the continuous mechanical whirring heard in this segment?",
    "answer": "The escalator.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01142.mp4",
    "question_id": "01142_1",
    "clip_path": "clips/01142/01142__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "[00:00 - 00:04] A continuous, medium-volume mechanical whirring is heard as the person approaches the entrance.",
    "question_type": "Sound Characteristics",
    "question": "How is the escalator's sound described in terms of quality and volume?",
    "answer": "It is a continuous, medium-volume mechanical whirring.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01142.mp4",
    "question_id": "01142_2",
    "clip_path": "clips/01142/01142__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "[00:00 - 00:04] The escalator's whirring is audible directly ahead as the person walks forward.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From what direction relative to the camera does the escalator sound originate?",
    "answer": "Directly ahead.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01142.mp4",
    "question_id": "01142_3",
    "clip_path": "clips/01142/01142__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "[00:00 - 00:04] The escalator's whirring is present throughout and grows slightly louder as the person approaches.",
    "question_type": "Temporal Information",
    "question": "Over this interval, is the escalator sound continuous or intermittent, and how does its volume change?",
    "answer": "It is continuous and grows slightly louder as the person approaches.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01142.mp4",
    "question_id": "01142_4",
    "clip_path": "clips/01142/01142__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "[00:00 - 00:04] The person moves toward the entrance while the escalator's sound becomes slightly louder.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why does the escalator sound grow slightly louder as the clip progresses?",
    "answer": "Because the person is walking closer to the escalator, reducing the distance to the sound source.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01142.mp4",
    "question_id": "01142_5",
    "clip_path": "clips/01142/01142__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "[00:00 - 00:04] The person explains their navigational method and says, \"Hearing the sound of this escalator, I know the subway station is here.\"",
    "question_type": "Cross-Modal Reasoning",
    "question": "When the person explains their navigational method, which environmental audio cue are they referring to as a guide?",
    "answer": "The whirring sound of the escalator.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01142.mp4",
    "question_id": "01142_6",
    "clip_path": "clips/01142/01142__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "[00:00 - 00:04] A single mechanical whirring source is described: the escalator ahead.",
    "question_type": "Counting",
    "question": "How many distinct mechanical whirring sound sources are mentioned in this segment?",
    "answer": "One—the escalator.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01142.mp4",
    "question_id": "01142_7",
    "clip_path": "clips/01142/01142__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "[00:00 - 00:04] The person, who is using a white cane, speaks about using the escalator sound to navigate.",
    "question_type": "Cross-Modal Reasoning",
    "question": "While explaining reliance on the escalator's sound, what mobility aid is the person seen using?",
    "answer": "A white cane.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01142.mp4",
    "question_id": "01142_8",
    "clip_path": "clips/01142/01142__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "In a bustling furniture store, the camera operator realizes they and their companion have mistakenly taken another person's shopping cart. Reacting to the awkward situation, the camera operator says in a clear voice close to the microphone, \"哦人家的购物车天哪好尴尬\".",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why does the camera operator say \"哦人家的购物车天哪好尴尬\" during this moment?",
    "answer": "Because they realized they and their companion had mistakenly taken someone else's shopping cart, which was embarrassing.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01145.mp4",
    "question_id": "01145_1",
    "clip_path": "clips/01145/01145__0000000_0004233.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "A faint, high-pitched metallic squeak from the cart's wheels is audible as they handle it.",
    "question_type": "Sound Source Identification",
    "question": "What object generated the faint, high-pitched metallic squeak heard as they handle it?",
    "answer": "The shopping cart's wheels.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01145.mp4",
    "question_id": "01145_2",
    "clip_path": "clips/01145/01145__0000000_0004233.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "A faint, high-pitched metallic squeak from the cart's wheels is audible as they handle it.",
    "question_type": "Sound Characteristics",
    "question": "What are the volume and timbre of the squeak from the cart?",
    "answer": "It is a faint, high-pitched, metallic squeak.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01145.mp4",
    "question_id": "01145_3",
    "clip_path": "clips/01145/01145__0000000_0004233.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "The camera operator says in a clear voice close to the microphone, \"哦人家的购物车天哪好尴尬\".",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where does the spoken line originate relative to the recording device?",
    "answer": "It is spoken close to the microphone.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01145.mp4",
    "question_id": "01145_4",
    "clip_path": "clips/01145/01145__0000000_0004233.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "A faint, high-pitched metallic squeak from the cart's wheels is audible as they handle it.",
    "question_type": "Temporal Information",
    "question": "When is the squeak audible relative to the action?",
    "answer": "During 00:00–00:04, as they handle the shopping cart.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01145.mp4",
    "question_id": "01145_5",
    "clip_path": "clips/01145/01145__0000000_0004233.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "A faint, high-pitched metallic squeak is heard, and the camera operator speaks a clear line close to the microphone.",
    "question_type": "Counting",
    "question": "How many distinct sound events are explicitly described in this interval?",
    "answer": "Two: the cart-wheel squeak and the camera operator’s spoken remark.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01145.mp4",
    "question_id": "01145_6",
    "clip_path": "clips/01145/01145__0000000_0004233.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "A faint, high-pitched metallic squeak from the cart's wheels is audible as they handle it.",
    "question_type": "Cross-Modal Reasoning",
    "question": "Based on the squeak heard, what object is likely being moved or handled at that moment?",
    "answer": "A shopping cart, indicated by the squeak coming from its wheels as it's handled.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01145.mp4",
    "question_id": "01145_7",
    "clip_path": "clips/01145/01145__0000000_0004233.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "[00:00 - 00:06] In a busy IKEA, ambient echoing sounds of distant conversations, the low rumble of shopping carts on concrete, and general crowd noise are heard. A nearby female voice says: \"IKEA's new product, Mango Pomelo Sago. Twelve point five yuan, ah. This is at the counter there. Let's go ask her.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why do they move toward the service or checkout area?",
    "answer": "Because the speaker intends to ask staff at the counter about IKEA's new Mango Pomelo Sago, priced at 12.5 yuan.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01146.mp4",
    "question_id": "01146_1",
    "clip_path": "clips/01146/01146__0000000_0006300.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "[00:00 - 00:06] The store is filled with ambient sounds including a low rumble alongside crowd noise.",
    "question_type": "Sound Source Identification",
    "question": "What generated the low rumble heard in the store?",
    "answer": "Shopping carts rolling on the concrete floor.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01146.mp4",
    "question_id": "01146_2",
    "clip_path": "clips/01146/01146__0000000_0006300.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "[00:00 - 00:06] A \"low rumble\" accompanies the ambient soundscape.",
    "question_type": "Sound Characteristics",
    "question": "How is the sound from the shopping carts described acoustically?",
    "answer": "As a low rumble.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01146.mp4",
    "question_id": "01146_3",
    "clip_path": "clips/01146/01146__0000000_0006300.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "[00:00 - 00:06] The space is described as having an echoing soundscape with distant conversations and general crowd noise.",
    "question_type": "Sound Characteristics",
    "question": "What is the overall acoustic character of the ambient store sound?",
    "answer": "Echoing, with distant conversations and general crowd noise.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01146.mp4",
    "question_id": "01146_4",
    "clip_path": "clips/01146/01146__0000000_0006300.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "[00:00 - 00:06] A clear female voice speaks very close to the microphone.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where relative to the microphone does the speaking female voice originate?",
    "answer": "Very close to the microphone (immediate vicinity).",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01146.mp4",
    "question_id": "01146_5",
    "clip_path": "clips/01146/01146__0000000_0006300.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "[00:00 - 00:06] Ambient crowd noise and distant conversations are present throughout the opening segment.",
    "question_type": "Temporal Information",
    "question": "Is the ambient crowd and conversation noise brief or continuous during this segment?",
    "answer": "Continuous throughout 00:00–00:06.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01146.mp4",
    "question_id": "01146_6",
    "clip_path": "clips/01146/01146__0000000_0006300.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "[00:00 - 00:06] The nearby female voice states the objective about Mango Pomelo Sago and going to the counter.",
    "question_type": "Temporal Information",
    "question": "When is the female speaker’s objective stated?",
    "answer": "Within 00:00–00:06 as they walk forward.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01146.mp4",
    "question_id": "01146_7",
    "clip_path": "clips/01146/01146__0000000_0006300.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "[00:00 - 00:06] The caption lists multiple ambient sounds: distant conversations, low rumble of shopping carts, and general crowd noise.",
    "question_type": "Counting",
    "question": "How many distinct types of ambient sounds are described at the start?",
    "answer": "Three: distant conversations, the low rumble of shopping carts, and general crowd noise.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01146.mp4",
    "question_id": "01146_8",
    "clip_path": "clips/01146/01146__0000000_0006300.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "[00:00 - 00:06] A clear female voice is very close to the microphone; other conversations are distant.",
    "question_type": "Counting",
    "question": "How many clearly audible close-range speakers are heard near the microphone?",
    "answer": "One, a clear female voice.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01146.mp4",
    "question_id": "01146_9",
    "clip_path": "clips/01146/01146__0000000_0006300.mp4"
  },
  {
    "timestamp": "00:00 - 00:02",
    "context": "A series of six loud, sharp metallic clicks are heard from the front, originating from the escalator mechanism as people get on it.",
    "question_type": "Counting",
    "question": "How many metallic clicks were heard at the start?",
    "answer": "Six.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01149.mp4",
    "question_id": "01149_1",
    "clip_path": "clips/01149/01149__0000000_0002500.mp4"
  },
  {
    "timestamp": "00:00 - 00:02",
    "context": "A series of six loud, sharp metallic clicks are heard from the front, originating from the escalator mechanism as people get on it.",
    "question_type": "Sound Source Identification",
    "question": "What generated the metallic clicking sounds?",
    "answer": "The escalator mechanism as people got on it.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01149.mp4",
    "question_id": "01149_2",
    "clip_path": "clips/01149/01149__0000000_0002500.mp4"
  },
  {
    "timestamp": "00:00 - 00:02",
    "context": "A series of six loud, sharp metallic clicks are heard from the front.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the metallic clicks originate relative to the camera?",
    "answer": "From the front.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01149.mp4",
    "question_id": "01149_3",
    "clip_path": "clips/01149/01149__0000000_0002500.mp4"
  },
  {
    "timestamp": "00:00 - 00:02",
    "context": "Six loud, sharp metallic clicks are heard from the escalator mechanism.",
    "question_type": "Sound Characteristics",
    "question": "What were the acoustic qualities of the metallic clicks?",
    "answer": "They were loud, sharp, and metallic.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01149.mp4",
    "question_id": "01149_4",
    "clip_path": "clips/01149/01149__0000000_0002500.mp4"
  },
  {
    "timestamp": "00:02 - 00:08",
    "context": "While descending the escalator, there is a continuous, low-pitched mechanical hum.",
    "question_type": "Temporal Information",
    "question": "Was the escalator's mechanical hum continuous or intermittent during 00:02–00:08?",
    "answer": "Continuous.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01149.mp4",
    "question_id": "01149_5",
    "clip_path": "clips/01149/01149__0001500_0008500.mp4"
  },
  {
    "timestamp": "00:02 - 00:08",
    "context": "The user says: \"这儿有个电梯我听着小心点好的\" to a companion while descending, indicating guidance and safety.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the user tell the companion to be careful while on the escalator?",
    "answer": "To provide guidance and ensure safety while descending the escalator, navigating primarily by sound.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01149.mp4",
    "question_id": "01149_6",
    "clip_path": "clips/01149/01149__0001500_0008500.mp4"
  },
  {
    "timestamp": "00:02 - 00:08",
    "context": "A companion responds softly with \"啊...啊好的\".",
    "question_type": "Sound Characteristics",
    "question": "What was the volume quality of the companion's reply?",
    "answer": "Soft.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01149.mp4",
    "question_id": "01149_7",
    "clip_path": "clips/01149/01149__0001500_0008500.mp4"
  },
  {
    "timestamp": "00:08 - 00:17",
    "context": "The user asks a shopper for directions to a restaurant. A female voice replies: \"往后...往后\" (\"Backwards... backwards\").",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the female voice say \"Backwards... backwards\"?",
    "answer": "She was responding to the user's request for directions to the restaurant.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01149.mp4",
    "question_id": "01149_8",
    "clip_path": "clips/01149/01149__0007500_0017500.mp4"
  },
  {
    "timestamp": "00:08 - 00:17",
    "context": "A female voice from the side replies with directions.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From where relative to the camera did the female voice give directions?",
    "answer": "From the side.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01149.mp4",
    "question_id": "01149_9",
    "clip_path": "clips/01149/01149__0007500_0017500.mp4"
  },
  {
    "timestamp": "00:18 - 00:21",
    "context": "After stepping off the escalator, the user says: \"这边是吧啊好谢谢\" (\"This way, right? Ah, okay, thank you.\").",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the user say \"This way, right? Ah, okay, thank you\" after stepping off the escalator?",
    "answer": "To confirm the directions received and express gratitude.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01149.mp4",
    "question_id": "01149_10",
    "clip_path": "clips/01149/01149__0017500_0021500.mp4"
  },
  {
    "timestamp": "00:21 - 00:24",
    "context": "Distinct, sharp, rhythmic tapping of a white cane hitting the hard floor is heard three times.",
    "question_type": "Counting",
    "question": "How many times was the cane heard tapping the floor?",
    "answer": "Three times.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01149.mp4",
    "question_id": "01149_11",
    "clip_path": "clips/01149/01149__0020500_0024200.mp4"
  },
  {
    "timestamp": "00:21 - 00:24",
    "context": "Distinct, sharp, rhythmic tapping of a white cane hitting the hard floor is heard.",
    "question_type": "Sound Source Identification",
    "question": "What produced the tapping sounds between 00:21 and 00:24?",
    "answer": "A white cane hitting the hard floor.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01149.mp4",
    "question_id": "01149_12",
    "clip_path": "clips/01149/01149__0020500_0024200.mp4"
  },
  {
    "timestamp": "00:21 - 00:24",
    "context": "The tapping is described as distinct, sharp, and rhythmic.",
    "question_type": "Sound Characteristics",
    "question": "How are the cane tapping sounds characterized?",
    "answer": "Distinct, sharp, and rhythmic.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01149.mp4",
    "question_id": "01149_13",
    "clip_path": "clips/01149/01149__0020500_0024200.mp4"
  },
  {
    "timestamp": "00:21 - 00:24",
    "context": "The cane tapping confirms the user is visually impaired and is using the cane for navigation and obstacle detection in an unfamiliar environment.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why was the cane tapping occurring as the user began walking into the store?",
    "answer": "Because the user is visually impaired and was using the cane to navigate and detect obstacles in the unfamiliar store environment.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01149.mp4",
    "question_id": "01149_14",
    "clip_path": "clips/01149/01149__0020500_0024200.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "The user steps onto a downward-moving escalator into a subway station and says, \"看一下是不是这边下去对\" (Let's see if this is the right way down). The camera later reveals white-tiled walls and a sign for the \"Beijing MTR\".",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the user say \"看一下是不是这边下去对\" while descending?",
    "answer": "To confirm they were heading down the correct way toward the subway/Beijing MTR.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01153.mp4",
    "question_id": "01153_1",
    "clip_path": "clips/01153/01153__0000000_0006433.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "The user steps onto a downward-moving escalator, which produces a continuous, low-volume mechanical whirring and rhythmic clatter.",
    "question_type": "Sound Source Identification",
    "question": "What produced the continuous whirring and rhythmic clatter heard during this segment?",
    "answer": "The downward-moving escalator.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01153.mp4",
    "question_id": "01153_2",
    "clip_path": "clips/01153/01153__0000000_0006433.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "The escalator produces a continuous, low-volume mechanical whirring and rhythmic clatter.",
    "question_type": "Sound Characteristics",
    "question": "What are the volume and acoustic qualities of the escalator's sound?",
    "answer": "It is a continuous, low-volume mechanical whirring with a rhythmic clatter.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01153.mp4",
    "question_id": "01153_3",
    "clip_path": "clips/01153/01153__0000000_0006433.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "The user is standing on the escalator as it descends, and the escalator produces mechanical sounds.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where do the escalator sounds originate relative to the camera?",
    "answer": "Immediately around and beneath the camera, from the escalator the user is standing on.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01153.mp4",
    "question_id": "01153_4",
    "clip_path": "clips/01153/01153__0000000_0006433.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "A continuous, low-volume mechanical whirring and rhythmic clatter from the escalator is audible throughout.",
    "question_type": "Temporal Information",
    "question": "Is the escalator noise brief or continuous during this interval, and how long does it last?",
    "answer": "It is continuous, lasting the entire 6-second clip.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01153.mp4",
    "question_id": "01153_5",
    "clip_path": "clips/01153/01153__0000000_0006433.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "The caption describes 'mechanical whirring' and 'rhythmic clatter' from the escalator.",
    "question_type": "Counting",
    "question": "How many distinct non-speech sound components from the escalator are described?",
    "answer": "Two: a mechanical whirring and a rhythmic clatter.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01153.mp4",
    "question_id": "01153_6",
    "clip_path": "clips/01153/01153__0000000_0006433.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "As the escalator's mechanical sounds play and the camera descends, the scene reveals white-tiled walls and a sign for the 'Beijing MTR'.",
    "question_type": "Cross-Modal Reasoning",
    "question": "Using the escalator sounds and downward motion as audio cues, what environment does the camera reveal that confirms the setting?",
    "answer": "A subway station, indicated by white-tiled walls and a sign for the 'Beijing MTR'.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01153.mp4",
    "question_id": "01153_7",
    "clip_path": "clips/01153/01153__0000000_0006433.mp4"
  },
  {
    "timestamp": "00:00 - 00:14",
    "context": "In a busy store with a low hum of distant conversations, the user asks a man in a black puffer jacket for directions to a restaurant. The interaction concludes with the user thanking the man and turning to follow the directions.",
    "question_type": "Inferential & Contextual Causality",
    "question": "What was the reason the user initiated the conversation at the start of the clip?",
    "answer": "The user needed navigational help to find the restaurant within the store.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01155.mp4",
    "question_id": "01155_1",
    "clip_path": "clips/01155/01155__0000000_0014500.mp4"
  },
  {
    "timestamp": "00:06 - 00:12",
    "context": "Man: \"一直走,不拐弯,小心前面有个柱子.\" (Go straight, don't turn. Be careful of the pillar in front.)",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the man tell the user to be careful of the pillar in front?",
    "answer": "To warn the user about a nearby obstacle so they wouldn’t run into the pillar while going straight.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01155.mp4",
    "question_id": "01155_2",
    "clip_path": "clips/01155/01155__0005500_0012500.mp4"
  },
  {
    "timestamp": "00:00 - 00:14",
    "context": "The environment is characterized by the low hum of distant conversations and activity in a busy store.",
    "question_type": "Sound Source Identification",
    "question": "What generated the low hum heard in the background?",
    "answer": "Distant conversations and general store activity.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01155.mp4",
    "question_id": "01155_3",
    "clip_path": "clips/01155/01155__0000000_0014500.mp4"
  },
  {
    "timestamp": "00:00 - 00:14",
    "context": "The dialogue is captured at a clear, moderate volume from directly in front.",
    "question_type": "Sound Characteristics",
    "question": "What was the volume and clarity of the dialogue?",
    "answer": "Clear, moderate volume.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01155.mp4",
    "question_id": "01155_4",
    "clip_path": "clips/01155/01155__0000000_0014500.mp4"
  },
  {
    "timestamp": "00:00 - 00:14",
    "context": "The dialogue between the user and the man is captured from directly in front of the camera.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From where relative to the camera did the speech primarily originate?",
    "answer": "Directly in front of the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01155.mp4",
    "question_id": "01155_5",
    "clip_path": "clips/01155/01155__0000000_0014500.mp4"
  },
  {
    "timestamp": "00:00 - 00:14",
    "context": "The conversation spans the entire described segment from start to finish.",
    "question_type": "Temporal Information",
    "question": "During what time interval did the conversation take place, and approximately how long did it last?",
    "answer": "From 00:00 to 00:14, lasting about 14 seconds.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01155.mp4",
    "question_id": "01155_6",
    "clip_path": "clips/01155/01155__0000000_0014500.mp4"
  },
  {
    "timestamp": "00:00 - 00:14",
    "context": "User speaks multiple times: asks where the restaurant is; asks which way; thanks and confirms; restates 'go straight'.",
    "question_type": "Counting",
    "question": "How many separate times did the user speak during the exchange?",
    "answer": "Four times.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01155.mp4",
    "question_id": "01155_7",
    "clip_path": "clips/01155/01155__0000000_0014500.mp4"
  },
  {
    "timestamp": "00:00 - 00:14",
    "context": "The man responds twice: first explaining where the restaurant area is; then giving step-by-step directions and a warning.",
    "question_type": "Counting",
    "question": "How many times did the man speak during the exchange?",
    "answer": "Two times.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01155.mp4",
    "question_id": "01155_8",
    "clip_path": "clips/01155/01155__0000000_0014500.mp4"
  },
  {
    "timestamp": "00:01 - 00:06",
    "context": "[00:01 - 00:05] A female voice from the camera says, “这是门吗？” and “感觉像是走到了员工休息室之类的地方.” [00:05 - 00:06] The staff member replies, “餐厅啊.”",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the staff member say “餐厅啊” at 00:05–00:06?",
    "answer": "To correct the camera-side person’s mistaken belief that they had entered a staff lounge.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01156.mp4",
    "question_id": "01156_1",
    "clip_path": "clips/01156/01156__0000500_0006500.mp4"
  },
  {
    "timestamp": "00:05 - 00:06",
    "context": "The staff member, directly in front of the camera about 1–2 meters away, responds clearly with “餐厅啊.”",
    "question_type": "Sound Source Identification",
    "question": "Who spoke the word “餐厅啊”?",
    "answer": "The female staff member in a yellow uniform standing 1–2 meters in front of the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01156.mp4",
    "question_id": "01156_2",
    "clip_path": "clips/01156/01156__0004500_0006500.mp4"
  },
  {
    "timestamp": "00:05 - 00:06",
    "context": "The staff member is directly in front of the camera about 1–2 meters away when she says, “餐厅啊.”",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the “餐厅啊” reply originate relative to the camera?",
    "answer": "Directly in front of the camera, about 1–2 meters away.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01156.mp4",
    "question_id": "01156_3",
    "clip_path": "clips/01156/01156__0004500_0006500.mp4"
  },
  {
    "timestamp": "00:01 - 00:05",
    "context": "A female voice, originating from the camera’s perspective, speaks in a curious and slightly confused tone.",
    "question_type": "Sound Characteristics",
    "question": "What was the tone of the initial female voice at 00:01–00:05?",
    "answer": "Curious and slightly confused.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01156.mp4",
    "question_id": "01156_4",
    "clip_path": "clips/01156/01156__0000500_0005500.mp4"
  },
  {
    "timestamp": "00:01 - 00:05",
    "context": "A female voice is described as originating from the camera’s perspective.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the initial female voice originate relative to the camera?",
    "answer": "From the camera’s position—very close to the microphone.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01156.mp4",
    "question_id": "01156_5",
    "clip_path": "clips/01156/01156__0000500_0005500.mp4"
  },
  {
    "timestamp": "00:06 - 00:13",
    "context": "A narrative female voiceover begins and explains the interaction during this interval.",
    "question_type": "Temporal Information",
    "question": "When did the narration begin and how long did it continue?",
    "answer": "It began at 00:06 and continued until 00:13, lasting about 7 seconds.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01156.mp4",
    "question_id": "01156_6",
    "clip_path": "clips/01156/01156__0005500_0013500.mp4"
  },
  {
    "timestamp": "00:01 - 00:05",
    "context": "The camera-side female says “这是门吗？” and then adds “感觉像是走到了员工休息室之类的地方.”",
    "question_type": "Counting",
    "question": "How many distinct statements did the camera-side female make in this interval?",
    "answer": "Two: one question and one observation.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01156.mp4",
    "question_id": "01156_7",
    "clip_path": "clips/01156/01156__0000500_0005500.mp4"
  },
  {
    "timestamp": "00:06 - 00:13",
    "context": "Narration: “好在遇到的大姐很热心, 一路帮我们买好了甜筒, 还帮我们找到了座位.”",
    "question_type": "Inferential & Contextual Causality",
    "question": "According to the narration, why is the lady described as very enthusiastic (热心)?",
    "answer": "Because she helped them buy an ice cream cone and found them a seat.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01156.mp4",
    "question_id": "01156_8",
    "clip_path": "clips/01156/01156__0005500_0013500.mp4"
  },
  {
    "timestamp": "00:13 - 00:14",
    "context": "Another female voice, originating very close to the camera, says, “我们想去买个冰激凌.”",
    "question_type": "Sound Source Identification",
    "question": "Who said “我们想去买个冰激凌”?",
    "answer": "Another female speaker positioned very close to the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01156.mp4",
    "question_id": "01156_9",
    "clip_path": "clips/01156/01156__0012500_0014500.mp4"
  },
  {
    "timestamp": "00:13 - 00:14",
    "context": "Another female voice is noted as originating very close to the camera when stating the goal to buy ice cream.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "What was the location of the voice that said “我们想去买个冰激凌” relative to the camera?",
    "answer": "Very close to the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01156.mp4",
    "question_id": "01156_10",
    "clip_path": "clips/01156/01156__0012500_0014500.mp4"
  },
  {
    "timestamp": "00:13 - 00:16",
    "context": "[00:13 - 00:14] Another female voice near the camera states the goal to buy ice cream. [00:14 - 00:16] The staff member points left and says, “哦买冰激凌在那头.”",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the staff member point left and say “哦买冰激凌在那头”?",
    "answer": "She was responding to the nearby speaker’s request to buy ice cream.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01156.mp4",
    "question_id": "01156_11",
    "clip_path": "clips/01156/01156__0012500_0016167.mp4"
  },
  {
    "timestamp": "00:14 - 00:16",
    "context": "The staff member provides clear instructions, saying, “哦买冰激凌在那头.”",
    "question_type": "Sound Characteristics",
    "question": "How clear were the staff member’s directions at 00:14–00:16?",
    "answer": "They were clear.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01156.mp4",
    "question_id": "01156_12",
    "clip_path": "clips/01156/01156__0013500_0016167.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "[00:00 - 00:04] The user walks toward a security checkpoint in a public transit station and speaks directly into the camera mic: \"...I'm so busy, and then I don't know when to turn.\" The space has a low ambient hum and faint, distant chatter.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why is the user speaking this clear, conversational monologue during 00:00–00:04?",
    "answer": "To narrate their journey or thoughts while navigating the station.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01164.mp4",
    "question_id": "01164_1",
    "clip_path": "clips/01164/01164__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "[00:00 - 00:04] The station environment is filled with a low ambient hum and faint, distant chatter while the user speaks.",
    "question_type": "Sound Characteristics",
    "question": "What are the volume and character of the background sounds in 00:00–00:04?",
    "answer": "A low ambient hum with faint, distant chatter.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01164.mp4",
    "question_id": "01164_2",
    "clip_path": "clips/01164/01164__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:04 - 00:08",
    "context": "[00:04 - 00:08] While passing through the metal detector, a female guard on the left, about 1–2 meters away, speaks first.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From what direction and approximate distance did the first speaking guard address the user?",
    "answer": "From the left side, about 1–2 meters away.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01164.mp4",
    "question_id": "01164_3",
    "clip_path": "clips/01164/01164__0003500_0008500.mp4"
  },
  {
    "timestamp": "00:04 - 00:08",
    "context": "[00:04 - 00:08] After the female guard speaks, another guard on the right asks a question in a clear, direct tone.",
    "question_type": "Sound Source Identification",
    "question": "Who generated the question heard during the security screening at 00:04–00:08?",
    "answer": "Another guard on the right.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01164.mp4",
    "question_id": "01164_4",
    "clip_path": "clips/01164/01164__0003500_0008500.mp4"
  },
  {
    "timestamp": "00:04 - 00:08",
    "context": "[00:04 - 00:08] A brief, multi-part exchange occurs: a female guard speaks first, then another guard asks a question.",
    "question_type": "Counting",
    "question": "How many guards' voices are heard during the exchange at 00:04–00:08?",
    "answer": "Two.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01164.mp4",
    "question_id": "01164_5",
    "clip_path": "clips/01164/01164__0003500_0008500.mp4"
  },
  {
    "timestamp": "00:04 - 00:08",
    "context": "[00:04 - 00:08] The second guard's question is delivered during the screening in a clear, direct tone.",
    "question_type": "Sound Characteristics",
    "question": "What was the tone quality of the guard's question during 00:04–00:08?",
    "answer": "It was delivered in a clear, direct tone.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01164.mp4",
    "question_id": "01164_6",
    "clip_path": "clips/01164/01164__0003500_0008500.mp4"
  },
  {
    "timestamp": "00:04 - 00:08",
    "context": "[00:04 - 00:08] The user passes through the metal detector and engages in a brief, multi-part exchange with the security staff.",
    "question_type": "Temporal Information",
    "question": "During what time interval does the brief, multi-part exchange with security staff occur?",
    "answer": "From 00:04 to 00:08, while passing through the metal detector.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01164.mp4",
    "question_id": "01164_7",
    "clip_path": "clips/01164/01164__0003500_0008500.mp4"
  },
  {
    "timestamp": "00:08 - 00:12",
    "context": "[00:08 - 00:12] Immediately after clearing the gate, the user turns slightly toward the staff and responds in a calm, reassuring voice from point-blank range: \"Oh, it's okay, you don't need to be nervous.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "What was the likely purpose of the user's reassuring remark right after clearing the gate?",
    "answer": "To reassure the guard and de-escalate any perceived awkwardness following their reaction, possibly to being filmed.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01164.mp4",
    "question_id": "01164_8",
    "clip_path": "clips/01164/01164__0007500_0012200.mp4"
  },
  {
    "timestamp": "00:08 - 00:12",
    "context": "[00:08 - 00:12] The user delivers the reassuring line in a calm voice from point-blank range after turning toward the staff.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the user's reassuring reply originate relative to the camera?",
    "answer": "From point-blank range.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01164.mp4",
    "question_id": "01164_9",
    "clip_path": "clips/01164/01164__0007500_0012200.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "An airport staff member in a black uniform at the front-right confirms identity and says, “您扶着我，我扶着您 (Hold on to me, I'll guide you).”",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the staff member say “Hold on to me, I'll guide you” at the start?",
    "answer": "To offer assistance and guide the user safely through the terminal after confirming her identity.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01169.mp4",
    "question_id": "01169_1",
    "clip_path": "clips/01169/01169__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "Another male voice is heard from the left saying farewells: “没事儿再见…祝您旅途愉快，谢谢.”",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From which direction did the male voice saying goodbye originate?",
    "answer": "From the left side of the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01169.mp4",
    "question_id": "01169_2",
    "clip_path": "clips/01169/01169__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "The staff member speaks in a clear, moderate voice from the front-right.",
    "question_type": "Sound Characteristics",
    "question": "How is the staff member’s voice described during the initial interaction?",
    "answer": "Clear and moderate in volume.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01169.mp4",
    "question_id": "01169_3",
    "clip_path": "clips/01169/01169__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "The staff member’s speech is noted as coming from the front-right.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where was the staff member’s voice located relative to the camera during the initial exchange?",
    "answer": "From the front-right.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01169.mp4",
    "question_id": "01169_4",
    "clip_path": "clips/01169/01169__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:07 - 00:48",
    "context": "As the user is guided by a woman in a grey coat, faint footsteps are audible on the polished floor.",
    "question_type": "Sound Source Identification",
    "question": "What produced the faint sounds on the polished floor during the walk?",
    "answer": "Their footsteps as the user and staff member walked.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01169.mp4",
    "question_id": "01169_5",
    "clip_path": "clips/01169/01169__0006500_0048500.mp4"
  },
  {
    "timestamp": "00:07 - 00:48",
    "context": "A calm female narrator’s voiceover begins while they walk through the terminal.",
    "question_type": "Temporal Information",
    "question": "When does the narrator’s monologue begin and over what period is it heard?",
    "answer": "It begins at 00:07 and continues during the walk until 00:48.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01169.mp4",
    "question_id": "01169_6",
    "clip_path": "clips/01169/01169__0006500_0048500.mp4"
  },
  {
    "timestamp": "00:07 - 00:48",
    "context": "The sound of their footsteps is described as faintly audible.",
    "question_type": "Sound Characteristics",
    "question": "What is the volume level of the footsteps heard during the walk?",
    "answer": "Faint.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01169.mp4",
    "question_id": "01169_7",
    "clip_path": "clips/01169/01169__0006500_0048500.mp4"
  },
  {
    "timestamp": "00:07 - 00:48",
    "context": "Narrator: She was pleasantly surprised that staff seemed professionally trained and knew to let a blind person hold their elbow.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why was the narrator pleasantly surprised after being met by the second staff member?",
    "answer": "Because the staff appeared professionally trained to guide the blind and knew to have her hold their elbow.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01169.mp4",
    "question_id": "01169_8",
    "clip_path": "clips/01169/01169__0006500_0048500.mp4"
  },
  {
    "timestamp": "00:07 - 00:48",
    "context": "Narrator explains that supporting/pushing a blind person from behind makes them feel insecure and hesitant since they can’t see ahead, slowing the pace.",
    "question_type": "Inferential & Contextual Causality",
    "question": "According to the narration, why can supporting a blind person from behind slow walking speed?",
    "answer": "Because they can’t see the road ahead and feel unsafe and hesitant, which slows their pace.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01169.mp4",
    "question_id": "01169_9",
    "clip_path": "clips/01169/01169__0006500_0048500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "Dialogue between the staff member, the user, and another male voice with greetings and farewells.",
    "question_type": "Counting",
    "question": "How many distinct speakers are heard in this opening segment?",
    "answer": "Three: the staff member, the user, and another male voice.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01169.mp4",
    "question_id": "01169_10",
    "clip_path": "clips/01169/01169__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "Expressions of thanks occur: User says “谢谢您啊,” male voice says “…谢谢,” user says “欸，谢谢.”",
    "question_type": "Counting",
    "question": "How many times is “谢谢” (thanks) expressed in this segment?",
    "answer": "Three times.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01169.mp4",
    "question_id": "01169_11",
    "clip_path": "clips/01169/01169__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:48 - 00:50",
    "context": "While walking along a yellow-marked path, the guide says, “坐摆渡车，对好,” and the user replies “好.”",
    "question_type": "Temporal Information",
    "question": "When is the suggestion to take a shuttle made?",
    "answer": "Between 00:48 and 00:50.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01169.mp4",
    "question_id": "01169_12",
    "clip_path": "clips/01169/01169__0047500_0050500.mp4"
  },
  {
    "timestamp": "00:48 - 00:50",
    "context": "The guide’s voice suggesting the shuttle is described as clear and coming from directly in front.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where is the guide positioned acoustically when suggesting the shuttle?",
    "answer": "Directly in front of the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01169.mp4",
    "question_id": "01169_13",
    "clip_path": "clips/01169/01169__0047500_0050500.mp4"
  },
  {
    "timestamp": "00:07 - 00:48",
    "context": "Narration explains proper guiding technique, which the airport staff are demonstrating during the walk.",
    "question_type": "Cross-Modal Reasoning",
    "question": "While the narrator speaks, what are the staff likely demonstrating visually?",
    "answer": "Letting the blind person hold the guide’s elbow and walk half a step behind to follow their pace and direction.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01169.mp4",
    "question_id": "01169_14",
    "clip_path": "clips/01169/01169__0006500_0048500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "[00:00 - 00:04] Inside an elevator, a person puts on a bright yellow reflective vest; its fabric produces a soft rustling sound.",
    "question_type": "Sound Source Identification",
    "question": "What generated the soft rustling sound at the start of the video?",
    "answer": "The fabric of the bright yellow reflective vest being put on.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01171.mp4",
    "question_id": "01171_1",
    "clip_path": "clips/01171/01171__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "[00:00 - 00:04] The vest is put on, and its fabric produces a soft rustling sound.",
    "question_type": "Sound Characteristics",
    "question": "What was the acoustic quality of the vest-related sound heard at 00:00–00:04?",
    "answer": "A soft rustling sound.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01171.mp4",
    "question_id": "01171_2",
    "clip_path": "clips/01171/01171__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "[00:00 - 00:04] A low, mechanical whirring sound is heard as the stainless steel elevator doors begin to open.",
    "question_type": "Sound Source Identification",
    "question": "What produced the low, mechanical whirring sound at 00:00–00:04?",
    "answer": "The stainless steel elevator doors beginning to open.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01171.mp4",
    "question_id": "01171_3",
    "clip_path": "clips/01171/01171__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "[00:00 - 00:04] As the elevator doors begin to open, a low, mechanical whirring is heard.",
    "question_type": "Temporal Information",
    "question": "When is the low, mechanical whirring heard relative to the events shown?",
    "answer": "During 00:00–00:04 as the elevator doors begin to open.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01171.mp4",
    "question_id": "01171_4",
    "clip_path": "clips/01171/01171__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:00 - 00:24",
    "context": "[00:00 - 00:04] One speaker says, 'Wear a reflective vest.' [00:04 - 00:13] The vest’s purpose is discussed: 'Because for remote stands, you can... you can board first.' [00:13 - 00:24] Further explanation: when returning, they must cross a road, and the vest makes people visible from far away; the listener replies, 'Ah, I see.'",
    "question_type": "Inferential & Contextual Causality",
    "question": "Based on the conversation, why was the reflective vest being worn?",
    "answer": "It was for operations at remote stands allowing earlier boarding and for safety visibility when crossing a road so people can be seen from far away.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01171.mp4",
    "question_id": "01171_5",
    "clip_path": "clips/01171/01171__0000000_0024233.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "[00:00 - 00:04] The vest’s fabric rustles softly; a low mechanical whir accompanies the elevator doors opening.",
    "question_type": "Counting",
    "question": "How many distinct non-speech sound events are heard between 00:00 and 00:04?",
    "answer": "Two: the soft rustling of the vest and the low, mechanical whirring of the elevator doors.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01171.mp4",
    "question_id": "01171_6",
    "clip_path": "clips/01171/01171__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:13 - 00:24",
    "context": "[00:13 - 00:24] The person exits the elevator and walks into the airport terminal; footsteps make soft, echoing sounds on the polished floor.",
    "question_type": "Temporal Information",
    "question": "When are the soft, echoing footsteps audible?",
    "answer": "From 00:13 to 00:24 as the person exits the elevator and walks into the terminal.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01171.mp4",
    "question_id": "01171_7",
    "clip_path": "clips/01171/01171__0012500_0024233.mp4"
  },
  {
    "timestamp": "00:13 - 00:24",
    "context": "[00:13 - 00:24] Footsteps on the polished floor produce an audible pattern.",
    "question_type": "Sound Characteristics",
    "question": "What are the characteristics of the footsteps heard in the terminal?",
    "answer": "They are soft and echoing on the polished floor.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01171.mp4",
    "question_id": "01171_8",
    "clip_path": "clips/01171/01171__0012500_0024233.mp4"
  },
  {
    "timestamp": "00:13 - 00:24",
    "context": "[00:13 - 00:24] The person exits the elevator and walks into the airport terminal; footsteps are heard.",
    "question_type": "Sound Source Identification",
    "question": "What action generated the echoing footsteps in the terminal?",
    "answer": "The person walking on the polished floor after exiting the elevator.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01171.mp4",
    "question_id": "01171_9",
    "clip_path": "clips/01171/01171__0012500_0024233.mp4"
  },
  {
    "timestamp": "00:13 - 00:24",
    "context": "[00:13 - 00:24] One person explains the vest’s safety function about crossing a road and being visible from far away; the other responds, 'Ah, I see.'",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the person say 'Ah, I see' at the end of the conversation?",
    "answer": "They understood the explanation that the vest improves visibility from far away when crossing a road.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01171.mp4",
    "question_id": "01171_10",
    "clip_path": "clips/01171/01171__0012500_0024233.mp4"
  },
  {
    "timestamp": "00:00 - 00:13",
    "context": "[00:00 - 00:04] 'Are we going to take the shuttle bus now?' [00:04 - 00:13] 'Isn't there a car/bus?'",
    "question_type": "Counting",
    "question": "How many questions about taking a shuttle bus were asked between 00:00 and 00:13?",
    "answer": "Two.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01171.mp4",
    "question_id": "01171_11",
    "clip_path": "clips/01171/01171__0000000_0013500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "A staff member at the bus door says, \"这边的上面有台阶哦, 没问题\" (There are steps up here, no problem). This dialogue serves as a verbal instruction to guide the user safely onto the bus.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the staff member say, \"There are steps up here, no problem\"?",
    "answer": "To instruct and guide the user safely onto the bus.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01174.mp4",
    "question_id": "01174_1",
    "clip_path": "clips/01174/01174__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "The low, continuous hum of the bus engine is audible in the background.",
    "question_type": "Sound Characteristics",
    "question": "What are the volume and continuity characteristics of the background hum?",
    "answer": "It is a low, continuous hum.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01174.mp4",
    "question_id": "01174_2",
    "clip_path": "clips/01174/01174__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "A staff member speaks to the user in a clear, medium-volume voice from the front-left.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From where relative to the camera did the staff member's initial instruction originate?",
    "answer": "From the front-left.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01174.mp4",
    "question_id": "01174_3",
    "clip_path": "clips/01174/01174__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:03 - 00:07",
    "context": "The user asks, \"您跟我一起吗还是我自己过去\" (Are you coming with me or should I go by myself?). The staff member immediately replies, \"哦我带您\" (Oh, I'll take you).",
    "question_type": "Temporal Information",
    "question": "How quickly did the staff member respond to the user's question about assistance?",
    "answer": "Immediately.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01174.mp4",
    "question_id": "01174_4",
    "clip_path": "clips/01174/01174__0002500_0007500.mp4"
  },
  {
    "timestamp": "00:07 - 00:11",
    "context": "The user is guided onto the bus. The sounds of their footsteps are audible as they walk on the bus floor.",
    "question_type": "Sound Source Identification",
    "question": "What produced the audible footsteps during this interval?",
    "answer": "The user's footsteps on the bus floor as they were guided onto the bus.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01174.mp4",
    "question_id": "01174_5",
    "clip_path": "clips/01174/01174__0006500_0011500.mp4"
  },
  {
    "timestamp": "00:07 - 00:11",
    "context": "A gentle, melodic acoustic guitar track begins to play.",
    "question_type": "Temporal Information",
    "question": "When does the acoustic guitar music start?",
    "answer": "It begins during 00:07–00:11 as the user is being guided onto the bus.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01174.mp4",
    "question_id": "01174_6",
    "clip_path": "clips/01174/01174__0006500_0011500.mp4"
  },
  {
    "timestamp": "00:07 - 00:11",
    "context": "A gentle, melodic acoustic guitar track begins to play, creating a calm atmosphere.",
    "question_type": "Sound Characteristics",
    "question": "How is the acoustic guitar track described?",
    "answer": "Gentle and melodic.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01174.mp4",
    "question_id": "01174_7",
    "clip_path": "clips/01174/01174__0006500_0011500.mp4"
  },
  {
    "timestamp": "00:11 - 00:13",
    "context": "The staff member directs the user to a seat, saying from the front, \"您坐这儿吧\" (Please sit here).",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From which direction did the staff member say, \"Please sit here\"?",
    "answer": "From the front.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01174.mp4",
    "question_id": "01174_8",
    "clip_path": "clips/01174/01174__0010500_0013500.mp4"
  },
  {
    "timestamp": "00:13 - 00:16",
    "context": "The user sits down, accompanied by the soft rustling sound of clothing as they settle into the seat.",
    "question_type": "Sound Source Identification",
    "question": "What caused the soft rustling sound as the user settled?",
    "answer": "The user's clothing as they sat down.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01174.mp4",
    "question_id": "01174_9",
    "clip_path": "clips/01174/01174__0012500_0016500.mp4"
  },
  {
    "timestamp": "00:00 - 00:13",
    "context": "[00:00 - 00:03] The user responds with \"好\" (Okay). [00:11 - 00:13] The user immediately agrees with \"好\" (Okay).",
    "question_type": "Counting",
    "question": "How many times does the user say \"好\" (Okay) in this segment?",
    "answer": "Twice.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01174.mp4",
    "question_id": "01174_10",
    "clip_path": "clips/01174/01174__0000000_0013500.mp4"
  },
  {
    "timestamp": "00:05 - 00:07",
    "context": "[00:00 - 00:05] She says, \"How to smoothly find the door to enter the airport after getting off the car is the biggest problem I am currently facing.\" [00:05 - 00:07] She adds, \"So I decided to follow the sound of other people's suitcases to find the door,\" as faint, continuous rolling of luggage wheels is heard ahead.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Based on her stated challenge, why did the user decide to follow the sound of other people's suitcases?",
    "answer": "To use the suitcase-wheel sounds as an audio cue to smoothly locate and enter the airport door.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01181.mp4",
    "question_id": "01181_1",
    "clip_path": "clips/01181/01181__0004500_0006967.mp4"
  },
  {
    "timestamp": "00:05 - 00:07",
    "context": "[00:05 - 00:07] Coinciding with her statement, the faint, continuous rolling sound of luggage wheels can be heard from travelers walking in front of her.",
    "question_type": "Sound Source Identification",
    "question": "What generated the faint, continuous rolling sound heard during 00:05 - 00:07?",
    "answer": "The luggage wheels of other travelers walking in front of her.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01181.mp4",
    "question_id": "01181_2",
    "clip_path": "clips/01181/01181__0004500_0006967.mp4"
  },
  {
    "timestamp": "00:05 - 00:07",
    "context": "[00:05 - 00:07] A faint, continuous rolling sound of luggage wheels is audible.",
    "question_type": "Sound Characteristics",
    "question": "What are the intensity and continuity characteristics of the luggage sound at 00:05 - 00:07?",
    "answer": "It is faint and continuous, with a rolling quality.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01181.mp4",
    "question_id": "01181_3",
    "clip_path": "clips/01181/01181__0004500_0006967.mp4"
  },
  {
    "timestamp": "00:05 - 00:07",
    "context": "[00:05 - 00:07] The rolling luggage sound is heard from travelers walking in front of her.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Relative to the camera, from where does the rolling luggage sound originate?",
    "answer": "From in front of the camera, produced by travelers ahead.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01181.mp4",
    "question_id": "01181_4",
    "clip_path": "clips/01181/01181__0004500_0006967.mp4"
  },
  {
    "timestamp": "00:05 - 00:07",
    "context": "[00:05 - 00:07] The rolling of luggage wheels coincides with her statement and continues faintly.",
    "question_type": "Temporal Information",
    "question": "When does the rolling luggage sound occur, and is it brief or continuous within this interval?",
    "answer": "It begins around 00:05 and persists through 00:07 as a continuous sound.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01181.mp4",
    "question_id": "01181_5",
    "clip_path": "clips/01181/01181__0004500_0006967.mp4"
  },
  {
    "timestamp": "00:05 - 00:07",
    "context": "[00:05 - 00:07] She speaks while a faint, continuous rolling of luggage wheels is also audible.",
    "question_type": "Counting",
    "question": "How many distinct concurrent sound sources are audible between 00:05 and 00:07?",
    "answer": "Two: her speech and the rolling luggage wheels.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01181.mp4",
    "question_id": "01181_6",
    "clip_path": "clips/01181/01181__0004500_0006967.mp4"
  },
  {
    "timestamp": "00:00 - 00:05",
    "context": "[00:00 - 00:05] Her clear female voice is heard originating directly from the camera's location as she begins a monologue.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where does the speaker's voice originate relative to the camera at the start?",
    "answer": "Directly from the camera's location.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01181.mp4",
    "question_id": "01181_7",
    "clip_path": "clips/01181/01181__0000000_0005500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "[00:00 - 00:05] Visually, travelers walk toward an entrance marked '5.' [00:05 - 00:07] She plans to follow suitcase sounds, and the faint, continuous rolling of luggage wheels from travelers in front is heard.",
    "question_type": "Cross-Modal Reasoning",
    "question": "Using the rolling suitcase sounds as an audio cue, what does the visual scene suggest about where those travelers are heading?",
    "answer": "They are walking toward an entrance marked with a large number '5.'",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01181.mp4",
    "question_id": "01181_8",
    "clip_path": "clips/01181/01181__0000000_0006967.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "A series of sharp, rhythmic, moderately loud tapping sounds is heard from directly in front of the camera, caused by a person using a white cane to navigate a city sidewalk.",
    "question_type": "Sound Source Identification",
    "question": "What generated the tapping sounds heard during 00:00–00:04?",
    "answer": "A person’s white cane used to navigate the city sidewalk.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01187.mp4",
    "question_id": "01187_1",
    "clip_path": "clips/01187/01187__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "A series of sharp, rhythmic, and moderately loud tapping sounds is heard, with a consistent tempo matching the person's walking pace.",
    "question_type": "Sound Characteristics",
    "question": "What are the acoustic characteristics of the tapping sounds?",
    "answer": "They are sharp, rhythmic, and moderately loud, with a consistent tempo.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01187.mp4",
    "question_id": "01187_2",
    "clip_path": "clips/01187/01187__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "The tapping sounds are heard from directly in front of the camera.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where relative to the camera did the tapping originate?",
    "answer": "Directly in front of the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01187.mp4",
    "question_id": "01187_3",
    "clip_path": "clips/01187/01187__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "A series of tapping sounds is heard throughout the 00:00–00:04 interval.",
    "question_type": "Temporal Information",
    "question": "When do the tapping sounds occur and do they persist throughout the interval?",
    "answer": "They occur from 00:00 to 00:04 as a continuing series over that period.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01187.mp4",
    "question_id": "01187_4",
    "clip_path": "clips/01187/01187__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "As the person approaches a planter with bushes, they say in Mandarin, '要绕出来' ('Have to go around'), and then proceed to walk around the obstacle.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the person say '要绕出来' ('Have to go around')?",
    "answer": "Because they were approaching a planter with bushes that obstructed their path and needed to bypass it.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01187.mp4",
    "question_id": "01187_5",
    "clip_path": "clips/01187/01187__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "The person audibly states '要绕出来' and then proceeds to walk around the planter with bushes.",
    "question_type": "Cross-Modal Reasoning",
    "question": "After the person says '要绕出来', what action follows and what object is involved?",
    "answer": "They walk around the obstacle—the planter with bushes.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01187.mp4",
    "question_id": "01187_6",
    "clip_path": "clips/01187/01187__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "Both cane tapping and a spoken Mandarin phrase are heard within the segment.",
    "question_type": "Counting",
    "question": "How many distinct types of sounds are heard in this segment?",
    "answer": "Two: the cane tapping and the spoken Mandarin utterance.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01187.mp4",
    "question_id": "01187_7",
    "clip_path": "clips/01187/01187__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "As the user walks, they produce three distinct, sharp, rhythmic clicking sounds of moderate volume from their own position.",
    "question_type": "Counting",
    "question": "How many clicking sounds are produced between 00:00 and 00:03?",
    "answer": "Three.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01188.mp4",
    "question_id": "01188_1",
    "clip_path": "clips/01188/01188__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "Three clicking sounds are heard; they are described as distinct, sharp, rhythmic, and of moderate volume.",
    "question_type": "Sound Characteristics",
    "question": "What are the acoustic characteristics and volume of the clicking sounds at 00:00–00:03?",
    "answer": "They are distinct, sharp, rhythmic, and of moderate volume.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01188.mp4",
    "question_id": "01188_2",
    "clip_path": "clips/01188/01188__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "The clicking sounds originate directly from the user's position while walking.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where do the clicking sounds originate relative to the camera?",
    "answer": "Directly from the user's position, essentially at the camera location (close range).",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01188.mp4",
    "question_id": "01188_3",
    "clip_path": "clips/01188/01188__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "The caption notes the clicks were likely generated by a handheld device.",
    "question_type": "Sound Source Identification",
    "question": "What likely generated the clicking sounds heard at the start?",
    "answer": "A handheld device used by the user.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01188.mp4",
    "question_id": "01188_4",
    "clip_path": "clips/01188/01188__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "The three clicks occur as the user walks along the walkway within the first three seconds.",
    "question_type": "Temporal Information",
    "question": "When do the clicking sounds occur and what is the length of the sequence?",
    "answer": "Between 00:00 and 00:03, as a brief three-second sequence.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01188.mp4",
    "question_id": "01188_5",
    "clip_path": "clips/01188/01188__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:03 - 00:09",
    "context": "A nearby male companion asks, “你都不敲了吗?” and the user replies, “这儿走熟了, 我就不敲了.”",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why does the user say they don’t tap here anymore?",
    "answer": "Because they are familiar with this path.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01188.mp4",
    "question_id": "01188_6",
    "clip_path": "clips/01188/01188__0002500_0009500.mp4"
  },
  {
    "timestamp": "00:00 - 00:09",
    "context": "Initial clicking sounds are followed by a conversation beginning with a nearby male companion.",
    "question_type": "Temporal Information",
    "question": "What audio event follows immediately after the initial clicking sequence?",
    "answer": "A conversation starts with a nearby male companion asking a question.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01188.mp4",
    "question_id": "01188_7",
    "clip_path": "clips/01188/01188__0000000_0009500.mp4"
  },
  {
    "timestamp": "00:03 - 00:09",
    "context": "The male companion’s voice is described as coming from nearby while walking alongside.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where is the male companion’s voice located relative to the camera during the conversation?",
    "answer": "Nearby, from someone walking alongside the user.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01188.mp4",
    "question_id": "01188_8",
    "clip_path": "clips/01188/01188__0002500_0009500.mp4"
  },
  {
    "timestamp": "00:03 - 00:09",
    "context": "The dialogue indicates the user taps when unfamiliar with an area and stops when familiar.",
    "question_type": "Inferential & Contextual Causality",
    "question": "According to the dialogue, what was the purpose of the earlier clicking sounds?",
    "answer": "They were a deliberate navigational technique used in unfamiliar areas.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01188.mp4",
    "question_id": "01188_9",
    "clip_path": "clips/01188/01188__0002500_0009500.mp4"
  },
  {
    "timestamp": "00:10 - 00:16",
    "context": "A man passes from the right and greets from about 1–2 meters away, saying, “早啊, 你们两个, 早啊.”",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From what direction and approximate distance does the passerby’s greeting originate?",
    "answer": "From the right side, about 1–2 meters away.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01188.mp4",
    "question_id": "01188_10",
    "clip_path": "clips/01188/01188__0009500_0016500.mp4"
  },
  {
    "timestamp": "00:10 - 00:16",
    "context": "The passerby says, “早啊, 你们两个, 早啊.”",
    "question_type": "Counting",
    "question": "How many times does the passerby say “早啊” (Morning) in his greeting?",
    "answer": "Twice.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01188.mp4",
    "question_id": "01188_11",
    "clip_path": "clips/01188/01188__0009500_0016500.mp4"
  },
  {
    "timestamp": "00:10 - 00:16",
    "context": "After the greeting, a female narrator says: “又遇到了一个同事, 公司的财务素素姐.”",
    "question_type": "Cross-Modal Reasoning",
    "question": "Based on the narration immediately following the greeting, who is identified as the person they ran into?",
    "answer": "A colleague from the company’s finance department, Susu Jie.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01188.mp4",
    "question_id": "01188_12",
    "clip_path": "clips/01188/01188__0009500_0016500.mp4"
  },
  {
    "timestamp": "00:00 - 00:01",
    "context": "[00:00] A security guard holds a card to the scanner, producing a short, high-pitched electronic beep. Immediately after, the turnstile gates make a low-volume mechanical whirring as they retract to grant access.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Based on the sequence of events, why did the turnstile gates begin retracting at 00:00 - 00:01?",
    "answer": "They retracted to grant access after the card was scanned and confirmed, indicated by the electronic beep.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01189.mp4",
    "question_id": "01189_1",
    "clip_path": "clips/01189/01189__0000000_0001500.mp4"
  },
  {
    "timestamp": "00:00 - 00:01",
    "context": "[00:00] A man in a black uniform holds a card to the scanner, which produces a short, high-pitched electronic beep.",
    "question_type": "Sound Source Identification",
    "question": "What generated the high-pitched electronic beep at 00:00?",
    "answer": "The scanner produced the beep when the security guard held a card to it.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01189.mp4",
    "question_id": "01189_2",
    "clip_path": "clips/01189/01189__0000000_0001500.mp4"
  },
  {
    "timestamp": "00:00 - 00:01",
    "context": "[00:00] The action produces a short, high-pitched electronic beep.",
    "question_type": "Sound Characteristics",
    "question": "What are the acoustic characteristics of the electronic beep heard at 00:00?",
    "answer": "It is short and high-pitched.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01189.mp4",
    "question_id": "01189_3",
    "clip_path": "clips/01189/01189__0000000_0001500.mp4"
  },
  {
    "timestamp": "00:00 - 00:01",
    "context": "[00:00 - 00:01] The beep is immediately followed by the distinct, low-volume mechanical whirring of the turnstile gates retracting.",
    "question_type": "Sound Characteristics",
    "question": "How is the mechanical whirring of the turnstile described in terms of volume and texture?",
    "answer": "It is a distinct, low-volume mechanical whirring.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01189.mp4",
    "question_id": "01189_4",
    "clip_path": "clips/01189/01189__0000000_0001500.mp4"
  },
  {
    "timestamp": "00:00 - 00:01",
    "context": "[00:00 - 00:01] A short beep occurs and is immediately followed by the mechanical whirring of the turnstile gates retracting.",
    "question_type": "Temporal Information",
    "question": "What is the temporal relationship between the beep and the mechanical whirring?",
    "answer": "The mechanical whirring occurs immediately after the beep.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01189.mp4",
    "question_id": "01189_5",
    "clip_path": "clips/01189/01189__0000000_0001500.mp4"
  },
  {
    "timestamp": "00:01 - 00:05",
    "context": "[00:01 - 00:05] As the camera holder walks through the open turnstile, a clear female voice originating from the camera's position says in Mandarin: \"Thank you. The security guard swipes the card for us every day.\"",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where does the speaking voice originate relative to the camera during 00:01 - 00:05?",
    "answer": "Directly from the camera's position.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01189.mp4",
    "question_id": "01189_6",
    "clip_path": "clips/01189/01189__0000500_0005500.mp4"
  },
  {
    "timestamp": "00:01 - 00:05",
    "context": "[00:01 - 00:05] The camera holder walks through the open turnstile while the female voice speaks.",
    "question_type": "Temporal Information",
    "question": "When does the female voice speak in relation to passing through the turnstile?",
    "answer": "As she passes through the open turnstile, during 00:01 - 00:05.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01189.mp4",
    "question_id": "01189_7",
    "clip_path": "clips/01189/01189__0000500_0005500.mp4"
  },
  {
    "timestamp": "00:01 - 00:05",
    "context": "[00:01 - 00:05] Her soft footsteps can be heard echoing slightly in the large, tiled lobby as she walks toward the elevators.",
    "question_type": "Sound Characteristics",
    "question": "How are the footsteps described acoustically while walking toward the elevators?",
    "answer": "They are soft and echo slightly.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01189.mp4",
    "question_id": "01189_8",
    "clip_path": "clips/01189/01189__0000500_0005500.mp4"
  },
  {
    "timestamp": "00:00 - 00:01",
    "context": "[00:00 - 00:01] A short electronic beep is heard, immediately followed by mechanical whirring from the turnstile gates.",
    "question_type": "Counting",
    "question": "How many distinct sound events are described between 00:00 and 00:01?",
    "answer": "Two: the electronic beep and the mechanical whirring.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01189.mp4",
    "question_id": "01189_9",
    "clip_path": "clips/01189/01189__0000000_0001500.mp4"
  },
  {
    "timestamp": "00:01 - 00:05",
    "context": "[00:01 - 00:05] A clear female voice originating from the camera's position speaks in Mandarin.",
    "question_type": "Counting",
    "question": "How many speaking voices are heard during 00:01 - 00:05?",
    "answer": "One clear female voice.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01189.mp4",
    "question_id": "01189_10",
    "clip_path": "clips/01189/01189__0000500_0005500.mp4"
  },
  {
    "timestamp": "00:01 - 00:05",
    "context": "[00:01 - 00:05] The female voice says, \"Thank you. The security guard swipes the card for us every day,\" providing context and gratitude.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why does the speaker say, \"Thank you. The security guard swipes the card for us every day\"?",
    "answer": "She is expressing gratitude and explaining that the guard routinely scans the card to let them through.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01189.mp4",
    "question_id": "01189_11",
    "clip_path": "clips/01189/01189__0000500_0005500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "[00:00 - 00:03] First-person walking at night using a white cane, which produces a series of rhythmic, sharp tapping and scraping sounds as its tip contacts the ground.",
    "question_type": "Sound Characteristics",
    "question": "What are the acoustic characteristics of the cane sounds at the start?",
    "answer": "They are rhythmic, sharp tapping and scraping sounds.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01195.mp4",
    "question_id": "01195_1",
    "clip_path": "clips/01195/01195__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "[00:00 - 00:03] The white cane’s tip makes contact with the ground, producing tapping and scraping sounds.",
    "question_type": "Sound Source Identification",
    "question": "What generated the tapping and scraping sounds at 00:00 - 00:03?",
    "answer": "The white cane’s tip contacting the ground.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01195.mp4",
    "question_id": "01195_2",
    "clip_path": "clips/01195/01195__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "[00:00 - 00:03] The cane produces a series of rhythmic tapping and scraping sounds.",
    "question_type": "Temporal Information",
    "question": "Were the cane sounds a single impact or a repeated series during 00:00 - 00:03?",
    "answer": "A repeated series of rhythmic taps and scrapes.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01195.mp4",
    "question_id": "01195_3",
    "clip_path": "clips/01195/01195__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:03 - 00:09",
    "context": "[00:03 - 00:09] As the person approaches a parked silver car on the right, a female voice from the front-right warns, “There's a car ahead, be careful.”",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From which direction relative to the camera did the warning voice originate?",
    "answer": "From the front-right.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01195.mp4",
    "question_id": "01195_4",
    "clip_path": "clips/01195/01195__0002500_0009500.mp4"
  },
  {
    "timestamp": "00:03 - 00:09",
    "context": "[00:03 - 00:09] The person approaches a parked silver car on the right; a female voice warns them, “There's a car ahead, be careful.”",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the female voice issue the warning?",
    "answer": "Because the person was approaching a parked car ahead on the right, posing an obstacle.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01195.mp4",
    "question_id": "01195_5",
    "clip_path": "clips/01195/01195__0002500_0009500.mp4"
  },
  {
    "timestamp": "00:03 - 00:09",
    "context": "[00:03 - 00:09] After the warning, they immediately reply, “Thank you, thank you. It's okay.”",
    "question_type": "Temporal Information",
    "question": "How soon after the warning did the person respond?",
    "answer": "Immediately.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01195.mp4",
    "question_id": "01195_6",
    "clip_path": "clips/01195/01195__0002500_0009500.mp4"
  },
  {
    "timestamp": "00:03 - 00:09",
    "context": "[00:03 - 00:09] The person replies in a grateful tone, “Thank you, thank you. It's okay.”",
    "question_type": "Sound Characteristics",
    "question": "What was the tone of the person's reply to the warning?",
    "answer": "Grateful.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01195.mp4",
    "question_id": "01195_7",
    "clip_path": "clips/01195/01195__0002500_0009500.mp4"
  },
  {
    "timestamp": "00:03 - 00:09",
    "context": "[00:03 - 00:09] The person says, “Thank you, thank you. It's okay.”",
    "question_type": "Counting",
    "question": "How many times did the person say “Thank you” in this exchange?",
    "answer": "Twice.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01195.mp4",
    "question_id": "01195_8",
    "clip_path": "clips/01195/01195__0002500_0009500.mp4"
  },
  {
    "timestamp": "00:03 - 00:09",
    "context": "[00:03 - 00:09] The white cane taps the ground near the car's front tire as the person approaches the parked car on the right.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Relative to the camera, where did the cane tap occur near the car?",
    "answer": "On the right side, near the car’s front tire.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01195.mp4",
    "question_id": "01195_9",
    "clip_path": "clips/01195/01195__0002500_0009500.mp4"
  },
  {
    "timestamp": "00:11 - 00:13",
    "context": "[00:11 - 00:13] After successfully maneuvering past the car and onto a crosswalk, the same female voice offers reassurance from nearby, saying, “Okay now.”",
    "question_type": "Cross-Modal Reasoning",
    "question": "Based on the reassurance “Okay now,” what had the person with the cane just accomplished?",
    "answer": "They had successfully maneuvered past the car and moved onto the crosswalk.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01195.mp4",
    "question_id": "01195_10",
    "clip_path": "clips/01195/01195__0010500_0013351.mp4"
  },
  {
    "timestamp": "00:11 - 00:13",
    "context": "[00:11 - 00:13] The same female voice offers reassurance from nearby, saying, “Okay now.”",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "What was the proximity of the voice saying “Okay now”?",
    "answer": "It was nearby.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01195.mp4",
    "question_id": "01195_11",
    "clip_path": "clips/01195/01195__0010500_0013351.mp4"
  },
  {
    "timestamp": "00:04 - 00:09",
    "context": "[00:04 - 00:09] In a dimly lit hallway, the person uses a white cane to locate a package on the floor. Upon discovering it, they say softly, \"There's a package. Oh, there's a package. I'll come out and get it later,\" deciding to enter their home first.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Based on the events, why did the person say, \"I'll come out and get it later\"?",
    "answer": "After discovering the package at the door, they chose to enter their home first and planned to retrieve the package later.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01199.mp4",
    "question_id": "01199_1",
    "clip_path": "clips/01199/01199__0003500_0009500.mp4"
  },
  {
    "timestamp": "00:04 - 00:09",
    "context": "[00:04 - 00:09] The person says, \"There's a package. Oh, there's a package.\"",
    "question_type": "Counting",
    "question": "How many times did the person say the word \"package\" during this segment?",
    "answer": "Twice.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01199.mp4",
    "question_id": "01199_2",
    "clip_path": "clips/01199/01199__0003500_0009500.mp4"
  },
  {
    "timestamp": "00:04 - 00:09",
    "context": "[00:04 - 00:09] The person speaks upon finding the package: \"There's a package... I'll come out and get it later.\"",
    "question_type": "Sound Characteristics",
    "question": "What was the tone and volume of the person's statement about the package?",
    "answer": "Soft and contemplative.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01199.mp4",
    "question_id": "01199_3",
    "clip_path": "clips/01199/01199__0003500_0009500.mp4"
  },
  {
    "timestamp": "00:09 - 00:11",
    "context": "[00:09 - 00:11] The person opens the front door, producing a metallic click as the lock disengages.",
    "question_type": "Sound Source Identification",
    "question": "What generated the metallic click heard when the person opened the door?",
    "answer": "The front door lock disengaging.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01199.mp4",
    "question_id": "01199_4",
    "clip_path": "clips/01199/01199__0008500_0011500.mp4"
  },
  {
    "timestamp": "00:09 - 00:11",
    "context": "[00:09 - 00:11] Opening the front door produces a loud, sharp metallic click.",
    "question_type": "Temporal Information",
    "question": "When did the metallic click occur, and was it brief or continuous?",
    "answer": "It occurred between 00:09 and 00:11 and was a brief single click.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01199.mp4",
    "question_id": "01199_5",
    "clip_path": "clips/01199/01199__0008500_0011500.mp4"
  },
  {
    "timestamp": "00:11 - 00:16",
    "context": "[00:11 - 00:16] Stepping into the home, the person announces, \"OK. I'm home. The lucky day is over, bye-bye,\" in a cheerful, medium-volume tone.",
    "question_type": "Sound Characteristics",
    "question": "How is the arrival announcement delivered in terms of tone and volume?",
    "answer": "Cheerful and medium-volume.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01199.mp4",
    "question_id": "01199_6",
    "clip_path": "clips/01199/01199__0010500_0016500.mp4"
  },
  {
    "timestamp": "00:11 - 00:16",
    "context": "[00:11 - 00:16] The person says, \"OK. I'm home. The lucky day is over, bye-bye,\" upon entering, which serves as a concluding statement for their outing.",
    "question_type": "Inferential & Contextual Causality",
    "question": "What was the purpose of saying, \"The lucky day is over, bye-bye,\" upon entering the home?",
    "answer": "It served as a concluding statement for their outing, marking their arrival home.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01199.mp4",
    "question_id": "01199_7",
    "clip_path": "clips/01199/01199__0010500_0016500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "A clear, moderate-volume female voice narrates from the user's perspective: \"因为有顺路的同事，所以下班之后我们经常一起去公交站。这个时候我就不用拿盲杖啦。\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Based on the narration, why does the user say they don't need to use their cane during this walk?",
    "answer": "Because a colleague lives in the same direction and they go to the bus stop together after work.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01200.mp4",
    "question_id": "01200_1",
    "clip_path": "clips/01200/01200__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "The user walks through a covered outdoor plaza at night, accompanied by the soft, rhythmic sound of their footsteps on the paved ground.",
    "question_type": "Sound Source Identification",
    "question": "What generated the soft, rhythmic sound heard during the walk?",
    "answer": "The user's footsteps on the paved ground.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01200.mp4",
    "question_id": "01200_2",
    "clip_path": "clips/01200/01200__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "Soft, rhythmic footsteps are audible as the user walks on the paved ground.",
    "question_type": "Sound Characteristics",
    "question": "What are the acoustic qualities of the footsteps heard in this segment?",
    "answer": "They are soft and rhythmic.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01200.mp4",
    "question_id": "01200_3",
    "clip_path": "clips/01200/01200__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "A clear, moderate-volume female voice provides narration directly from the user's perspective.",
    "question_type": "Sound Characteristics",
    "question": "What is the clarity and volume level of the narrator's voice?",
    "answer": "Clear and moderate-volume.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01200.mp4",
    "question_id": "01200_4",
    "clip_path": "clips/01200/01200__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "The narration is described as coming directly from the user's perspective in a first-person view.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where does the narration originate relative to the camera?",
    "answer": "From the user's perspective, close to the camera position.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01200.mp4",
    "question_id": "01200_5",
    "clip_path": "clips/01200/01200__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "The walking is accompanied throughout by the soft, rhythmic sound of footsteps.",
    "question_type": "Temporal Information",
    "question": "When do the footsteps occur and how long do they last within this clip?",
    "answer": "They are present continuously throughout 00:00 to 00:06.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01200.mp4",
    "question_id": "01200_6",
    "clip_path": "clips/01200/01200__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "Within the 6-second segment, only two sounds are described: footsteps and a female narration.",
    "question_type": "Counting",
    "question": "How many distinct sound sources are described in this segment?",
    "answer": "Two: the user's footsteps and a female narrator's voice.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01200.mp4",
    "question_id": "01200_7",
    "clip_path": "clips/01200/01200__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "A rhythmic, distinct tapping sound is continuously audible as the tip of the cane strikes the ground directly in front, serving as a navigation aid.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why is the rhythmic tapping sound heard throughout this segment?",
    "answer": "Because the visually impaired person is using a white cane as a navigation aid, with its tip striking the ground.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01202.mp4",
    "question_id": "01202_1",
    "clip_path": "clips/01202/01202__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "A rhythmic, distinct tapping sound is continuously audible as the tip of the cane strikes the ground directly in front.",
    "question_type": "Sound Source Identification",
    "question": "What generated the rhythmic tapping sound?",
    "answer": "The tip of the white cane striking the ground.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01202.mp4",
    "question_id": "01202_2",
    "clip_path": "clips/01202/01202__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "A rhythmic, distinct tapping sound is continuously audible as the tip of the cane strikes the ground.",
    "question_type": "Sound Characteristics",
    "question": "What are the acoustic characteristics of the tapping sound?",
    "answer": "It is rhythmic, distinct, and continuously audible.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01202.mp4",
    "question_id": "01202_3",
    "clip_path": "clips/01202/01202__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "The tip of the cane strikes the ground directly in front.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where does the tapping originate relative to the camera?",
    "answer": "From directly in front of the camera, at ground level.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01202.mp4",
    "question_id": "01202_4",
    "clip_path": "clips/01202/01202__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "A rhythmic, distinct tapping sound is continuously audible.",
    "question_type": "Temporal Information",
    "question": "Is the tapping brief or continuous during this time window?",
    "answer": "It is continuous throughout 00:00–00:04.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01202.mp4",
    "question_id": "01202_5",
    "clip_path": "clips/01202/01202__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "The person is concluding a conversation, saying in Mandarin, \"Are you OK? I have to go, I've arrived. Thank you. It's okay, bye-bye.\"",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Did the concluding Mandarin speech originate from the camera-wearer or from someone at a distance?",
    "answer": "From the camera-wearer, i.e., very close to the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01202.mp4",
    "question_id": "01202_6",
    "clip_path": "clips/01202/01202__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "The person says, \"I have to go, I've arrived. Thank you. It's okay, bye-bye.\" This exchange implies the other person has just assisted the camera-wearer in reaching their destination and is walking away on the left.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Based on the exchange, why does the speaker thank the other person and say goodbye?",
    "answer": "Because the other person had just assisted them in reaching their destination and was leaving.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01202.mp4",
    "question_id": "01202_7",
    "clip_path": "clips/01202/01202__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "The person says in Mandarin, \"Are you OK? I have to go, I've arrived. Thank you. It's okay, bye-bye.\"",
    "question_type": "Counting",
    "question": "How many separate sentences does the speaker utter in Mandarin?",
    "answer": "Four.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01202.mp4",
    "question_id": "01202_8",
    "clip_path": "clips/01202/01202__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "Two sound events are described: the cane's rhythmic tapping and the speaker's Mandarin speech.",
    "question_type": "Counting",
    "question": "How many distinct types of sounds are described in this segment?",
    "answer": "Two: cane tapping and speech.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01202.mp4",
    "question_id": "01202_9",
    "clip_path": "clips/01202/01202__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:02 - 00:07",
    "context": "An automated, synthesized female voice plays from the user's smartphone, stating in Mandarin that the platform has prioritized and hailed a vehicle, confirming the ride request was successful.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the synthesized female voice announce that a vehicle had been prioritized and hailed?",
    "answer": "To notify the user that their ride request was successfully matched with a vehicle.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01213.mp4",
    "question_id": "01213_1",
    "clip_path": "clips/01213/01213__0001500_0007200.mp4"
  },
  {
    "timestamp": "00:02 - 00:07",
    "context": "The synthesized female voice emanates directly from the smartphone being held by the user.",
    "question_type": "Sound Source Identification",
    "question": "What device generated the synthesized female voice notification?",
    "answer": "The smartphone being held by the user.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01213.mp4",
    "question_id": "01213_2",
    "clip_path": "clips/01213/01213__0001500_0007200.mp4"
  },
  {
    "timestamp": "00:02 - 00:07",
    "context": "An automated, synthesized female voice is heard at a clear, moderate volume.",
    "question_type": "Sound Characteristics",
    "question": "What were the acoustic qualities of the notification voice?",
    "answer": "It was an automated, synthesized female voice at a clear, moderate volume.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01213.mp4",
    "question_id": "01213_3",
    "clip_path": "clips/01213/01213__0001500_0007200.mp4"
  },
  {
    "timestamp": "00:02 - 00:07",
    "context": "Sounds of passing traffic, including cars and scooters, are audible in the background from the nearby street.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the background traffic sounds originate relative to the camera?",
    "answer": "From the nearby street in the background.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01213.mp4",
    "question_id": "01213_4",
    "clip_path": "clips/01213/01213__0001500_0007200.mp4"
  },
  {
    "timestamp": "00:02 - 00:07",
    "context": "The voice notification is heard during this time window.",
    "question_type": "Temporal Information",
    "question": "During what time interval was the voice notification heard?",
    "answer": "Between 00:02 and 00:07.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01213.mp4",
    "question_id": "01213_5",
    "clip_path": "clips/01213/01213__0001500_0007200.mp4"
  },
  {
    "timestamp": "00:02 - 00:07",
    "context": "Background audio includes passing traffic: cars and scooters.",
    "question_type": "Counting",
    "question": "How many types of vehicles are mentioned in the background traffic sounds?",
    "answer": "Two—cars and scooters.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01213.mp4",
    "question_id": "01213_6",
    "clip_path": "clips/01213/01213__0001500_0007200.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "[00:00 - 00:03] From the front-right, a man's voice asks, \"Is this the car you called?\" The user replies, \"Yes, it is.\" Ambient traffic sounds are present. This interaction serves to confirm the ride-hailing pickup.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the man ask, \"Is this the car you called?\"",
    "answer": "To confirm the ride-hailing pickup.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01214.mp4",
    "question_id": "01214_1",
    "clip_path": "clips/01214/01214__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:04 - 00:08",
    "context": "[00:04 - 00:08] The driver walks toward the user and audibly offers to guide her to the car. His speech is slightly muffled but clearly an offer of assistance.",
    "question_type": "Sound Characteristics",
    "question": "How is the driver's offer of assistance acoustically described?",
    "answer": "His speech is slightly muffled but clearly intended as an offer of assistance.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01214.mp4",
    "question_id": "01214_2",
    "clip_path": "clips/01214/01214__0003500_0008500.mp4"
  },
  {
    "timestamp": "00:08 - 00:12",
    "context": "[00:08 - 00:12] The user says, \"Oh, let me hold your arm,\" followed by a rustle of clothing as she holds on. This action indicates she is visually impaired and requires physical guidance.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the user ask to hold the driver's arm?",
    "answer": "Because she is visually impaired and needed physical guidance to the vehicle.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01214.mp4",
    "question_id": "01214_3",
    "clip_path": "clips/01214/01214__0007500_0012500.mp4"
  },
  {
    "timestamp": "00:15 - 00:18",
    "context": "[00:15 - 00:18] While being led along the car, the driver moves to the rear passenger door, and a soft, metallic click is heard as he engages the door handle.",
    "question_type": "Sound Source Identification",
    "question": "What produced the soft, metallic click heard here?",
    "answer": "The driver engaging the rear passenger door handle.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01214.mp4",
    "question_id": "01214_4",
    "clip_path": "clips/01214/01214__0014500_0018500.mp4"
  },
  {
    "timestamp": "00:18 - 00:22",
    "context": "[00:18 - 00:22] The driver opens the car door, making a solid thudding sound as it swings open. He advises, \"Slowly,\" and the user replies, \"Okay.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the driver say \"Slowly\" as the door opened?",
    "answer": "To caution the user as she prepared to enter the vehicle.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01214.mp4",
    "question_id": "01214_5",
    "clip_path": "clips/01214/01214__0017500_0022500.mp4"
  },
  {
    "timestamp": "00:18 - 00:22",
    "context": "[00:18 - 00:22] The car door swings open with a solid thudding sound.",
    "question_type": "Sound Characteristics",
    "question": "How is the sound of the car door opening described?",
    "answer": "A solid thudding sound as it swings open.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01214.mp4",
    "question_id": "01214_6",
    "clip_path": "clips/01214/01214__0017500_0022500.mp4"
  },
  {
    "timestamp": "00:23 - 00:25",
    "context": "[00:23 - 00:25] The sound of fabric rustling is heard as the user gets into the car and settles into the back seat.",
    "question_type": "Sound Source Identification",
    "question": "What action generated the fabric rustling sound?",
    "answer": "The user getting into the car and settling into the back seat.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01214.mp4",
    "question_id": "01214_7",
    "clip_path": "clips/01214/01214__0022500_0025500.mp4"
  },
  {
    "timestamp": "00:23 - 00:25",
    "context": "[00:23 - 00:25] Fabric rustling is audible while the user enters and settles.",
    "question_type": "Temporal Information",
    "question": "Approximately how long does the fabric rustling last?",
    "answer": "About 2 seconds, from 00:23 to 00:25.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01214.mp4",
    "question_id": "01214_8",
    "clip_path": "clips/01214/01214__0022500_0025500.mp4"
  },
  {
    "timestamp": "00:28 - 00:32",
    "context": "[00:28 - 00:32] From the driver's seat, the man offers further assistance, mentioning her clothing to ensure it's clear of the door. The user responds, \"Okay, thank you.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the driver mention the user's clothing?",
    "answer": "To ensure her clothing was clear of the door.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01214.mp4",
    "question_id": "01214_9",
    "clip_path": "clips/01214/01214__0027500_0032500.mp4"
  },
  {
    "timestamp": "00:32 - 00:36",
    "context": "[00:32 - 00:36] The user says, \"Mmm, okay,\" immediately followed by a loud, definitive thud as the driver closes the car door from the outside, sealing the car's interior.",
    "question_type": "Sound Characteristics",
    "question": "How is the door-closing sound described?",
    "answer": "A loud, definitive thud.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01214.mp4",
    "question_id": "01214_10",
    "clip_path": "clips/01214/01214__0031500_0036500.mp4"
  },
  {
    "timestamp": "00:32 - 00:36",
    "context": "[00:32 - 00:36] The driver closes the car door from the outside, producing a loud thud and sealing the car's interior.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the door-closing sound originate relative to the camera?",
    "answer": "From outside the car.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01214.mp4",
    "question_id": "01214_11",
    "clip_path": "clips/01214/01214__0031500_0036500.mp4"
  },
  {
    "timestamp": "00:39 - 00:49",
    "context": "[00:39 - 00:49] A clear, synthesized female voice from the car's GPS in the front provides directions: turn left, far left lane; in 100 meters speed and red light cameras; speed limit 50. This marks the beginning of the journey.",
    "question_type": "Cross-Modal Reasoning",
    "question": "What audio event marked the beginning of the car journey?",
    "answer": "The car’s GPS navigation system voice providing directions.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01214.mp4",
    "question_id": "01214_12",
    "clip_path": "clips/01214/01214__0038500_0049500.mp4"
  },
  {
    "timestamp": "00:52 - 00:57",
    "context": "[00:52 - 00:57] A distinct, melodic tune is heard from outside the car. The user asks about the source, and the driver identifies it as a water sprinkler truck.",
    "question_type": "Sound Source Identification",
    "question": "What was the source of the distinct melodic tune heard outside the car?",
    "answer": "A water sprinkler truck.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01214.mp4",
    "question_id": "01214_13",
    "clip_path": "clips/01214/01214__0051500_0057500.mp4"
  },
  {
    "timestamp": "00:52 - 00:57",
    "context": "[00:52 - 00:57] Hearing a distinct, melodic tune from outside, the user asks, \"What kind of car is this that's singing?\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the user ask, \"What kind of car is this that's singing?\"",
    "answer": "She heard a distinct melodic tune outside and wanted to know its source.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01214.mp4",
    "question_id": "01214_14",
    "clip_path": "clips/01214/01214__0051500_0057500.mp4"
  },
  {
    "timestamp": "00:39 - 00:49; 00:57 - 00:59",
    "context": "[00:39 - 00:49] The GPS gives a set of directions. [00:57 - 00:59] The GPS gives another instruction: \"Turn left, enter Ningjin Road.\"",
    "question_type": "Counting",
    "question": "How many separate GPS instruction segments are heard in the clip?",
    "answer": "Two.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01214.mp4",
    "question_id": "01214_15",
    "clip_path": "clips/01214/01214__0038500_0059500.mp4"
  },
  {
    "timestamp": "00:15 - 00:36",
    "context": "[00:15 - 00:18] Soft, metallic click as the driver engages the rear passenger door handle. [00:18 - 00:22] Solid thudding sound as the door swings open. [00:32 - 00:36] Loud, definitive thud as the door is closed from outside.",
    "question_type": "Counting",
    "question": "How many distinct car-door-related sounds are heard between 00:15 and 00:36?",
    "answer": "Three: the soft metallic click of the handle, the solid thud as it swings open, and the loud thud when it closes.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01214.mp4",
    "question_id": "01214_16",
    "clip_path": "clips/01214/01214__0014500_0036500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "[00:00 - 00:06] From a first-person view, a hand carries a transparent plastic bag with food while walking. A clear, female voice from the camera's position says in Mandarin: “今天没有买到那个乌米饭团，但是买了麻团和豆浆.” The monologue provides context for carrying the food.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why does the speaker describe not getting a black rice ball but buying a sesame ball and soy milk?",
    "answer": "To explain the contents of the food bag she is carrying.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01216.mp4",
    "question_id": "01216_1",
    "clip_path": "clips/01216/01216__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "[00:00 - 00:06] A transparent plastic bag is being carried while walking. The plastic bag makes a soft, intermittent rustling sound as it moves.",
    "question_type": "Sound Source Identification",
    "question": "What generated the rustling sound heard during this segment?",
    "answer": "The transparent plastic bag being carried.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01216.mp4",
    "question_id": "01216_2",
    "clip_path": "clips/01216/01216__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "[00:00 - 00:06] The plastic bag makes a soft, intermittent rustling sound as it moves.",
    "question_type": "Sound Characteristics",
    "question": "How is the rustling sound of the plastic bag acoustically described?",
    "answer": "It is soft and intermittent.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01216.mp4",
    "question_id": "01216_3",
    "clip_path": "clips/01216/01216__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "[00:00 - 00:06] A clear, female voice, originating from the camera's position, speaks in Mandarin.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where does the female voice originate relative to the camera?",
    "answer": "From the camera’s position (co-located with the camera, very close).",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01216.mp4",
    "question_id": "01216_4",
    "clip_path": "clips/01216/01216__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "[00:00 - 00:06] The speaker talks in Mandarin while carrying the food bag.",
    "question_type": "Temporal Information",
    "question": "During what time interval is the Mandarin speech heard?",
    "answer": "Between 00:00 and 00:06.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01216.mp4",
    "question_id": "01216_5",
    "clip_path": "clips/01216/01216__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "[00:00 - 00:06] Two audible elements are described: the speaker’s clear Mandarin voice and the bag’s soft, intermittent rustling.",
    "question_type": "Counting",
    "question": "How many distinct sound sources are explicitly described in this segment?",
    "answer": "Two: the female speech and the plastic bag’s rustling.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01216.mp4",
    "question_id": "01216_6",
    "clip_path": "clips/01216/01216__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:01 - 00:08",
    "context": "An elderly vendor, about 2 meters away, confirms an order; the camera holder corrects it to include soy milk; the vendor repeats the corrected order and says fried dough sticks are sold out.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From what approximate distance did the elderly vendor speak to the camera holder during the initial order confirmation?",
    "answer": "About 2 meters away.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01219.mp4",
    "question_id": "01219_1",
    "clip_path": "clips/01219/01219__0000500_0008500.mp4"
  },
  {
    "timestamp": "00:01 - 00:08",
    "context": "The vendor says, \"一个麻团, 一个油条\" (one sesame ball, one fried dough stick).",
    "question_type": "Counting",
    "question": "How many items did the vendor list in her first order confirmation?",
    "answer": "Two items—one sesame ball and one fried dough stick.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01219.mp4",
    "question_id": "01219_2",
    "clip_path": "clips/01219/01219__0000500_0008500.mp4"
  },
  {
    "timestamp": "00:01 - 00:08",
    "context": "The camera holder corrects the vendor, saying \"一个豆浆\" (one soy milk), and the vendor repeats the corrected order.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the camera holder say \"一个豆浆\" during the exchange?",
    "answer": "To correct the vendor’s order because they wanted soy milk instead of a fried dough stick.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01219.mp4",
    "question_id": "01219_3",
    "clip_path": "clips/01219/01219__0000500_0008500.mp4"
  },
  {
    "timestamp": "00:09 - 00:20",
    "context": "The vendor, holding a plastic bag with soy milk, states the price; the bag’s rustling is audible.",
    "question_type": "Sound Source Identification",
    "question": "What object produced the rustling sound heard while the price was stated?",
    "answer": "The plastic bag the vendor was holding.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01219.mp4",
    "question_id": "01219_4",
    "clip_path": "clips/01219/01219__0008500_0020500.mp4"
  },
  {
    "timestamp": "00:09 - 00:20",
    "context": "The vendor states the price from less than a meter away while handing over the order.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From what distance did the vendor state the price \"4块钱\"?",
    "answer": "From less than a meter away.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01219.mp4",
    "question_id": "01219_5",
    "clip_path": "clips/01219/01219__0008500_0020500.mp4"
  },
  {
    "timestamp": "00:09 - 00:20",
    "context": "The vendor quotes the price and the plastic bag rustles audibly during the handover.",
    "question_type": "Temporal Information",
    "question": "During which time interval were the price announcement and the bag’s rustling audible?",
    "answer": "00:09 - 00:20.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01219.mp4",
    "question_id": "01219_6",
    "clip_path": "clips/01219/01219__0008500_0020500.mp4"
  },
  {
    "timestamp": "00:23 - 00:33",
    "context": "A second, younger female vendor in a red jacket and black apron explains they don’t close early but rice balls sell out quickly.",
    "question_type": "Sound Source Identification",
    "question": "Who explained that they don't close early but that rice balls sell out quickly?",
    "answer": "A second, younger female vendor in a red jacket and black apron.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01219.mp4",
    "question_id": "01219_7",
    "clip_path": "clips/01219/01219__0022500_0033500.mp4"
  },
  {
    "timestamp": "00:34 - 00:52",
    "context": "Vendors explain that sweet-flavored rice balls have been sold out; the camera holder decides to return the next day.",
    "question_type": "Inferential & Contextual Causality",
    "question": "What was the reason the camera holder decided to return the next day?",
    "answer": "Because the rice balls—especially the sweet-flavored ones—were already sold out.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01219.mp4",
    "question_id": "01219_8",
    "clip_path": "clips/01219/01219__0033500_0052500.mp4"
  },
  {
    "timestamp": "00:52 - 01:09",
    "context": "The camera holder guides an elderly woman through a 4 yuan mobile payment: \"按个4...等我一下...现在自己按密码\".",
    "question_type": "Counting",
    "question": "How many distinct instructions did the camera holder give while helping the elderly customer pay?",
    "answer": "Three: press 4, wait a moment, then enter the password yourself.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01219.mp4",
    "question_id": "01219_9",
    "clip_path": "clips/01219/01219__0051500_0069500.mp4"
  },
  {
    "timestamp": "00:52 - 01:09",
    "context": "After the payment is complete, the customer says thank you; the camera holder replies \"不谢\".",
    "question_type": "Sound Source Identification",
    "question": "Who expressed thanks after the mobile payment was completed?",
    "answer": "The elderly woman customer.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01219.mp4",
    "question_id": "01219_10",
    "clip_path": "clips/01219/01219__0051500_0069500.mp4"
  },
  {
    "timestamp": "01:10 - 01:16",
    "context": "As the vendor ties the plastic bag, a distinct, sharp rustling sound is heard at close range.",
    "question_type": "Sound Characteristics",
    "question": "What was the character of the sound produced when the vendor tied the bag?",
    "answer": "A distinct, sharp rustling sound.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01219.mp4",
    "question_id": "01219_11",
    "clip_path": "clips/01219/01219__0069500_0076255.mp4"
  },
  {
    "timestamp": "01:10 - 01:16",
    "context": "The camera holder takes the bag while the vendor ties it, producing close-range rustling.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where, relative to the camera, did the bag-tying rustling originate?",
    "answer": "At close range, right near the camera as the vendor tied the bag.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01219.mp4",
    "question_id": "01219_12",
    "clip_path": "clips/01219/01219__0069500_0076255.mp4"
  },
  {
    "timestamp": "00:00 - 00:12",
    "context": "[00:00-00:12] The individual taps a white cane on the pavement in a rhythmic, continuous manner. They say in Mandarin: “那个盲道的引导…让我感觉有点没有安全感.” The caption explains this gives causal context: the poor quality of the path sensed by the cane drives the insecurity.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the speaker say that the environment makes them feel a bit unsafe?",
    "answer": "Because the guidance from the tactile paving—and the poor path quality they are sensing with the cane—makes them feel insecure.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01222.mp4",
    "question_id": "01222_1",
    "clip_path": "clips/01222/01222__0000000_0012067.mp4"
  },
  {
    "timestamp": "00:00 - 00:12",
    "context": "[00:00-00:12] Sharp, distinct taps are heard as a white cane contacts the pavement.",
    "question_type": "Sound Source Identification",
    "question": "What generated the sharp, distinct tapping sounds?",
    "answer": "A white cane tapping the pavement.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01222.mp4",
    "question_id": "01222_2",
    "clip_path": "clips/01222/01222__0000000_0012067.mp4"
  },
  {
    "timestamp": "00:00 - 00:12",
    "context": "[00:00-00:12] The cane produces sharp, distinct taps in a rhythmic, continuous manner.",
    "question_type": "Sound Characteristics",
    "question": "How are the acoustic qualities of the cane taps described?",
    "answer": "They are sharp, distinct, rhythmic, and continuous.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01222.mp4",
    "question_id": "01222_3",
    "clip_path": "clips/01222/01222__0000000_0012067.mp4"
  },
  {
    "timestamp": "00:00 - 00:12",
    "context": "[00:00-00:12] The tapping is said to originate directly in front of the camera.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where do the cane taps originate relative to the camera?",
    "answer": "Directly in front of the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01222.mp4",
    "question_id": "01222_4",
    "clip_path": "clips/01222/01222__0000000_0012067.mp4"
  },
  {
    "timestamp": "00:00 - 00:12",
    "context": "[00:00-00:12] The tapping occurs in a rhythmic, continuous manner throughout the clip.",
    "question_type": "Temporal Information",
    "question": "Is the cane tapping brief or sustained during this interval?",
    "answer": "It is sustained and continuous throughout 00:00–00:12.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01222.mp4",
    "question_id": "01222_5",
    "clip_path": "clips/01222/01222__0000000_0012067.mp4"
  },
  {
    "timestamp": "00:00 - 00:12",
    "context": "[Speech] First the speaker says, “那个盲道的引导…”, and after a brief pause adds, “…让我感觉有点没有安全感.”",
    "question_type": "Temporal Information",
    "question": "What is the timing relationship between the two Mandarin statements?",
    "answer": "They are separated by a brief pause before the second statement.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01222.mp4",
    "question_id": "01222_6",
    "clip_path": "clips/01222/01222__0000000_0012067.mp4"
  },
  {
    "timestamp": "00:00 - 00:12",
    "context": "[Speech] Two separate Mandarin statements are spoken: one about the tactile paving’s guidance and another stating it feels unsafe.",
    "question_type": "Counting",
    "question": "How many separate Mandarin statements does the speaker make?",
    "answer": "Two.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01222.mp4",
    "question_id": "01222_7",
    "clip_path": "clips/01222/01222__0000000_0012067.mp4"
  },
  {
    "timestamp": "00:00 - 00:12",
    "context": "[Audio-Visual] The cane taps are used to probe the walking surface; visuals note an uneven, cracked sidewalk.",
    "question_type": "Cross-Modal Reasoning",
    "question": "Based on the tapping sounds and the visuals, what is the cane probing?",
    "answer": "The uneven and cracked sidewalk surface.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01222.mp4",
    "question_id": "01222_8",
    "clip_path": "clips/01222/01222__0000000_0012067.mp4"
  },
  {
    "timestamp": "00:00 - 00:08",
    "context": "[00:00 - 00:08] A woman in a wheelchair rolls through a modern mall corridor while speaking. A faint, continuous whirring accompanies her speech as the wheels move on the polished floor.",
    "question_type": "Sound Source Identification",
    "question": "What generated the faint, continuous whirring sound that accompanies the woman's speech?",
    "answer": "The wheelchair's wheels rolling on the polished floor.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01232.mp4",
    "question_id": "01232_1",
    "clip_path": "clips/01232/01232__0000000_0008500.mp4"
  },
  {
    "timestamp": "00:00 - 00:08",
    "context": "[00:00 - 00:08] The woman speaks while moving through a spacious mall corridor; her voice slightly echoes.",
    "question_type": "Sound Characteristics",
    "question": "What were the volume and reverberation characteristics of the woman's speech?",
    "answer": "Her speech was at a moderate volume and echoed slightly.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01232.mp4",
    "question_id": "01232_2",
    "clip_path": "clips/01232/01232__0000000_0008500.mp4"
  },
  {
    "timestamp": "00:00 - 00:08",
    "context": "[00:00 - 00:08] The woman's voice is heard as she rolls ahead; it comes from the front and slightly to the right of the camera.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From where relative to the camera did the woman's voice originate?",
    "answer": "From the front and slightly to the right of the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01232.mp4",
    "question_id": "01232_3",
    "clip_path": "clips/01232/01232__0000000_0008500.mp4"
  },
  {
    "timestamp": "00:00 - 00:08",
    "context": "[00:00 - 00:08] A faint whirring from the wheelchair wheels is heard throughout her speech.",
    "question_type": "Temporal Information",
    "question": "Was the wheelchair-wheel whirring brief or continuous during 00:00–00:08?",
    "answer": "It was continuous throughout the 00:00–00:08 interval.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01232.mp4",
    "question_id": "01232_4",
    "clip_path": "clips/01232/01232__0000000_0008500.mp4"
  },
  {
    "timestamp": "00:00 - 00:08",
    "context": "[00:00 - 00:08] The woman's voice echoes slightly while she speaks in a modern mall corridor.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the woman's voice echo slightly?",
    "answer": "Because she was in a spacious mall corridor that caused slight reverberation.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01232.mp4",
    "question_id": "01232_5",
    "clip_path": "clips/01232/01232__0000000_0008500.mp4"
  },
  {
    "timestamp": "00:08 - 00:33",
    "context": "[00:08] They arrive at the restroom entrance. The camera-wearer uses their left hand to explore illuminated, raised pictograms on a marble wall, producing rubbing and tapping sounds.",
    "question_type": "Sound Source Identification",
    "question": "What produced the soft rubbing and tapping sounds at the restroom entrance?",
    "answer": "The camera-wearer rubbing and tapping the raised pictograms on the marble wall with their left hand.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01232.mp4",
    "question_id": "01232_6",
    "clip_path": "clips/01232/01232__0007500_0033500.mp4"
  },
  {
    "timestamp": "00:08 - 00:33",
    "context": "[00:08 - 00:33] As the camera-wearer feels the signs, the audio captures rubbing and tapping noises.",
    "question_type": "Sound Characteristics",
    "question": "How are the rubbing and tapping sounds described?",
    "answer": "They are soft rubbing and tapping sounds.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01232.mp4",
    "question_id": "01232_7",
    "clip_path": "clips/01232/01232__0007500_0033500.mp4"
  },
  {
    "timestamp": "00:08 - 00:33",
    "context": "[00:08 - 00:33] A multi-person conversation starts about the abstract restroom signs. The camera-wearer asks, “哎厕所厕所这这是啥” while exploring by touch; they are identified as visually impaired.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the camera-wearer ask, “哎厕所厕所这这是啥” (“Hey, restroom, what is this?”)?",
    "answer": "Because, being visually impaired and finding the abstract signs confusing, they were trying to identify the restroom sign by touch.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01232.mp4",
    "question_id": "01232_8",
    "clip_path": "clips/01232/01232__0007500_0033500.mp4"
  },
  {
    "timestamp": "00:08 - 00:33",
    "context": "[00:08 - 00:33] A multi-person conversation ensues about interpreting the signs and concludes with the camera-wearer saying, “对啊了解了.”",
    "question_type": "Temporal Information",
    "question": "What was the duration of the multi-person conversation about the restroom signs?",
    "answer": "Approximately 25 seconds, from 00:08 to 00:33.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01232.mp4",
    "question_id": "01232_9",
    "clip_path": "clips/01232/01232__0007500_0033500.mp4"
  },
  {
    "timestamp": "00:20 - 00:33",
    "context": "[During the conversation] The camera-wearer confirms understanding through touch, describing the female sign as “像把伞一样” (like an umbrella) and the male sign as “一把扇子” (a fan).",
    "question_type": "Counting",
    "question": "How many distinct comparative descriptions did the camera-wearer give for the restroom signs?",
    "answer": "Two: the female sign “like an umbrella” and the male sign “a fan.”",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01232.mp4",
    "question_id": "01232_10",
    "clip_path": "clips/01232/01232__0019500_0033500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "Hands hold a white alpaca; a small red bell on its collar jingles faintly with movement.",
    "question_type": "Sound Source Identification",
    "question": "What generated the faint, high-pitched jingle during this segment?",
    "answer": "The small red bell on the alpaca's collar.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01242.mp4",
    "question_id": "01242_1",
    "clip_path": "clips/01242/01242__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "The bell on the alpaca’s collar produces a faint, high-pitched jingle corresponding to movement.",
    "question_type": "Sound Characteristics",
    "question": "What were the volume and pitch characteristics of the bell's jingle?",
    "answer": "It was faint and high-pitched.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01242.mp4",
    "question_id": "01242_2",
    "clip_path": "clips/01242/01242__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "As the alpaca is held and stroked, its collar bell jingles in sync with the movement.",
    "question_type": "Temporal Information",
    "question": "Did the bell's jingle occur as a single event or correspond with movement over time?",
    "answer": "It corresponded with the alpaca's movement while being held and stroked.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01242.mp4",
    "question_id": "01242_3",
    "clip_path": "clips/01242/01242__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "Camera-wearer asks playfully, “是不是被我们玩蒙了?” Immediately after, the woman in the background laughs (“哈哈哈”).",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the woman in the background laugh?",
    "answer": "She laughed in response to the camera-wearer’s playful question about the alpaca being dazed.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01242.mp4",
    "question_id": "01242_4",
    "clip_path": "clips/01242/01242__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "A woman in the background speaks clearly: “这个这个爱叫这个”.",
    "question_type": "Sound Source Identification",
    "question": "Who said “这个这个爱叫这个”?",
    "answer": "The woman in the background.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01242.mp4",
    "question_id": "01242_5",
    "clip_path": "clips/01242/01242__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "The camera-wearer adds: “奶茶这个”, identifying the alpaca by name.",
    "question_type": "Sound Source Identification",
    "question": "Who said “奶茶这个” to identify the alpaca?",
    "answer": "The camera-wearer.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01242.mp4",
    "question_id": "01242_6",
    "clip_path": "clips/01242/01242__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "After the woman’s comment, the camera-wearer says: “奶茶这个”, identifying the alpaca.",
    "question_type": "Inferential & Contextual Causality",
    "question": "What was the likely purpose of the camera-wearer saying “奶茶这个”?",
    "answer": "To identify the alpaca by name (“Milk Tea”).",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01242.mp4",
    "question_id": "01242_7",
    "clip_path": "clips/01242/01242__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "The woman is described as being in the background when she speaks.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Did the woman's speech originate from the background or near the camera?",
    "answer": "From the background.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01242.mp4",
    "question_id": "01242_8",
    "clip_path": "clips/01242/01242__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "The woman in the background speaks; the camera-wearer also speaks twice.",
    "question_type": "Counting",
    "question": "How many people spoke during this segment?",
    "answer": "Two—the woman in the background and the camera-wearer.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01242.mp4",
    "question_id": "01242_9",
    "clip_path": "clips/01242/01242__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "The camera-wearer says “奶茶这个” and then asks “是不是被我们玩蒙了?”",
    "question_type": "Counting",
    "question": "How many times did the camera-wearer speak?",
    "answer": "Twice.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01242.mp4",
    "question_id": "01242_10",
    "clip_path": "clips/01242/01242__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "An alpaca eats a piece of carrot from the user's hand, producing wet, crunchy chewing sounds right in front of the camera. The alpaca accidentally nibbles the user's fingers, and she says in a high-pitched, slightly alarmed tone, \"Don't bite, don't bite me, sis.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the user say \"Don't bite, don't bite me, sis\" in a high-pitched, slightly alarmed tone?",
    "answer": "Because the alpaca's eagerness led it to accidentally nibble her fingers.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01243.mp4",
    "question_id": "01243_1",
    "clip_path": "clips/01243/01243__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "An alpaca eats a piece of carrot directly from the user's hand, producing wet, crunchy chewing sounds.",
    "question_type": "Sound Source Identification",
    "question": "What generated the wet, crunchy chewing sounds at the start?",
    "answer": "An alpaca chewing a piece of carrot from the user's hand.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01243.mp4",
    "question_id": "01243_2",
    "clip_path": "clips/01243/01243__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "Wet, crunchy chewing sounds occur as the alpaca eats right in front of the camera.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where were the chewing sounds located relative to the camera?",
    "answer": "Directly in front of the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01243.mp4",
    "question_id": "01243_3",
    "clip_path": "clips/01243/01243__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "A friend in the background laughs and advises from approximately 3-4 meters away.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "What was the estimated distance of the friend's laughter and advice from the camera?",
    "answer": "Approximately 3–4 meters away, in the background.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01243.mp4",
    "question_id": "01243_4",
    "clip_path": "clips/01243/01243__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:06 - 00:10",
    "context": "As multiple alpacas crowd closer, the user exclaims, \"Don't bite, don't bite, I'm begging you!\" followed by a mix of laughter and a pained cry, \"It hurts so much!\"",
    "question_type": "Sound Characteristics",
    "question": "What was the acoustic character of the user's reaction when the alpacas crowded closer?",
    "answer": "A mix of laughter and a pained cry.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01243.mp4",
    "question_id": "01243_5",
    "clip_path": "clips/01243/01243__0005500_0010500.mp4"
  },
  {
    "timestamp": "00:10 - 00:15",
    "context": "To resolve the situation and appease the competing alpacas, the user decides to give them the rest of the food and says, \"Here, take it, take it...\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the user decide to give the rest of the food to the alpacas?",
    "answer": "To resolve the situation and appease the competing alpacas.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01243.mp4",
    "question_id": "01243_6",
    "clip_path": "clips/01243/01243__0009500_0015500.mp4"
  },
  {
    "timestamp": "00:10 - 00:15",
    "context": "She says, \"Here, take it, take it, 'Milk Tea.' Or here, take the 'Candy.' Here you go, here you go,\" indicating playful naming of the treats.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the user refer to the treats as 'Milk Tea' and 'Candy' while offering them?",
    "answer": "She was playfully naming the treats to help manage the feeding.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01243.mp4",
    "question_id": "01243_7",
    "clip_path": "clips/01243/01243__0009500_0015500.mp4"
  },
  {
    "timestamp": "00:15 - 00:24",
    "context": "Multiple alpacas swarm her hand, eating with loud, rapid, wet crunching and smacking sounds.",
    "question_type": "Sound Characteristics",
    "question": "How are the feeding sounds described during the swarm?",
    "answer": "Loud, rapid, and wet crunching and smacking.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01243.mp4",
    "question_id": "01243_8",
    "clip_path": "clips/01243/01243__0014500_0024500.mp4"
  },
  {
    "timestamp": "00:15 - 00:24",
    "context": "The alpacas continue eating vigorously for several seconds.",
    "question_type": "Temporal Information",
    "question": "Were the feeding sounds brief or sustained during this period?",
    "answer": "Sustained for several seconds.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01243.mp4",
    "question_id": "01243_9",
    "clip_path": "clips/01243/01243__0014500_0024500.mp4"
  },
  {
    "timestamp": "00:15 - 00:24",
    "context": "The user exclaims, \"Aiyo, aiyo, aiyo, aiyo, aiyo\" in surprise.",
    "question_type": "Counting",
    "question": "How many times did the user exclaim \"Aiyo\"?",
    "answer": "Five times.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01243.mp4",
    "question_id": "01243_10",
    "clip_path": "clips/01243/01243__0014500_0024500.mp4"
  },
  {
    "timestamp": "00:27 - 00:29",
    "context": "After all the food is gone, one alpaca continues to gently nibble at the user's empty hand. The user says, \"He's biting my hand.\"",
    "question_type": "Counting",
    "question": "How many alpacas continued to nibble the user's hand after the food was gone?",
    "answer": "One alpaca.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01243.mp4",
    "question_id": "01243_11",
    "clip_path": "clips/01243/01243__0026500_0029500.mp4"
  },
  {
    "timestamp": "00:27 - 00:29",
    "context": "After the food is gone, one alpaca continues to nibble the user's empty hand.",
    "question_type": "Temporal Information",
    "question": "Did the alpaca's gentle nibbling occur before or after all the food was gone?",
    "answer": "After the food was gone.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01243.mp4",
    "question_id": "01243_12",
    "clip_path": "clips/01243/01243__0026500_0029500.mp4"
  },
  {
    "timestamp": "00:27 - 00:29",
    "context": "She moves her hand to gently stroke the alpaca's fluffy neck, accompanied by a soft, affectionate \"Ah\" sound.",
    "question_type": "Sound Source Identification",
    "question": "Who made the soft, affectionate \"Ah\" sound at the end?",
    "answer": "The user.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01243.mp4",
    "question_id": "01243_13",
    "clip_path": "clips/01243/01243__0026500_0029500.mp4"
  },
  {
    "timestamp": "00:27 - 00:29",
    "context": "After the food is gone and one alpaca keeps nibbling her empty hand, she immediately moves her hand from a feeding position to gently stroke the alpaca's neck.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Based on the preceding events, why did the user shift from feeding to gently stroking the alpaca's neck?",
    "answer": "Because the food was gone and an alpaca was still nibbling her empty hand, prompting her to switch to petting.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01243.mp4",
    "question_id": "01243_14",
    "clip_path": "clips/01243/01243__0026500_0029500.mp4"
  },
  {
    "timestamp": "00:00 - 00:10",
    "context": "Standing on an outdoor street, the camera wearer holds a small brown paper gift bag and a white cane and speaks directly to the camera: \"Today, I came out to mail a cup to my fan. I hope that after receiving this cup, they will be happy for the whole year. Drink more hot water.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Based on the statement, why did the speaker come out today?",
    "answer": "To mail a cup to their fan.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01248.mp4",
    "question_id": "01248_1",
    "clip_path": "clips/01248/01248__0000000_0010433.mp4"
  },
  {
    "timestamp": "00:00 - 00:10",
    "context": "The camera wearer speaks directly to the camera in a clear, conversational voice while holding a small brown paper gift bag and a white cane.",
    "question_type": "Sound Source Identification",
    "question": "Who produced the clear, conversational monologue heard in this segment?",
    "answer": "The camera wearer.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01248.mp4",
    "question_id": "01248_2",
    "clip_path": "clips/01248/01248__0000000_0010433.mp4"
  },
  {
    "timestamp": "00:00 - 00:10",
    "context": "They speak directly to the camera in a clear, conversational voice, stating their purpose and well-wishes.",
    "question_type": "Sound Characteristics",
    "question": "How is the speaker's voice characterized during the monologue?",
    "answer": "It is clear and conversational.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01248.mp4",
    "question_id": "01248_3",
    "clip_path": "clips/01248/01248__0000000_0010433.mp4"
  },
  {
    "timestamp": "00:00 - 00:10",
    "context": "While holding the bag and a white cane, they speak directly to the camera.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where relative to the camera does the speech originate?",
    "answer": "Directly in front of and very close to the camera (from the camera wearer).",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01248.mp4",
    "question_id": "01248_4",
    "clip_path": "clips/01248/01248__0000000_0010433.mp4"
  },
  {
    "timestamp": "00:00 - 00:10",
    "context": "A single, uninterrupted monologue is spoken: \"Today, I came out to mail a cup to my fan. I hope that after receiving this cup, they will be happy for the whole year. Drink more hot water.\"",
    "question_type": "Temporal Information",
    "question": "When does the monologue occur, and is it continuous?",
    "answer": "It occurs from 00:00 to 00:10 and is continuous throughout the interval.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01248.mp4",
    "question_id": "01248_5",
    "clip_path": "clips/01248/01248__0000000_0010433.mp4"
  },
  {
    "timestamp": "00:00 - 00:10",
    "context": "Quoted speech: \"Today, I came out to mail a cup to my fan. I hope that after receiving this cup, they will be happy for the whole year. Drink more hot water.\"",
    "question_type": "Counting",
    "question": "How many sentences are in the quoted monologue?",
    "answer": "Three.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01248.mp4",
    "question_id": "01248_6",
    "clip_path": "clips/01248/01248__0000000_0010433.mp4"
  },
  {
    "timestamp": "00:00 - 00:10",
    "context": "The speaker holds a small brown paper gift bag and says they came out to mail a cup to a fan.",
    "question_type": "Cross-Modal Reasoning",
    "question": "Based on the spoken intent and the visible gift bag, what is likely inside the bag?",
    "answer": "A cup intended as a gift for a fan.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01248.mp4",
    "question_id": "01248_7",
    "clip_path": "clips/01248/01248__0000000_0010433.mp4"
  },
  {
    "timestamp": "00:00 - 00:10",
    "context": "The speaker says, \"I hope that after receiving this cup, they will be happy for the whole year.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "According to the monologue, what event is expected to lead to the fan's happiness?",
    "answer": "Receiving the cup.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01248.mp4",
    "question_id": "01248_8",
    "clip_path": "clips/01248/01248__0000000_0010433.mp4"
  },
  {
    "timestamp": "00:00 - 00:01",
    "context": "A loud, high-pitched electronic chime sounds from the smartphone held in the user's hand, indicating a successful mobile payment. Immediately after the chime, the user says '好了' ('It's done') to confirm the transaction is complete.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the user say '好了' immediately after the chime?",
    "answer": "Because the chime indicated the mobile payment succeeded, and she was confirming the transaction was complete.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01249.mp4",
    "question_id": "01249_1",
    "clip_path": "clips/01249/01249__0000000_0001500.mp4"
  },
  {
    "timestamp": "00:02 - 00:04",
    "context": "An older man in a red jacket, standing just outside the shop entrance, asks the user, '哦听到了吧' ('Oh, you heard it, right?'), referring to the payment confirmation sound. The user replies that she heard it.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the older man ask '哦听到了吧' ('Oh, you heard it, right?')?",
    "answer": "He was checking whether she heard the payment confirmation chime.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01249.mp4",
    "question_id": "01249_2",
    "clip_path": "clips/01249/01249__0001500_0004500.mp4"
  },
  {
    "timestamp": "00:00 - 00:01",
    "context": "A loud, high-pitched electronic chime sounds from the smartphone held in the user's hand.",
    "question_type": "Sound Source Identification",
    "question": "What device generated the electronic chime?",
    "answer": "The smartphone held in the user's hand.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01249.mp4",
    "question_id": "01249_3",
    "clip_path": "clips/01249/01249__0000000_0001500.mp4"
  },
  {
    "timestamp": "00:00 - 00:01",
    "context": "A loud, high-pitched electronic chime sounds from the smartphone held in the user's hand.",
    "question_type": "Sound Characteristics",
    "question": "What were the volume and pitch characteristics of the chime?",
    "answer": "It was loud and high-pitched, an electronic chime.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01249.mp4",
    "question_id": "01249_4",
    "clip_path": "clips/01249/01249__0000000_0001500.mp4"
  },
  {
    "timestamp": "00:02 - 00:04",
    "context": "The older man stands just outside the shop entrance in front of the user and asks, '哦听到了吧' ('Oh, you heard it, right?').",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From where relative to the user did the older man's question originate?",
    "answer": "From in front of her, just outside the shop entrance.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01249.mp4",
    "question_id": "01249_5",
    "clip_path": "clips/01249/01249__0001500_0004500.mp4"
  },
  {
    "timestamp": "00:06 - 00:08",
    "context": "While walking away from the shop, the user says '拜拜' ('Bye-bye'). The man's voice is heard again from a short distance behind her, repeating '慢点' ('Take it slow').",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the man's repeated '慢点' come from relative to the user?",
    "answer": "From a short distance behind her.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01249.mp4",
    "question_id": "01249_6",
    "clip_path": "clips/01249/01249__0005500_0008500.mp4"
  },
  {
    "timestamp": "00:00 - 00:01",
    "context": "The electronic payment confirmation chime sounds, and immediately afterward the user says '好了' ('It's done').",
    "question_type": "Temporal Information",
    "question": "When did the chime occur, and how soon did the user respond verbally?",
    "answer": "The chime occurred between 00:00 and 00:01, and she responded immediately after.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01249.mp4",
    "question_id": "01249_7",
    "clip_path": "clips/01249/01249__0000000_0001500.mp4"
  },
  {
    "timestamp": "00:04 - 00:08",
    "context": "At 00:04-00:06, the man behind her says '好慢点, 拜拜' ('Okay, take care, bye-bye'). At 00:06-00:08, his voice is heard again from a short distance behind, repeating '慢点' ('Take it slow').",
    "question_type": "Counting",
    "question": "How many times did the older man say '慢点' ('Take it slow')?",
    "answer": "Twice—once at 00:04-00:06 and again at 00:06-00:08.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01249.mp4",
    "question_id": "01249_8",
    "clip_path": "clips/01249/01249__0003500_0008500.mp4"
  },
  {
    "timestamp": "00:06 - 00:08",
    "context": "The user acknowledges his concern with '嗯好好好' ('Yes, okay, okay, okay.').",
    "question_type": "Counting",
    "question": "How many times did the user say '好' ('okay') in '嗯好好好'?",
    "answer": "Three times.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01249.mp4",
    "question_id": "01249_9",
    "clip_path": "clips/01249/01249__0005500_0008500.mp4"
  },
  {
    "timestamp": "00:04 - 00:08",
    "context": "At 00:04-00:06, the man says '拜拜' ('Bye-bye'). At 00:06-00:08, the user says '拜拜' ('Bye-bye') in response.",
    "question_type": "Counting",
    "question": "How many '拜拜' ('bye-bye') utterances occur in the exchange?",
    "answer": "Two—one from the man and one from the user.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01249.mp4",
    "question_id": "01249_10",
    "clip_path": "clips/01249/01249__0003500_0008500.mp4"
  },
  {
    "timestamp": "00:00 - 00:02",
    "context": "[00:00 - 00:02] A soft, continuous electronic chime originating from the elevator in front, signaling its arrival.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the soft electronic chime occur at the start of the video?",
    "answer": "It signaled the elevator's arrival.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01250.mp4",
    "question_id": "01250_1",
    "clip_path": "clips/01250/01250__0000000_0002500.mp4"
  },
  {
    "timestamp": "00:00 - 00:02",
    "context": "[00:00 - 00:02] A soft, continuous electronic chime originating from the elevator in front.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the electronic chime originate relative to the camera?",
    "answer": "From the elevator in front of the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01250.mp4",
    "question_id": "01250_2",
    "clip_path": "clips/01250/01250__0000000_0002500.mp4"
  },
  {
    "timestamp": "00:00 - 00:02",
    "context": "[00:00 - 00:02] Coinciding with the end of the chime, the elevator doors slide open with a gentle, low-volume whooshing sound.",
    "question_type": "Sound Characteristics",
    "question": "What was the quality and volume of the elevator doors opening sound?",
    "answer": "A gentle, low-volume whooshing sound.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01250.mp4",
    "question_id": "01250_3",
    "clip_path": "clips/01250/01250__0000000_0002500.mp4"
  },
  {
    "timestamp": "00:00 - 00:02",
    "context": "[00:00 - 00:02] Coinciding with the end of the chime, the elevator doors slide open.",
    "question_type": "Temporal Information",
    "question": "When did the doors open relative to the chime?",
    "answer": "They opened coinciding with the end of the chime.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01250.mp4",
    "question_id": "01250_4",
    "clip_path": "clips/01250/01250__0000000_0002500.mp4"
  },
  {
    "timestamp": "00:02 - 00:06",
    "context": "[00:02 - 00:06] The user enters the elevator, producing a series of soft, muffled footsteps on the floor.",
    "question_type": "Sound Characteristics",
    "question": "How are the user's footsteps described as they enter the elevator?",
    "answer": "They are soft and muffled.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01250.mp4",
    "question_id": "01250_5",
    "clip_path": "clips/01250/01250__0001500_0006500.mp4"
  },
  {
    "timestamp": "00:02 - 00:06",
    "context": "[00:02 - 00:06] As they step inside, the elevator doors begin to slide shut, creating a quiet, mechanical hum.",
    "question_type": "Sound Source Identification",
    "question": "What generated the quiet mechanical hum during 00:02 - 00:06?",
    "answer": "The elevator doors sliding shut.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01250.mp4",
    "question_id": "01250_6",
    "clip_path": "clips/01250/01250__0001500_0006500.mp4"
  },
  {
    "timestamp": "00:02 - 00:06",
    "context": "[00:02 - 00:06] As they step inside, the elevator doors begin to slide shut.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Based on the preceding action, why did the elevator doors begin to slide shut?",
    "answer": "Because the user had stepped inside, prompting the doors to close.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01250.mp4",
    "question_id": "01250_7",
    "clip_path": "clips/01250/01250__0001500_0006500.mp4"
  },
  {
    "timestamp": "00:07 - 00:10",
    "context": "[00:07 - 00:10] The user presses the 5th floor button, producing a single, sharp, high-pitched electronic beep from the button panel.",
    "question_type": "Counting",
    "question": "How many electronic beeps occurred when the 5th floor button was pressed?",
    "answer": "One beep.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01250.mp4",
    "question_id": "01250_8",
    "clip_path": "clips/01250/01250__0006500_0010500.mp4"
  },
  {
    "timestamp": "00:07 - 00:10",
    "context": "[00:07 - 00:10] The user presses the 5th floor button; a sharp, high-pitched electronic beep is heard.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the high-pitched electronic beep sound between 00:07 and 00:10?",
    "answer": "It was triggered by the user pressing the 5th-floor button to select a destination.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01250.mp4",
    "question_id": "01250_9",
    "clip_path": "clips/01250/01250__0006500_0010500.mp4"
  },
  {
    "timestamp": "00:10 - 00:11",
    "context": "[00:10 - 00:11] The elevator doors fully close with a soft, final thud, sealing the cabin.",
    "question_type": "Sound Characteristics",
    "question": "What sound marked the doors fully closing at 00:10 - 00:11?",
    "answer": "A soft, final thud.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01250.mp4",
    "question_id": "01250_10",
    "clip_path": "clips/01250/01250__0009500_0011067.mp4"
  },
  {
    "timestamp": "00:10 - 00:11",
    "context": "[00:10 - 00:11] Immediately after the thud, a low-frequency, continuous mechanical hum begins.",
    "question_type": "Temporal Information",
    "question": "When did the low-frequency mechanical hum begin relative to the closing thud?",
    "answer": "Immediately after the thud.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01250.mp4",
    "question_id": "01250_11",
    "clip_path": "clips/01250/01250__0009500_0011067.mp4"
  },
  {
    "timestamp": "00:10 - 00:11",
    "context": "[00:10 - 00:11] A low-frequency, continuous mechanical hum begins, indicating that the elevator is now in motion, ascending to the selected floor.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the low-frequency continuous hum begin after the doors closed?",
    "answer": "Because the elevator started moving, ascending to the selected floor.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01250.mp4",
    "question_id": "01250_12",
    "clip_path": "clips/01250/01250__0009500_0011067.mp4"
  },
  {
    "timestamp": "00:00 - 00:11",
    "context": "[00:00 - 00:02] Doors open with a gentle, low-volume whooshing sound. [00:02 - 00:06] As they slide shut, a quiet, mechanical hum is created. [00:10 - 00:11] Doors fully close with a soft, final thud.",
    "question_type": "Counting",
    "question": "How many distinct door-related sounds are described from opening through fully closing?",
    "answer": "Three: the gentle, low-volume whooshing as they open, the quiet mechanical hum as they slide shut, and the soft, final thud when they fully close.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01250.mp4",
    "question_id": "01250_13",
    "clip_path": "clips/01250/01250__0000000_0011067.mp4"
  },
  {
    "timestamp": "00:06 - 00:09",
    "context": "[00:06] The camera operator asks, 'Okay, can you help me operate it?'. The man agrees, says 'Thanks,' turns, and walks back into the shop; his voice becomes slightly more distant as he moves away.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the man turn and walk back into the shop at 00:06 - 00:09?",
    "answer": "Because he agreed to help operate the transaction for her, so he went inside to handle it.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01252.mp4",
    "question_id": "01252_1",
    "clip_path": "clips/01252/01252__0005500_0009500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "[00:00 - 00:06] A conversation occurs between the camera operator and a man standing in a shop doorway, approximately 1 meter away.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where was the man located relative to the camera during the initial dialogue?",
    "answer": "In the shop doorway, about 1 meter away.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01252.mp4",
    "question_id": "01252_2",
    "clip_path": "clips/01252/01252__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "[00:00 - 00:06] The man asks, 'You have Alipay, right?' The exchange is described as clear and direct at a moderate volume.",
    "question_type": "Sound Characteristics",
    "question": "What were the volume and clarity of the initial conversation at 00:00 - 00:06?",
    "answer": "Moderate volume and clear, direct speech.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01252.mp4",
    "question_id": "01252_3",
    "clip_path": "clips/01252/01252__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:28 - 00:30",
    "context": "[00:28 - 00:30] The camera operator takes the smartphone; the soft rustle of her sweater sleeve is audible as she grips the phone.",
    "question_type": "Sound Source Identification",
    "question": "What generated the soft rustling sound heard at 00:28 - 00:30?",
    "answer": "The camera operator’s sweater sleeve rubbing as she gripped the phone.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01252.mp4",
    "question_id": "01252_4",
    "clip_path": "clips/01252/01252__0027500_0030500.mp4"
  },
  {
    "timestamp": "00:31 - 00:38",
    "context": "[00:31 - 00:38] While holding the phone, the camera operator asks, 'Is it done?' The man, now further away inside the shop, replies, 'Wait a moment, I need to print the receipt...'.",
    "question_type": "Temporal Information",
    "question": "When did the man state he needed to print the receipt?",
    "answer": "Between 00:31 and 00:38.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01252.mp4",
    "question_id": "01252_5",
    "clip_path": "clips/01252/01252__0030500_0038500.mp4"
  },
  {
    "timestamp": "00:06 - 00:38",
    "context": "[00:06] 'Okay, can you help me operate it?'; [00:28 - 00:30] She says 'Okay' after taking the phone; [00:31 - 00:38] She says, 'Okay, wrap it with a few more layers.'",
    "question_type": "Counting",
    "question": "How many times did the camera operator say 'Okay' throughout the clip?",
    "answer": "Three times.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01252.mp4",
    "question_id": "01252_6",
    "clip_path": "clips/01252/01252__0005500_0038500.mp4"
  },
  {
    "timestamp": "00:25 - 00:28",
    "context": "[00:25 - 00:28] The man returns from inside the shop, walks toward the camera, and says in a slightly raised tone, 'Hold the phone,' extending a smartphone toward the camera operator; this indicates he was unable to complete the transaction for her and is returning her device.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Based on the surrounding events, why did the man return and hand the smartphone back at 00:25 - 00:28?",
    "answer": "He couldn’t complete the transaction for her, so he returned her device.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01252.mp4",
    "question_id": "01252_7",
    "clip_path": "clips/01252/01252__0024500_0028500.mp4"
  },
  {
    "timestamp": "00:31 - 00:38",
    "context": "[00:31 - 00:38] The man replies from a distance, his voice slightly muffled, 'Wait a moment, I need to print the receipt...' while he is further away inside the shop.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From where relative to the camera did the man's 'Wait a moment...' reply originate?",
    "answer": "From further away inside the shop.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01252.mp4",
    "question_id": "01252_8",
    "clip_path": "clips/01252/01252__0030500_0038500.mp4"
  },
  {
    "timestamp": "00:25 - 00:28",
    "context": "[00:25 - 00:28] The man walks directly toward the camera and says, 'Hold the phone,' in a slightly raised, direct tone.",
    "question_type": "Sound Characteristics",
    "question": "What was the tone and volume of the man's speech when he said 'Hold the phone'?",
    "answer": "Slightly raised and direct.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01252.mp4",
    "question_id": "01252_9",
    "clip_path": "clips/01252/01252__0024500_0028500.mp4"
  },
  {
    "timestamp": "00:25 - 00:28",
    "context": "[00:25 - 00:28] After a period of waiting, the man returns from inside the shop and walks directly toward the camera.",
    "question_type": "Temporal Information",
    "question": "At what time did the man return from inside the shop and approach the camera?",
    "answer": "Between 00:25 and 00:28.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01252.mp4",
    "question_id": "01252_10",
    "clip_path": "clips/01252/01252__0024500_0028500.mp4"
  },
  {
    "timestamp": "00:00 - 00:05",
    "context": "From inside a shop directly in front of the camera, a series of five loud, sharp, and repetitive tearing sounds are heard, consistent with a tape gun sealing a cardboard box.",
    "question_type": "Counting",
    "question": "How many tearing sounds were heard at the start of the video?",
    "answer": "Five.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01254.mp4",
    "question_id": "01254_1",
    "clip_path": "clips/01254/01254__0000000_0005500.mp4"
  },
  {
    "timestamp": "00:00 - 00:05",
    "context": "A series of tearing sounds are described as loud, sharp, and repetitive.",
    "question_type": "Sound Characteristics",
    "question": "What were the acoustic qualities of the tearing sounds heard between 00:00 and 00:05?",
    "answer": "Loud, sharp, and repetitive tearing.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01254.mp4",
    "question_id": "01254_2",
    "clip_path": "clips/01254/01254__0000000_0005500.mp4"
  },
  {
    "timestamp": "00:00 - 00:05",
    "context": "The tearing sounds are consistent with a tape gun being used to seal a cardboard box by a person behind the counter.",
    "question_type": "Sound Source Identification",
    "question": "What object most likely generated the tearing sounds at 00:00–00:05?",
    "answer": "A tape gun used to seal a cardboard box.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01254.mp4",
    "question_id": "01254_3",
    "clip_path": "clips/01254/01254__0000000_0005500.mp4"
  },
  {
    "timestamp": "00:00 - 00:05",
    "context": "The sounds come from inside a shop directly in front of the camera.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where relative to the camera did the tearing sounds originate?",
    "answer": "From inside the shop directly in front of the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01254.mp4",
    "question_id": "01254_4",
    "clip_path": "clips/01254/01254__0000000_0005500.mp4"
  },
  {
    "timestamp": "00:05 - 00:09",
    "context": "A man in a black leather jacket hands a package to the camera holder and says, '这里玻璃...你检查一下子' ('This has glass... check it').",
    "question_type": "Inferential & Contextual Causality",
    "question": "What was the man's likely reason for saying 'This has glass... check it'?",
    "answer": "To warn the recipient about the fragile contents of the parcel.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01254.mp4",
    "question_id": "01254_5",
    "clip_path": "clips/01254/01254__0004500_0009500.mp4"
  },
  {
    "timestamp": "00:05 - 00:09",
    "context": "As he hands over the package, the man speaks in a clear, cautionary tone.",
    "question_type": "Sound Characteristics",
    "question": "What was the tone of the man's warning as he handed over the package?",
    "answer": "Clear and cautionary.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01254.mp4",
    "question_id": "01254_6",
    "clip_path": "clips/01254/01254__0004500_0009500.mp4"
  },
  {
    "timestamp": "00:05 - 00:09",
    "context": "The man exits the shop, walks directly toward the camera holder, hands them the package, and delivers the warning.",
    "question_type": "Temporal Information",
    "question": "When did the cautionary exchange about checking the glass occur?",
    "answer": "Between 00:05 and 00:09, as the man handed the package to the camera holder.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01254.mp4",
    "question_id": "01254_7",
    "clip_path": "clips/01254/01254__0004500_0009500.mp4"
  },
  {
    "timestamp": "00:09 - 00:11",
    "context": "The camera holder says '谢谢' ('Thank you'); the man replies '啊可以了' ('Ah, it's okay now'); the camera holder repeats '可以了' ('It's okay').",
    "question_type": "Counting",
    "question": "How many distinct utterances occurred in the closing exchange between 00:09 and 00:11?",
    "answer": "Three.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01254.mp4",
    "question_id": "01254_8",
    "clip_path": "clips/01254/01254__0008500_0011167.mp4"
  },
  {
    "timestamp": "00:09 - 00:11",
    "context": "While holding the box, the camera holder says '谢谢' ('Thank you') to the man.",
    "question_type": "Sound Source Identification",
    "question": "Who said '谢谢' during the closing exchange?",
    "answer": "The camera holder.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01254.mp4",
    "question_id": "01254_9",
    "clip_path": "clips/01254/01254__0008500_0011167.mp4"
  },
  {
    "timestamp": "00:09 - 00:11",
    "context": "Following the handoff and brief thanks/acknowledgment, the exchange ends.",
    "question_type": "Inferential & Contextual Causality",
    "question": "What did the final exchange ('谢谢' / '啊可以了' / '可以了') indicate about the interaction?",
    "answer": "That the package handoff was complete and the transaction concluded.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01254.mp4",
    "question_id": "01254_10",
    "clip_path": "clips/01254/01254__0008500_0011167.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "[00:00 - 00:03] A woman at the front right says '等一下哦' (Wait a moment) in a clear, close-up voice. As she speaks, a soft, brief metallic jingle is heard, likely from the wire mesh gate they are about to open to enter an animal enclosure.",
    "question_type": "Sound Source Identification",
    "question": "What was the likely source of the soft, brief metallic jingle heard as she said '等一下哦'?",
    "answer": "The wire mesh gate they were about to open to enter the animal enclosure.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01255.mp4",
    "question_id": "01255_1",
    "clip_path": "clips/01255/01255__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "[00:00 - 00:03] A person wearing a white puffer jacket, holding a microphone on a stick, speaks from the front right in a clear, close-up female voice, saying, '等一下哦' (Wait a moment).",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From which direction and approximate distance relative to the camera did the voice saying '等一下哦' originate?",
    "answer": "From the front right, at close range.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01255.mp4",
    "question_id": "01255_2",
    "clip_path": "clips/01255/01255__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "[00:00 - 00:03] As she speaks, a soft, brief metallic jingle is heard.",
    "question_type": "Sound Characteristics",
    "question": "What were the volume and duration characteristics of the metallic jingle?",
    "answer": "It was soft and brief.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01255.mp4",
    "question_id": "01255_3",
    "clip_path": "clips/01255/01255__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "[00:00 - 00:03] She says, '等一下哦' (Wait a moment) as they are about to open the wire mesh gate to enter an animal enclosure.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Based on the surrounding events, why did she say '等一下哦' (Wait a moment)?",
    "answer": "She was pausing as they prepared to open the wire mesh gate to enter the animal enclosure.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01255.mp4",
    "question_id": "01255_4",
    "clip_path": "clips/01255/01255__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:03 - 00:07",
    "context": "[00:03 - 00:07] As the camera wearer enters the enclosure, they make two soft, cooing sounds.",
    "question_type": "Counting",
    "question": "How many soft cooing sounds does the camera wearer make as they enter the enclosure?",
    "answer": "Two.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01255.mp4",
    "question_id": "01255_5",
    "clip_path": "clips/01255/01255__0002500_0007500.mp4"
  },
  {
    "timestamp": "00:03 - 00:07",
    "context": "[00:03 - 00:07] As the camera wearer enters the enclosure, they make two soft, cooing sounds.",
    "question_type": "Temporal Information",
    "question": "When do the cooing sounds occur relative to the action in the scene?",
    "answer": "They occur as the camera wearer enters the enclosure, between 00:03 and 00:07.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01255.mp4",
    "question_id": "01255_6",
    "clip_path": "clips/01255/01255__0002500_0007500.mp4"
  },
  {
    "timestamp": "00:03 - 00:07",
    "context": "[00:03 - 00:07] The person in the white jacket, who has just been helped through the entrance, expresses gratitude by saying '谢谢谢谢谢谢' (Thank you, thank you, thank you) from nearby.",
    "question_type": "Counting",
    "question": "How many times does the person in the white jacket say '谢谢' (thank you)?",
    "answer": "Three times.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01255.mp4",
    "question_id": "01255_7",
    "clip_path": "clips/01255/01255__0002500_0007500.mp4"
  },
  {
    "timestamp": "00:03 - 00:07",
    "context": "[00:03 - 00:07] The person in the white jacket, who has just been helped through the entrance, expresses gratitude by saying '谢谢谢谢谢谢' from nearby.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the person in the white jacket repeatedly say '谢谢' (thank you)?",
    "answer": "Because she had just been helped through the entrance.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01255.mp4",
    "question_id": "01255_8",
    "clip_path": "clips/01255/01255__0002500_0007500.mp4"
  },
  {
    "timestamp": "00:03 - 00:07",
    "context": "[00:03 - 00:07] The camera pans to reveal the interior of the enclosure: a green floor, a large artificial tree, and a wall decorated with butterfly paintings, while nearby thanks are spoken.",
    "question_type": "Cross-Modal Reasoning",
    "question": "Following the entry and expressions of thanks, what visual details does the camera pan reveal about the enclosure's interior?",
    "answer": "A green floor, a large artificial tree, and a wall decorated with butterfly paintings.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01255.mp4",
    "question_id": "01255_9",
    "clip_path": "clips/01255/01255__0002500_0007500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "[00:00 - 00:04] A female voice, originating very close to the camera, speaks at a clear, conversational volume. Faint, high-pitched bird chirps are audible in the background of the aviary.",
    "question_type": "Sound Characteristics",
    "question": "What were the pitch and volume characteristics of the background bird chirps at the start?",
    "answer": "They were faint and high-pitched.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01260.mp4",
    "question_id": "01260_1",
    "clip_path": "clips/01260/01260__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "[00:00 - 00:04] Faint, high-pitched bird chirps are audible in the background of the aviary while a small green parrot perches on the bowl edge, eating.",
    "question_type": "Sound Source Identification",
    "question": "What was the source of the faint, high-pitched chirping heard in the background?",
    "answer": "Birds in the aviary.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01260.mp4",
    "question_id": "01260_2",
    "clip_path": "clips/01260/01260__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "[00:00 - 00:04] A female voice, originating very close to the camera, makes a joking remark.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the joking female voice originate relative to the camera?",
    "answer": "Very close to the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01260.mp4",
    "question_id": "01260_3",
    "clip_path": "clips/01260/01260__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "[00:00 - 00:04] A female voice speaks at a clear, conversational volume: “这两天我在偷吃环尾狐猴的食物”。",
    "question_type": "Sound Characteristics",
    "question": "What was the volume of the female speaker making the joking remark?",
    "answer": "Clear, conversational volume.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01260.mp4",
    "question_id": "01260_4",
    "clip_path": "clips/01260/01260__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:05 - 00:11",
    "context": "[00:05 - 00:11] The camera operator extends a hand toward the parrot. In response to this action, their companion warns with a light laugh: “他会躲着你... 他会躲着你哈哈”。",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the companion warn, “It will hide from you”?",
    "answer": "Because the camera operator was reaching toward the parrot to touch it.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01260.mp4",
    "question_id": "01260_5",
    "clip_path": "clips/01260/01260__0004500_0011500.mp4"
  },
  {
    "timestamp": "00:05 - 00:11",
    "context": "[00:05 - 00:11] Companion warns, “他会躲着你... 他会躲着你哈哈”. As predicted, the parrot shuffles nervously and then takes flight to the left.",
    "question_type": "Temporal Information",
    "question": "Did the companion’s warning occur before or after the parrot flew away?",
    "answer": "Before; it preceded and predicted the bird’s flight.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01260.mp4",
    "question_id": "01260_6",
    "clip_path": "clips/01260/01260__0004500_0011500.mp4"
  },
  {
    "timestamp": "00:05 - 00:11",
    "context": "[00:05 - 00:11] Companion says, “他会躲着你... 他会躲着你哈哈”.",
    "question_type": "Counting",
    "question": "How many times did the companion repeat the phrase “It will hide from you”?",
    "answer": "Twice.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01260.mp4",
    "question_id": "01260_7",
    "clip_path": "clips/01260/01260__0004500_0011500.mp4"
  },
  {
    "timestamp": "00:05 - 00:11",
    "context": "[00:05 - 00:11] The companion warns with a light laugh while speaking to the camera operator.",
    "question_type": "Sound Characteristics",
    "question": "What was the quality of the laugh that accompanied the companion’s warning?",
    "answer": "A light laugh.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01260.mp4",
    "question_id": "01260_8",
    "clip_path": "clips/01260/01260__0004500_0011500.mp4"
  },
  {
    "timestamp": "00:11 - 00:13",
    "context": "[00:11 - 00:13] Immediately after the parrot flies away and the hand is retracted, a female voice says gently: “好吧不摸你了”。",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the female speaker say, “Alright, I won’t touch you anymore”?",
    "answer": "Because the parrot had flown away in shyness, prompting her to stop trying to touch it.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01260.mp4",
    "question_id": "01260_9",
    "clip_path": "clips/01260/01260__0010500_0013500.mp4"
  },
  {
    "timestamp": "00:11 - 00:13",
    "context": "[00:11 - 00:13] A female voice speaks in a gentle, conversational tone: “好吧不摸你了”。",
    "question_type": "Sound Characteristics",
    "question": "What was the tone of the female voice when saying “Alright, I won’t touch you anymore”?",
    "answer": "Gentle and conversational.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01260.mp4",
    "question_id": "01260_10",
    "clip_path": "clips/01260/01260__0010500_0013500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "In an indoor aviary, several people excitedly observe and film parrots. Overlapping, moderate-volume speech from multiple people is heard in front of the camera; one says, “看到了看到了...鹦鹉呢?”",
    "question_type": "Sound Characteristics",
    "question": "What was the nature and volume of the group’s speech at the start?",
    "answer": "Overlapping, moderate-volume speech conveying excitement.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01261.mp4",
    "question_id": "01261_1",
    "clip_path": "clips/01261/01261__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "A group of people speak excitedly while observing the parrots, with overlapping speech described as being in front of the camera.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the overlapping speech originate relative to the camera?",
    "answer": "From in front of the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01261.mp4",
    "question_id": "01261_2",
    "clip_path": "clips/01261/01261__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:07 - 00:15",
    "context": "As the parrots eat and crack seed shells, a woman off-camera explains their feeding habits in a clear tone, prompted by the visual of the parrots skillfully peeling kernels.",
    "question_type": "Inferential & Contextual Causality",
    "question": "What prompted the woman off-camera to explain the parrots’ feeding habits?",
    "answer": "The visual of the parrots cracking seed shells and cleanly peeling the kernels.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01261.mp4",
    "question_id": "01261_3",
    "clip_path": "clips/01261/01261__0006500_0015500.mp4"
  },
  {
    "timestamp": "00:07 - 00:15",
    "context": "Parrots crack seed shells while eating, producing a series of faint, sharp cracking sounds audible from the dish.",
    "question_type": "Sound Characteristics",
    "question": "How are the seed-cracking sounds described?",
    "answer": "A series of faint, sharp cracking sounds.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01261.mp4",
    "question_id": "01261_4",
    "clip_path": "clips/01261/01261__0006500_0015500.mp4"
  },
  {
    "timestamp": "00:07 - 00:15",
    "context": "The seed-cracking sounds are audible from the glass dish directly in front of the camera.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where do the seed-cracking sounds come from relative to the camera?",
    "answer": "From the glass dish directly in front of the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01261.mp4",
    "question_id": "01261_5",
    "clip_path": "clips/01261/01261__0006500_0015500.mp4"
  },
  {
    "timestamp": "00:18 - 00:22",
    "context": "Another person says, “哦我听见了, 就像人磕一样,” showing they are actively listening to the parrots’ seed-cracking sounds.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the person compare the sound to how people crack seeds?",
    "answer": "Because they heard the parrots’ seed-cracking sounds and related them to human seed-cracking.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01261.mp4",
    "question_id": "01261_6",
    "clip_path": "clips/01261/01261__0017500_0022500.mp4"
  },
  {
    "timestamp": "00:22 - 00:28",
    "context": "A brief, sharp sound of wings flapping is heard as a parrot flies off the feeding dish, followed by a surprised exclamation, “啊在我头上!”",
    "question_type": "Sound Source Identification",
    "question": "What produced the brief, sharp sound heard just before the exclamation?",
    "answer": "Wings flapping as a parrot flew off the feeding dish.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01261.mp4",
    "question_id": "01261_7",
    "clip_path": "clips/01261/01261__0021500_0028500.mp4"
  },
  {
    "timestamp": "00:22 - 00:28",
    "context": "A brief, sharp wings-flapping sound is heard as a parrot departs the dish.",
    "question_type": "Temporal Information",
    "question": "Was the wings-flapping sound brief or sustained?",
    "answer": "Brief.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01261.mp4",
    "question_id": "01261_8",
    "clip_path": "clips/01261/01261__0021500_0028500.mp4"
  },
  {
    "timestamp": "00:22 - 00:28",
    "context": "After the wing-flapping sound, a woman exclaims, “啊在我头上!” Another person asks if it’s pulling her hair, and she confirms she felt it.",
    "question_type": "Cross-Modal Reasoning",
    "question": "After the flapping sound, what likely happened visually that led to the woman’s exclamation?",
    "answer": "The parrot flew from the dish and landed on her head, interacting with (pulling) her hair.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01261.mp4",
    "question_id": "01261_9",
    "clip_path": "clips/01261/01261__0021500_0028500.mp4"
  },
  {
    "timestamp": "00:28 - 00:30",
    "context": "Following the surprising interaction with the parrot on her head, the woman laughs heartily.",
    "question_type": "Inferential & Contextual Causality",
    "question": "What was the likely reason for the woman’s hearty laughter?",
    "answer": "Amusement at the unexpected parrot-on-head interaction and hair pulling.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01261.mp4",
    "question_id": "01261_10",
    "clip_path": "clips/01261/01261__0027500_0030500.mp4"
  },
  {
    "timestamp": "00:07 - 00:28",
    "context": "During these intervals, two distinct non-speech sounds are noted: faint, sharp seed-cracking from the dish and a brief, sharp wings-flapping sound as a parrot takes off.",
    "question_type": "Counting",
    "question": "How many distinct types of non-speech animal/environmental sounds are described between 00:07 and 00:28?",
    "answer": "Two: seed-cracking and wings flapping.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01261.mp4",
    "question_id": "01261_11",
    "clip_path": "clips/01261/01261__0006500_0028500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "Several people observe ring-tailed lemurs eating. A woman's voice from the left comments on grapes; another woman agrees. The lemurs emit intermittent, high-pitched calls as they eat.",
    "question_type": "Sound Source Identification",
    "question": "What animals produced the intermittent, high-pitched calls during 00:00-00:04?",
    "answer": "The ring-tailed lemurs.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01264.mp4",
    "question_id": "01264_1",
    "clip_path": "clips/01264/01264__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "A woman's voice from the left comments on the lemurs' grape preference.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From which direction relative to the camera did the woman comment on the lemurs' grape preference?",
    "answer": "From the left.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01264.mp4",
    "question_id": "01264_2",
    "clip_path": "clips/01264/01264__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "One woman comments about grapes and another woman replies, \"Mmm, yes it is.\"",
    "question_type": "Counting",
    "question": "How many women spoke in this segment?",
    "answer": "Two—one commenting and another agreeing.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01264.mp4",
    "question_id": "01264_3",
    "clip_path": "clips/01264/01264__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:04 - 00:08",
    "context": "A woman asks about the lemurs' preferred climate. In response, a lemur directly in front of the camera stands and lets out a loud, sustained, high-pitched call.",
    "question_type": "Inferential & Contextual Causality",
    "question": "What was the likely reason the lemur let out a loud call at 00:04-00:08?",
    "answer": "It called in response to the woman's question about the environment and weather the lemurs are adapted to.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01264.mp4",
    "question_id": "01264_4",
    "clip_path": "clips/01264/01264__0003500_0008500.mp4"
  },
  {
    "timestamp": "00:04 - 00:08",
    "context": "A lemur directly in front of the camera emits a loud, sustained, high-pitched call.",
    "question_type": "Sound Characteristics",
    "question": "How would you describe the lemur's call at 00:04-00:08 in terms of duration and pitch?",
    "answer": "It was loud, sustained, and high-pitched.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01264.mp4",
    "question_id": "01264_5",
    "clip_path": "clips/01264/01264__0003500_0008500.mp4"
  },
  {
    "timestamp": "00:04 - 00:08",
    "context": "The calling lemur is described as directly in front of the camera.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where was the calling lemur located relative to the camera?",
    "answer": "Directly in front of the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01264.mp4",
    "question_id": "01264_6",
    "clip_path": "clips/01264/01264__0003500_0008500.mp4"
  },
  {
    "timestamp": "00:08 - 00:19",
    "context": "As the male guide explains they are 'sun-worshippers,' two lemurs stand and emit loud, prolonged, high-pitched calls.",
    "question_type": "Counting",
    "question": "How many lemurs emitted loud, prolonged high-pitched calls during 00:08-00:19?",
    "answer": "Two.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01264.mp4",
    "question_id": "01264_7",
    "clip_path": "clips/01264/01264__0007500_0019500.mp4"
  },
  {
    "timestamp": "00:08 - 00:19",
    "context": "Two lemurs look upwards and emit loud, prolonged, high-pitched calls as the guide speaks.",
    "question_type": "Sound Characteristics",
    "question": "What were the acoustic characteristics of the lemur calls between 00:08 and 00:19?",
    "answer": "They were loud, prolonged, and high-pitched.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01264.mp4",
    "question_id": "01264_8",
    "clip_path": "clips/01264/01264__0007500_0019500.mp4"
  },
  {
    "timestamp": "00:34 - 00:48",
    "context": "A woman summarizes the climate control explanation; her voice is clear and close to the microphone. Lemurs intermittently vocalize with high-pitched calls while eating.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the woman's speaking voice originate relative to the microphone during 00:34-00:48?",
    "answer": "Very close to the microphone (near the camera).",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01264.mp4",
    "question_id": "01264_9",
    "clip_path": "clips/01264/01264__0033500_0048500.mp4"
  },
  {
    "timestamp": "00:34 - 00:48",
    "context": "While the woman speaks near the microphone, the lemurs intermittently vocalize with high-pitched calls as they eat.",
    "question_type": "Temporal Information",
    "question": "During 00:34-00:48, were the lemurs' calls continuous or intermittent while eating?",
    "answer": "Intermittent.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01264.mp4",
    "question_id": "01264_10",
    "clip_path": "clips/01264/01264__0033500_0048500.mp4"
  },
  {
    "timestamp": "00:48 - 00:50",
    "context": "After the woman's summary, the guide adds, \"Right, it likes hotter places.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the guide add the remark, \"Right, it likes hotter places\" at 00:48-00:50?",
    "answer": "To conclude and emphasize the lemurs’ preference for warmer climates following the woman’s summary.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01264.mp4",
    "question_id": "01264_11",
    "clip_path": "clips/01264/01264__0047500_0050333.mp4"
  },
  {
    "timestamp": "00:00 - 00:05",
    "context": "The camera holder (male) asks from close range, “美女到哪里啊?” The elderly woman stops in front of the camera and replies, “啊我去公交站台啊... 我去坐公交.” The narration notes the man was potentially offering a ride, which she declined by choosing public transport.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the woman state that she was going to the bus stop to take the bus?",
    "answer": "To politely decline a likely offer of a ride and indicate she planned to use public transport.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01267.mp4",
    "question_id": "01267_1",
    "clip_path": "clips/01267/01267__0000000_0005500.mp4"
  },
  {
    "timestamp": "00:00 - 00:05",
    "context": "A clear, friendly male voice initiates the interaction, asking from about 1 meter away: “美女到哪里啊?”",
    "question_type": "Sound Source Identification",
    "question": "Who produced the clear, friendly male voice asking “美女到哪里啊?”",
    "answer": "The camera holder.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01267.mp4",
    "question_id": "01267_2",
    "clip_path": "clips/01267/01267__0000000_0005500.mp4"
  },
  {
    "timestamp": "00:00 - 00:05",
    "context": "The woman replies, “啊我去公交站台啊... 我去坐公交,” described as a clear, slightly high-pitched voice.",
    "question_type": "Sound Characteristics",
    "question": "What were the acoustic qualities of the woman's reply?",
    "answer": "Her voice was clear and slightly high-pitched.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01267.mp4",
    "question_id": "01267_3",
    "clip_path": "clips/01267/01267__0000000_0005500.mp4"
  },
  {
    "timestamp": "00:00 - 00:05",
    "context": "The woman stops directly in front of the camera before replying.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the woman's reply originate relative to the camera?",
    "answer": "Directly in front of the camera at close range.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01267.mp4",
    "question_id": "01267_4",
    "clip_path": "clips/01267/01267__0000000_0005500.mp4"
  },
  {
    "timestamp": "00:00 - 00:05",
    "context": "A brief exchange occurs: the man asks, the woman replies, and the man confirms as she begins to walk past.",
    "question_type": "Temporal Information",
    "question": "What is the duration of the brief conversation from greeting to confirmation?",
    "answer": "About 5 seconds (00:00 to 00:05).",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01267.mp4",
    "question_id": "01267_5",
    "clip_path": "clips/01267/01267__0000000_0005500.mp4"
  },
  {
    "timestamp": "00:00 - 00:05",
    "context": "The sequence of speech is: man asks, woman replies, man confirms.",
    "question_type": "Counting",
    "question": "How many distinct spoken turns occur in the exchange?",
    "answer": "Three spoken turns.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01267.mp4",
    "question_id": "01267_6",
    "clip_path": "clips/01267/01267__0000000_0005500.mp4"
  },
  {
    "timestamp": "00:00 - 00:09",
    "context": "A cheerful, upbeat electronic jingle plays continuously from the machine in front.",
    "question_type": "Sound Source Identification",
    "question": "What generates the cheerful, upbeat electronic jingle heard during this segment?",
    "answer": "The claw machine directly in front of the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01271.mp4",
    "question_id": "01271_1",
    "clip_path": "clips/01271/01271__0000000_0009167.mp4"
  },
  {
    "timestamp": "00:00 - 00:09",
    "context": "A cheerful, upbeat electronic jingle plays continuously from the machine in front.",
    "question_type": "Sound Characteristics",
    "question": "How is the electronic jingle described acoustically?",
    "answer": "It is a cheerful, upbeat electronic tune played continuously.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01271.mp4",
    "question_id": "01271_2",
    "clip_path": "clips/01271/01271__0000000_0009167.mp4"
  },
  {
    "timestamp": "00:00 - 00:09",
    "context": "A cheerful, upbeat electronic jingle plays continuously from the machine in front.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From where relative to the camera does the jingle originate?",
    "answer": "From the claw machine directly in front of the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01271.mp4",
    "question_id": "01271_3",
    "clip_path": "clips/01271/01271__0000000_0009167.mp4"
  },
  {
    "timestamp": "00:00 - 00:09",
    "context": "A cheerful, upbeat electronic jingle plays continuously from the machine in front.",
    "question_type": "Temporal Information",
    "question": "Is the jingle brief or continuous during 00:00–00:09?",
    "answer": "It is continuous for the entire segment.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01271.mp4",
    "question_id": "01271_4",
    "clip_path": "clips/01271/01271__0000000_0009167.mp4"
  },
  {
    "timestamp": "00:00 - 00:09",
    "context": "Several voices are heard from nearby. A female voice exclaims, '快过来呀' (Come here quickly). Another female voice suggests, '你来一次' (You try once). A male voice questions, '我来一次吗?' (Should I try once?).",
    "question_type": "Counting",
    "question": "How many distinct speakers are involved in the dialogue?",
    "answer": "Three: two female voices and one male voice.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01271.mp4",
    "question_id": "01271_5",
    "clip_path": "clips/01271/01271__0000000_0009167.mp4"
  },
  {
    "timestamp": "00:00 - 00:09",
    "context": "A female voice exclaims, '快过来呀' (Come here quickly).",
    "question_type": "Sound Source Identification",
    "question": "Which type of voice delivers the line '快过来呀' (Come here quickly)?",
    "answer": "A female voice.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01271.mp4",
    "question_id": "01271_6",
    "clip_path": "clips/01271/01271__0000000_0009167.mp4"
  },
  {
    "timestamp": "00:00 - 00:09",
    "context": "A male voice questions, '我来一次吗?' (Should I try once?).",
    "question_type": "Sound Source Identification",
    "question": "Who asks '我来一次吗?' (Should I try once)?",
    "answer": "A male voice.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01271.mp4",
    "question_id": "01271_7",
    "clip_path": "clips/01271/01271__0000000_0009167.mp4"
  },
  {
    "timestamp": "00:00 - 00:09",
    "context": "Another female voice suggests, '你来一次' (You try once). The first female voice then responds with a slight sigh, '嗯, 我来一次也抓不着呀' (Mhm, I can't catch it even if I try once).",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why does the first female respond with a slight sigh saying she can't catch it even if she tries?",
    "answer": "She is reacting to the suggestion that someone try the claw machine, expressing doubt and resignation about her ability to win.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01271.mp4",
    "question_id": "01271_8",
    "clip_path": "clips/01271/01271__0000000_0009167.mp4"
  },
  {
    "timestamp": "00:00 - 00:09",
    "context": "As she speaks, a person in a white puffer jacket, holding a small purple basket, approaches the claw machine from the left, positioning themselves to play.",
    "question_type": "Cross-Modal Reasoning",
    "question": "Following the conversation about who should try, what visual action occurs?",
    "answer": "A person in a white puffer jacket with a small purple basket approaches from the left to play the claw machine.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01271.mp4",
    "question_id": "01271_9",
    "clip_path": "clips/01271/01271__0000000_0009167.mp4"
  },
  {
    "timestamp": "00:00 - 00:09",
    "context": "Several voices are heard from nearby.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where are the speaking voices located relative to the camera?",
    "answer": "Nearby the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01271.mp4",
    "question_id": "01271_10",
    "clip_path": "clips/01271/01271__0000000_0009167.mp4"
  },
  {
    "timestamp": "00:00 - 00:09",
    "context": "Quoted lines include: '快过来呀', '你来一次', '我来一次吗?', and '嗯, 我来一次也抓不着呀'.",
    "question_type": "Counting",
    "question": "How many quoted lines of dialogue are spoken in this segment?",
    "answer": "Four.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01271.mp4",
    "question_id": "01271_11",
    "clip_path": "clips/01271/01271__0000000_0009167.mp4"
  },
  {
    "timestamp": "00:00 - 00:09",
    "context": "The first female voice then responds with a slight sigh, '嗯, 我来一次也抓不着呀'.",
    "question_type": "Sound Characteristics",
    "question": "What notable vocal nuance accompanies the first female's final response?",
    "answer": "A slight sigh.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01271.mp4",
    "question_id": "01271_12",
    "clip_path": "clips/01271/01271__0000000_0009167.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "Cheerful, high-pitched electronic arcade music plays. A female voice from the immediate left laughs loudly and continuously, seemingly amused by the claw machine filled with yellow duck plushies.",
    "question_type": "Inferential & Contextual Causality",
    "question": "What was the likely reason for the female’s loud, continuous laughter at the start?",
    "answer": "She was amused by the claw machine filled with yellow duck plushies.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01276.mp4",
    "question_id": "01276_1",
    "clip_path": "clips/01276/01276__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "A female voice from the immediate left laughs loudly and continuously.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From where relative to the camera did the laughing voice originate?",
    "answer": "From the immediate left, very close to the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01276.mp4",
    "question_id": "01276_2",
    "clip_path": "clips/01276/01276__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "Cheerful, high-pitched electronic arcade music plays in the background.",
    "question_type": "Sound Characteristics",
    "question": "How is the background arcade music characterized?",
    "answer": "It is cheerful, high-pitched electronic music playing in the background.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01276.mp4",
    "question_id": "01276_3",
    "clip_path": "clips/01276/01276__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:04 - 00:06",
    "context": "A hand enters from the left and gives the user a game coin while saying “给你” (“Here, for you”) in a clear, nearby voice.",
    "question_type": "Sound Source Identification",
    "question": "Who produced the speech saying “给你”?",
    "answer": "The person whose hand entered from the left to give the user a game coin.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01276.mp4",
    "question_id": "01276_4",
    "clip_path": "clips/01276/01276__0003500_0006500.mp4"
  },
  {
    "timestamp": "00:04 - 00:06",
    "context": "The person gives a coin while saying “给你” in a clear, nearby voice.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "What was the direction and proximity of the voice that said “给你”?",
    "answer": "It came from the left side, very nearby.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01276.mp4",
    "question_id": "01276_5",
    "clip_path": "clips/01276/01276__0003500_0006500.mp4"
  },
  {
    "timestamp": "00:06 - 00:08",
    "context": "The user inserts the coin into the slot, producing a single, sharp metallic clinking sound as the coin drops into the machine’s mechanism.",
    "question_type": "Counting",
    "question": "How many metallic clinking sounds were produced when the coin was inserted?",
    "answer": "One.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01276.mp4",
    "question_id": "01276_6",
    "clip_path": "clips/01276/01276__0005500_0008500.mp4"
  },
  {
    "timestamp": "00:06 - 00:08",
    "context": "The user moves to the right side of the machine and inserts the coin; a metallic clink is heard as it drops into the mechanism.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the metallic clinking sound originate relative to the camera?",
    "answer": "From the machine’s coin slot/mechanism on the right side.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01276.mp4",
    "question_id": "01276_7",
    "clip_path": "clips/01276/01276__0005500_0008500.mp4"
  },
  {
    "timestamp": "00:06 - 00:08",
    "context": "A single, sharp metallic clinking sound is heard as the coin drops into the machine’s mechanism.",
    "question_type": "Sound Source Identification",
    "question": "What generated the sharp metallic clinking sound?",
    "answer": "The coin dropping into the machine’s internal mechanism.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01276.mp4",
    "question_id": "01276_8",
    "clip_path": "clips/01276/01276__0005500_0008500.mp4"
  },
  {
    "timestamp": "00:08 - 00:14",
    "context": "Immediately after the coin is inserted, the main button glows with pulsing blue and purple light. A series of upbeat, high-pitched electronic jingles and chimes play, and a continuous, low-volume mechanical whirring begins as the claw starts moving.",
    "question_type": "Cross-Modal Reasoning",
    "question": "After the main button begins to glow, what audio feedback follows and what does it indicate?",
    "answer": "A series of upbeat, high-pitched electronic jingles and chimes play, indicating the game is ready to start.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01276.mp4",
    "question_id": "01276_9",
    "clip_path": "clips/01276/01276__0007500_0014500.mp4"
  },
  {
    "timestamp": "00:08 - 00:14",
    "context": "A continuous, low-volume mechanical whirring sound begins from the top of the machine as the claw mechanism moves horizontally.",
    "question_type": "Temporal Information",
    "question": "When does the mechanical whirring start and how is its continuity described in this interval?",
    "answer": "It starts immediately after the coin is inserted and is continuous at low volume during 00:08–00:14.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01276.mp4",
    "question_id": "01276_10",
    "clip_path": "clips/01276/01276__0007500_0014500.mp4"
  },
  {
    "timestamp": "00:08 - 00:14",
    "context": "A continuous, low-volume mechanical whirring sound begins from the top of the machine.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where is the mechanical whirring sound coming from?",
    "answer": "From the top of the machine.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01276.mp4",
    "question_id": "01276_11",
    "clip_path": "clips/01276/01276__0007500_0014500.mp4"
  },
  {
    "timestamp": "00:15 - 00:18",
    "context": "While the claw is still moving, a female voice from the left says, “它老是在那边掉怎么办,” and another nearby female voice lets out a short laugh.",
    "question_type": "Counting",
    "question": "How many distinct female voices are heard in this segment?",
    "answer": "Two.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01276.mp4",
    "question_id": "01276_12",
    "clip_path": "clips/01276/01276__0014500_0018500.mp4"
  },
  {
    "timestamp": "00:15 - 00:18",
    "context": "A female voice from the left comments, “它老是在那边掉怎么办,” described as referencing a likely previous failed attempt.",
    "question_type": "Inferential & Contextual Causality",
    "question": "What prompted the comment about it “always dropping over there”?",
    "answer": "A likely previous failed attempt where the claw dropped the prize in that spot.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01276.mp4",
    "question_id": "01276_13",
    "clip_path": "clips/01276/01276__0014500_0018500.mp4"
  },
  {
    "timestamp": "00:00 - 01:01",
    "context": "The interaction is accompanied by a continuous, close-range conversation between the two individuals and faint, calm background music.",
    "question_type": "Sound Characteristics",
    "question": "How is the background music described during the interaction?",
    "answer": "It is faint and calm.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01282.mp4",
    "question_id": "01282_1",
    "clip_path": "clips/01282/01282__0000000_0060633.mp4"
  },
  {
    "timestamp": "00:00 - 01:01",
    "context": "The interaction is accompanied by a continuous, close-range conversation between the two individuals.",
    "question_type": "Sound Characteristics",
    "question": "What is the nature of the conversation's sound during the clip?",
    "answer": "It is close-range and continuous.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01282.mp4",
    "question_id": "01282_2",
    "clip_path": "clips/01282/01282__0000000_0060633.mp4"
  },
  {
    "timestamp": "00:00 - 01:01",
    "context": "The interaction is accompanied by a continuous, close-range conversation between the two individuals.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where does the conversation originate relative to the camera?",
    "answer": "From close range, near the camera wearer and the staff member.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01282.mp4",
    "question_id": "01282_3",
    "clip_path": "clips/01282/01282__0000000_0060633.mp4"
  },
  {
    "timestamp": "00:00 - 01:01",
    "context": "The interaction is accompanied by faint, calm background music.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where is the background music relative to the camera?",
    "answer": "In the background (ambient, not close to the camera).",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01282.mp4",
    "question_id": "01282_4",
    "clip_path": "clips/01282/01282__0000000_0060633.mp4"
  },
  {
    "timestamp": "00:00 - 01:01",
    "context": "Faint, calm background music plays during the interaction.",
    "question_type": "Temporal Information",
    "question": "Is the background music continuous or intermittent throughout the clip?",
    "answer": "Continuous.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01282.mp4",
    "question_id": "01282_5",
    "clip_path": "clips/01282/01282__0000000_0060633.mp4"
  },
  {
    "timestamp": "00:00 - 01:01",
    "context": "The staff member provides care instructions: \"You can use it after washing it... It can go in the microwave, so scald it with hot water to disinfect it before using,\" and this occurs after the presentation and compliments.",
    "question_type": "Temporal Information",
    "question": "When do the care instructions occur within the clip?",
    "answer": "At the end of the interaction (finally).",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01282.mp4",
    "question_id": "01282_6",
    "clip_path": "clips/01282/01282__0000000_0060633.mp4"
  },
  {
    "timestamp": "00:00 - 01:01",
    "context": "The staff member says, \"Looks pretty good,\" and then suggests the user can share it with colleagues.",
    "question_type": "Temporal Information",
    "question": "Did the compliment come before or after the suggestion to share it with colleagues?",
    "answer": "Before.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01282.mp4",
    "question_id": "01282_7",
    "clip_path": "clips/01282/01282__0000000_0060633.mp4"
  },
  {
    "timestamp": "00:00 - 01:01",
    "context": "The staff member explains: \"You can use it after washing it... It can go in the microwave, so scald it with hot water to disinfect it before using.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the staff member recommend scalding the cup with hot water before using it?",
    "answer": "To disinfect it before use.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01282.mp4",
    "question_id": "01282_8",
    "clip_path": "clips/01282/01282__0000000_0060633.mp4"
  },
  {
    "timestamp": "00:00 - 01:01",
    "context": "The staff member compliments the work, saying, \"Looks pretty good,\" and suggests the user can share it with colleagues.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Based on the conversation, why did the staff member suggest sharing the cup with colleagues?",
    "answer": "Because they thought the cup looked pretty good.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01282.mp4",
    "question_id": "01282_9",
    "clip_path": "clips/01282/01282__0000000_0060633.mp4"
  },
  {
    "timestamp": "00:00 - 01:01",
    "context": "The staff member compliments the work, saying, \"Looks pretty good.\"",
    "question_type": "Sound Source Identification",
    "question": "Who said, \"Looks pretty good\"?",
    "answer": "The staff member.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01282.mp4",
    "question_id": "01282_10",
    "clip_path": "clips/01282/01282__0000000_0060633.mp4"
  },
  {
    "timestamp": "00:00 - 01:01",
    "context": "The user says, \"Let me feel it. Oh, it's really become so smooth.\"",
    "question_type": "Sound Source Identification",
    "question": "Who said, \"Let me feel it. Oh, it's really become so smooth\"?",
    "answer": "The camera wearer (user).",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01282.mp4",
    "question_id": "01282_11",
    "clip_path": "clips/01282/01282__0000000_0060633.mp4"
  },
  {
    "timestamp": "00:00 - 01:01",
    "context": "Finally, the staff member provides care instructions about washing, microwave use, and scalding with hot water.",
    "question_type": "Sound Source Identification",
    "question": "Who provided the care instructions for using the cup?",
    "answer": "The staff member.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01282.mp4",
    "question_id": "01282_12",
    "clip_path": "clips/01282/01282__0000000_0060633.mp4"
  },
  {
    "timestamp": "00:00 - 01:01",
    "context": "They both observe that the inside of the cup is green.",
    "question_type": "Sound Source Identification",
    "question": "Who noted that the inside of the cup is green?",
    "answer": "Both the staff member and the user.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01282.mp4",
    "question_id": "01282_13",
    "clip_path": "clips/01282/01282__0000000_0060633.mp4"
  },
  {
    "timestamp": "00:00 - 01:01",
    "context": "A continuous, close-range conversation occurs between two individuals.",
    "question_type": "Counting",
    "question": "How many people are actively participating in the conversation?",
    "answer": "Two.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01282.mp4",
    "question_id": "01282_14",
    "clip_path": "clips/01282/01282__0000000_0060633.mp4"
  },
  {
    "timestamp": "00:00 - 01:01",
    "context": "They identify a small chicken drawing that is now hard to see, a heart shape that is very obvious, and a rabbit whose texture the user can feel.",
    "question_type": "Counting",
    "question": "How many distinct designs on the cup are verbally identified?",
    "answer": "Three: a chicken, a heart, and a rabbit.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01282.mp4",
    "question_id": "01282_15",
    "clip_path": "clips/01282/01282__0000000_0060633.mp4"
  },
  {
    "timestamp": "00:00 - 01:01",
    "context": "Care instructions include: use after washing, it can go in the microwave, and scald with hot water to disinfect before using.",
    "question_type": "Counting",
    "question": "How many distinct care instructions are given?",
    "answer": "Three.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01282.mp4",
    "question_id": "01282_16",
    "clip_path": "clips/01282/01282__0000000_0060633.mp4"
  },
  {
    "timestamp": "00:05 - 00:08",
    "context": "A shopkeeper places a small ceramic object into a brown paper bag, causing soft rustling sounds and a gentle thud.",
    "question_type": "Sound Source Identification",
    "question": "What action generated the soft rustling sounds and the gentle thud at 00:05 - 00:08?",
    "answer": "Placing a small ceramic object into a brown paper bag.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01283.mp4",
    "question_id": "01283_1",
    "clip_path": "clips/01283/01283__0004500_0008500.mp4"
  },
  {
    "timestamp": "00:05 - 00:08",
    "context": "Soft rustling sounds and a gentle thud occur as the ceramic object is bagged.",
    "question_type": "Sound Characteristics",
    "question": "How are the bagging sounds described at 00:05 - 00:08?",
    "answer": "They are soft rustling sounds followed by a gentle thud.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01283.mp4",
    "question_id": "01283_2",
    "clip_path": "clips/01283/01283__0004500_0008500.mp4"
  },
  {
    "timestamp": "00:05 - 00:08",
    "context": "The shopkeeper, located directly in front of the user, says, \"It's wrapped up, just right,\" and the user replies, \"Okay.\"",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the shopkeeper’s speech originate relative to the camera during 00:05 - 00:08?",
    "answer": "Directly in front of the camera/user.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01283.mp4",
    "question_id": "01283_3",
    "clip_path": "clips/01283/01283__0004500_0008500.mp4"
  },
  {
    "timestamp": "00:08 - 00:22",
    "context": "The shopkeeper takes the ceramic piece out, invites the user to touch the bottom, and explains the uneven texture and repairs with ceramic glue.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the shopkeeper take the ceramic piece out again and ask the user to touch the bottom?",
    "answer": "To let the user feel the unevenness from shrinkage cracks and explain that it had been patched with ceramic glue.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01283.mp4",
    "question_id": "01283_4",
    "clip_path": "clips/01283/01283__0007500_0022500.mp4"
  },
  {
    "timestamp": "00:08 - 00:22",
    "context": "During the explanation, faint scratching sounds are produced as the user's fingers feel the repaired surface.",
    "question_type": "Sound Source Identification",
    "question": "What produced the faint scratching sounds between 00:08 and 00:22?",
    "answer": "The user's fingers rubbing the repaired surface of the ceramic piece.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01283.mp4",
    "question_id": "01283_5",
    "clip_path": "clips/01283/01283__0007500_0022500.mp4"
  },
  {
    "timestamp": "00:08 - 00:22",
    "context": "Faint scratching sounds are heard while the user feels the repaired surface during the explanation.",
    "question_type": "Temporal Information",
    "question": "When did the faint scratching sounds occur relative to the shopkeeper’s explanation?",
    "answer": "They occurred during the explanation, as the user felt the surface.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01283.mp4",
    "question_id": "01283_6",
    "clip_path": "clips/01283/01283__0007500_0022500.mp4"
  },
  {
    "timestamp": "00:08 - 00:22",
    "context": "The caption describes 'faint scratching sounds' as the user feels the repaired surface.",
    "question_type": "Sound Characteristics",
    "question": "What are the volume and texture of the scratching sounds heard between 00:08 and 00:22?",
    "answer": "They are faint and have a scratching texture.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01283.mp4",
    "question_id": "01283_7",
    "clip_path": "clips/01283/01283__0007500_0022500.mp4"
  },
  {
    "timestamp": "00:22 - 00:26",
    "context": "The shopkeeper places the ceramic piece back into the paper bag, producing a brief rustling sound, then hands the bag to the user.",
    "question_type": "Temporal Information",
    "question": "Is the paper bag’s rustling sound at 00:22 - 00:26 brief or continuous?",
    "answer": "It is brief.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01283.mp4",
    "question_id": "01283_8",
    "clip_path": "clips/01283/01283__0021500_0026500.mp4"
  },
  {
    "timestamp": "00:26 - 00:38",
    "context": "The user tells the shopkeeper, \"No, no need. I'll go find my friend... because he's nearby... It's okay, I'm very familiar with this road.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the user decline further assistance before leaving?",
    "answer": "Because their friend is nearby and they are very familiar with the road.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01283.mp4",
    "question_id": "01283_9",
    "clip_path": "clips/01283/01283__0025500_0038500.mp4"
  },
  {
    "timestamp": "00:38 - 00:41",
    "context": "As the user walks toward the exit, the shopkeeper asks which way to go. The user replies with directions.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the shopkeeper ask which way to go at 00:38 - 00:41?",
    "answer": "To confirm the correct direction as the user headed toward the exit.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01283.mp4",
    "question_id": "01283_10",
    "clip_path": "clips/01283/01283__0037500_0041500.mp4"
  },
  {
    "timestamp": "00:41 - 00:46",
    "context": "Another person near the glass door offers to open it; a soft click is heard as they interact with the door handle.",
    "question_type": "Sound Source Identification",
    "question": "What produced the soft click heard at 00:41 - 00:46?",
    "answer": "Interaction with the glass door’s handle.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01283.mp4",
    "question_id": "01283_11",
    "clip_path": "clips/01283/01283__0040500_0046500.mp4"
  },
  {
    "timestamp": "00:46 - 00:49",
    "context": "The person pushes the glass door open, which emits a continuous, low-pitched creaking sound.",
    "question_type": "Sound Characteristics",
    "question": "What are the pitch and continuity of the door sound at 00:46 - 00:49?",
    "answer": "It is a continuous, low-pitched creaking sound.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01283.mp4",
    "question_id": "01283_12",
    "clip_path": "clips/01283/01283__0045500_0048667.mp4"
  },
  {
    "timestamp": "00:41 - 00:49",
    "context": "A soft click occurs as the door handle is used, followed by a continuous, low-pitched creaking as the glass door is pushed open.",
    "question_type": "Cross-Modal Reasoning",
    "question": "After the soft click on the door handle, what sound followed and what does it indicate about the action taking place?",
    "answer": "A continuous, low-pitched creaking followed, indicating the glass door was being pushed open.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01283.mp4",
    "question_id": "01283_13",
    "clip_path": "clips/01283/01283__0040500_0048667.mp4"
  },
  {
    "timestamp": "00:41 - 00:49",
    "context": "Door interaction produces a soft click and then a continuous creak as it opens.",
    "question_type": "Counting",
    "question": "How many distinct door-related sounds are heard between 00:41 and 00:49?",
    "answer": "Two: a soft click from the handle and a continuous, low-pitched creak as the door opens.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01283.mp4",
    "question_id": "01283_14",
    "clip_path": "clips/01283/01283__0040500_0048667.mp4"
  },
  {
    "timestamp": "00:05 - 00:08; 00:22 - 00:26",
    "context": "Rustling occurs when the ceramic is first placed into the paper bag and again when it is placed back into the bag later.",
    "question_type": "Counting",
    "question": "Across the scene, how many separate times is a paper bag rustling sound heard?",
    "answer": "Twice: once at 00:05 - 00:08 and again at 00:22 - 00:26.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01283.mp4",
    "question_id": "01283_15",
    "clip_path": "clips/01283/01283__0004500_0026500.mp4"
  },
  {
    "timestamp": "00:00 - 00:14",
    "context": "The female says, \"This should be an old man from Beijing.\" The male, close by, responds with a light laugh, \"Haha, an old man from Beijing, why?\" She then explains, \"Because aren't people from Beijing known for liking to walk birds?\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Based on the preceding exchange, why did the female provide the explanation about people from Beijing walking birds?",
    "answer": "She was answering the male's \"why\" after she said the statue was an old man from Beijing.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01287.mp4",
    "question_id": "01287_1",
    "clip_path": "clips/01287/01287__0000000_0014500.mp4"
  },
  {
    "timestamp": "00:00 - 00:14",
    "context": "A male speaker, also close by, responds with a light laugh, saying, \"Haha, an old man from Beijing, why?\"",
    "question_type": "Sound Source Identification",
    "question": "What was the source of the light laugh heard before the question \"why\"?",
    "answer": "The male speaker close by.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01287.mp4",
    "question_id": "01287_2",
    "clip_path": "clips/01287/01287__0000000_0014500.mp4"
  },
  {
    "timestamp": "00:00 - 00:14",
    "context": "A female speaker, positioned very close to the camera, says, \"This is a bird, a birdcage.\"",
    "question_type": "Sound Source Identification",
    "question": "Who uttered the line, \"This is a bird, a birdcage\"?",
    "answer": "The female speaker very close to the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01287.mp4",
    "question_id": "01287_3",
    "clip_path": "clips/01287/01287__0000000_0014500.mp4"
  },
  {
    "timestamp": "00:00 - 00:14",
    "context": "The male speaker responds with a light laugh: \"Haha, an old man from Beijing, why?\"",
    "question_type": "Sound Characteristics",
    "question": "How is the male's laugh characterized?",
    "answer": "It is a light laugh.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01287.mp4",
    "question_id": "01287_4",
    "clip_path": "clips/01287/01287__0000000_0014500.mp4"
  },
  {
    "timestamp": "00:00 - 00:14",
    "context": "A female speaker, positioned very close to the camera, analyzes the statue. Her clear voice is heard.",
    "question_type": "Sound Characteristics",
    "question": "What is the quality of the female speaker's voice?",
    "answer": "Clear.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01287.mp4",
    "question_id": "01287_5",
    "clip_path": "clips/01287/01287__0000000_0014500.mp4"
  },
  {
    "timestamp": "00:00 - 00:14",
    "context": "A female speaker is described as positioned very close to the camera while speaking.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where relative to the camera is the female speaker's voice coming from?",
    "answer": "Very close to the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01287.mp4",
    "question_id": "01287_6",
    "clip_path": "clips/01287/01287__0000000_0014500.mp4"
  },
  {
    "timestamp": "00:00 - 00:14",
    "context": "A male speaker, also close by, responds to the female.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where relative to the camera is the male speaker located when he speaks?",
    "answer": "Also very close to the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01287.mp4",
    "question_id": "01287_7",
    "clip_path": "clips/01287/01287__0000000_0014500.mp4"
  },
  {
    "timestamp": "00:00 - 00:14",
    "context": "A conversation unfolds as the hand touches the statue, and the speakers exchange lines from start to end of the described segment.",
    "question_type": "Temporal Information",
    "question": "Does the conversation occur briefly or span the duration of the 00:00–00:14 segment?",
    "answer": "It spans the 00:00–00:14 segment.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01287.mp4",
    "question_id": "01287_8",
    "clip_path": "clips/01287/01287__0000000_0014500.mp4"
  },
  {
    "timestamp": "00:00 - 00:14",
    "context": "A female speaker and a male speaker, both close to the camera, participate in the dialogue.",
    "question_type": "Counting",
    "question": "How many speakers participate in the conversation?",
    "answer": "Two.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01287.mp4",
    "question_id": "01287_9",
    "clip_path": "clips/01287/01287__0000000_0014500.mp4"
  },
  {
    "timestamp": "00:00 - 00:14",
    "context": "The female says, \"This is a bird, a birdcage\" and adds, \"he's holding a walking stick,\" before inferring the statue's identity.",
    "question_type": "Counting",
    "question": "How many distinct features of the statue did the female identify aloud before inferring its identity?",
    "answer": "Two: a bird/birdcage and a walking stick.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01287.mp4",
    "question_id": "01287_10",
    "clip_path": "clips/01287/01287__0000000_0014500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "The user and an employee converse at close range. The employee, standing to the right, replies: \"Buy the extra-long one, I'll show you.\" A faint, high-pitched jingle is audible from a small bell on a knitted rabbit keychain attached to the user's white cane.",
    "question_type": "Sound Source Identification",
    "question": "What generated the faint, high-pitched jingle heard at the start?",
    "answer": "A small bell on a knitted rabbit keychain attached to the user's white cane.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01294.mp4",
    "question_id": "01294_1",
    "clip_path": "clips/01294/01294__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "A faint, high-pitched jingle is heard from the small bell on the user's cane keychain.",
    "question_type": "Sound Characteristics",
    "question": "How is the jingle’s sound quality described?",
    "answer": "Faint and high-pitched.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01294.mp4",
    "question_id": "01294_2",
    "clip_path": "clips/01294/01294__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "The employee, standing to the right, replies helpfully: \"Buy the extra-long one, I'll show you.\"",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From which direction relative to the camera did the employee’s reply originate?",
    "answer": "From the right side.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01294.mp4",
    "question_id": "01294_3",
    "clip_path": "clips/01294/01294__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:04 - 00:09",
    "context": "A female voice-over begins and expresses gratitude while the employee keeps searching the shelves.",
    "question_type": "Temporal Information",
    "question": "When does the female voice-over begin?",
    "answer": "At 00:04.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01294.mp4",
    "question_id": "01294_4",
    "clip_path": "clips/01294/01294__0003500_0009500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "As the employee bends down to search the lower shelves, the user thanks her.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the employee bend down at this moment?",
    "answer": "To search the lower shelves for the product.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01294.mp4",
    "question_id": "01294_5",
    "clip_path": "clips/01294/01294__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:35 - 00:55",
    "context": "The user asks about sale items; the employee confirms the products there are not on sale. The user then pivots to ask for period pants.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Based on the sequence of events, why did the user pivot to asking for period pants?",
    "answer": "After learning the items in that section weren’t on promotion, the user moved on to request period pants.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01294.mp4",
    "question_id": "01294_6",
    "clip_path": "clips/01294/01294__0034500_0055500.mp4"
  },
  {
    "timestamp": "00:16 - 00:24",
    "context": "The employee says, \"240 is too small,\" then asks, \"You want the extra-long version, right?\" The user replies, \"Yes.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the employee ask, \"You want the extra-long version, right?\"",
    "answer": "She was confirming the user’s earlier request for an extra-long product.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01294.mp4",
    "question_id": "01294_7",
    "clip_path": "clips/01294/01294__0015500_0024500.mp4"
  },
  {
    "timestamp": "00:09 - 00:24",
    "context": "The employee mentions size options while searching: \"There's a 280,\" later, \"240 is too small,\" and then, \"The extra-long 420 is okay.\"",
    "question_type": "Counting",
    "question": "How many distinct size numbers did the employee mention while comparing options?",
    "answer": "Three: 280, 240, and 420.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01294.mp4",
    "question_id": "01294_8",
    "clip_path": "clips/01294/01294__0008500_0024500.mp4"
  },
  {
    "timestamp": "00:25 - 00:34",
    "context": "While holding the product package, a soft rustling sound is heard.",
    "question_type": "Sound Source Identification",
    "question": "What caused the soft rustling sound during this moment?",
    "answer": "The product’s plastic package being handled.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01294.mp4",
    "question_id": "01294_9",
    "clip_path": "clips/01294/01294__0024500_0034500.mp4"
  },
  {
    "timestamp": "00:25 - 00:34",
    "context": "The employee holds the product package, producing a soft rustling sound.",
    "question_type": "Sound Characteristics",
    "question": "How is the sound produced by the package described?",
    "answer": "A soft rustling sound.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01294.mp4",
    "question_id": "01294_10",
    "clip_path": "clips/01294/01294__0024500_0034500.mp4"
  },
  {
    "timestamp": "00:56 - 01:06",
    "context": "The user’s voice-over notes the employee called another staff member. The employee hands period pants to the user, producing a soft rustling sound from the plastic packaging.",
    "question_type": "Cross-Modal Reasoning",
    "question": "When the employee hands the period pants to the user, what sound accompanies the action and what is its source?",
    "answer": "A soft rustling sound from the plastic packaging.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01294.mp4",
    "question_id": "01294_11",
    "clip_path": "clips/01294/01294__0055500_0066500.mp4"
  },
  {
    "timestamp": "00:56 - 01:06",
    "context": "Voice-over: \"Maybe they didn't have my size, so she called another employee over to help me get it.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "According to the voice-over, why did the employee call another employee over?",
    "answer": "Possibly because they didn’t have the user’s size and needed help getting it.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01294.mp4",
    "question_id": "01294_12",
    "clip_path": "clips/01294/01294__0055500_0066500.mp4"
  },
  {
    "timestamp": "00:00 - 00:35",
    "context": "Companion: \"你要安全裤还得找个全棉时代。\" and \"全棉时代是两件5折。\" The context is to find the correct size and take advantage of a store promotion.",
    "question_type": "Inferential & Contextual Causality",
    "question": "What was the likely reason the companion mentioned the Cotton Times brand and its 'two for 50% off' offer?",
    "answer": "To suggest a cost-effective option for period panties and take advantage of the store promotion.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01295.mp4",
    "question_id": "01295_1",
    "clip_path": "clips/01295/01295__0000000_0035500.mp4"
  },
  {
    "timestamp": "00:00 - 00:35",
    "context": "User: \"s码到m码，是不是我穿会小呀？\" and \"有没有更大一点？\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the user ask if S–M might be too small and whether there is a bigger size?",
    "answer": "They were concerned about finding a size that fits properly.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01295.mp4",
    "question_id": "01295_2",
    "clip_path": "clips/01295/01295__0000000_0035500.mp4"
  },
  {
    "timestamp": "00:00 - 00:35",
    "context": "Companion: \"这个我还没买过。\" followed by \"我来问一下啊。\" User: \"嗯好，谢谢。\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Based on the preceding exchange, why did the companion offer to 'go ask'?",
    "answer": "Because they hadn't bought this product before and wanted to get clarification about the sizing/options.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01295.mp4",
    "question_id": "01295_3",
    "clip_path": "clips/01295/01295__0000000_0035500.mp4"
  },
  {
    "timestamp": "00:00 - 00:35",
    "context": "As they talk, the user's hand holds a package, producing very faint crinkling sounds from the plastic packaging.",
    "question_type": "Sound Source Identification",
    "question": "What generated the very faint crinkling sounds heard during the conversation?",
    "answer": "The plastic packaging of the 'ABC THINPRO' sanitary pads being held by the user.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01295.mp4",
    "question_id": "01295_4",
    "clip_path": "clips/01295/01295__0000000_0035500.mp4"
  },
  {
    "timestamp": "00:00 - 00:35",
    "context": "Very faint crinkling sounds come from the plastic packaging as the product is held.",
    "question_type": "Sound Characteristics",
    "question": "How would you describe the volume and texture of the crinkling sounds from the packaging?",
    "answer": "They are very faint, light plastic crinkling sounds.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01295.mp4",
    "question_id": "01295_5",
    "clip_path": "clips/01295/01295__0000000_0035500.mp4"
  },
  {
    "timestamp": "00:00 - 00:35",
    "context": "They are engaged in close-range, clear conversational dialogue.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Were the speakers' voices close or distant relative to the camera?",
    "answer": "Close-range.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01295.mp4",
    "question_id": "01295_6",
    "clip_path": "clips/01295/01295__0000000_0035500.mp4"
  },
  {
    "timestamp": "00:00 - 00:35",
    "context": "As they talk, the user's hand produces faint crinkling sounds from the plastic packaging.",
    "question_type": "Temporal Information",
    "question": "Do the faint crinkling sounds occur concurrently with the dialogue or at a separate time?",
    "answer": "Concurrently, as they talk.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01295.mp4",
    "question_id": "01295_7",
    "clip_path": "clips/01295/01295__0000000_0035500.mp4"
  },
  {
    "timestamp": "00:00 - 00:35",
    "context": "A multi-turn conversation unfolds throughout the segment.",
    "question_type": "Temporal Information",
    "question": "Is the conversational exchange brief or sustained throughout the 00:00–00:35 clip?",
    "answer": "It is sustained throughout the clip.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01295.mp4",
    "question_id": "01295_8",
    "clip_path": "clips/01295/01295__0000000_0035500.mp4"
  },
  {
    "timestamp": "00:00 - 00:35",
    "context": "User asks: \"s码到m码，是不是我穿会小呀？\"; \"有没有更大一点？\"; \"有没有那个是s大还是m大呀？\"",
    "question_type": "Counting",
    "question": "How many distinct size-related questions does the user ask?",
    "answer": "Three.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01295.mp4",
    "question_id": "01295_9",
    "clip_path": "clips/01295/01295__0000000_0035500.mp4"
  },
  {
    "timestamp": "00:00 - 00:35",
    "context": "Two people converse: the user and a companion. Speech and faint packaging sounds are audible.",
    "question_type": "Counting",
    "question": "How many types of sounds are present during the scene (excluding silence)?",
    "answer": "Two: close-range speech and very faint plastic crinkling.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01295.mp4",
    "question_id": "01295_10",
    "clip_path": "clips/01295/01295__0000000_0035500.mp4"
  },
  {
    "timestamp": "00:00 - 00:35",
    "context": "Companion clarifies: \"s小m大。\"",
    "question_type": "Counting",
    "question": "How many times does the companion explicitly state the size relationship between S and M?",
    "answer": "Once.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01295.mp4",
    "question_id": "01295_11",
    "clip_path": "clips/01295/01295__0000000_0035500.mp4"
  },
  {
    "timestamp": "00:00 - 00:35",
    "context": "As faint crinkling is heard, the user is holding a white and pink package of 'ABC THINPRO' sanitary pads.",
    "question_type": "Cross-Modal Reasoning",
    "question": "When the faint crinkling sounds occur, which object is the user likely handling?",
    "answer": "The white and pink 'ABC THINPRO' sanitary pads package.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01295.mp4",
    "question_id": "01295_12",
    "clip_path": "clips/01295/01295__0000000_0035500.mp4"
  },
  {
    "timestamp": "00:00 - 00:41",
    "context": "The user holds and examines a package of 420mm ABC brand overnight pads, causing the plastic packaging to rustle softly.",
    "question_type": "Sound Source Identification",
    "question": "What generated the soft rustling sound heard during the clip?",
    "answer": "The plastic packaging of the pads being handled by the user.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01296.mp4",
    "question_id": "01296_1",
    "clip_path": "clips/01296/01296__0000000_0041500.mp4"
  },
  {
    "timestamp": "00:00 - 00:41",
    "context": "…causing the plastic packaging to rustle softly.",
    "question_type": "Sound Characteristics",
    "question": "How is the rustling sound from the packaging described in terms of volume/texture?",
    "answer": "It is a soft rustling.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01296.mp4",
    "question_id": "01296_2",
    "clip_path": "clips/01296/01296__0000000_0041500.mp4"
  },
  {
    "timestamp": "00:00 - 00:41",
    "context": "The camera remains focused on the product package in the user's hand…",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the rustling originate relative to the camera?",
    "answer": "From very close range, directly in front of the camera where the package in the user's hand is held.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01296.mp4",
    "question_id": "01296_3",
    "clip_path": "clips/01296/01296__0000000_0041500.mp4"
  },
  {
    "timestamp": "00:00 - 00:41",
    "context": "A continuous, clear conversation at a close distance takes place between the user and another person…",
    "question_type": "Temporal Information",
    "question": "Was the conversation continuous or intermittent during the clip, and over what period?",
    "answer": "It was continuous throughout the entire 00:00–00:41 clip.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01296.mp4",
    "question_id": "01296_4",
    "clip_path": "clips/01296/01296__0000000_0041500.mp4"
  },
  {
    "timestamp": "00:00 - 00:41",
    "context": "A continuous, clear conversation at a close distance takes place…",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Was the conversation recorded at a close or far distance from the microphone?",
    "answer": "At a close distance.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01296.mp4",
    "question_id": "01296_5",
    "clip_path": "clips/01296/01296__0000000_0041500.mp4"
  },
  {
    "timestamp": "00:00 - 00:41",
    "context": "The other person initiates, 'He needs to use this one,' and the user confirms…",
    "question_type": "Sound Source Identification",
    "question": "Who initiated the conversation with the line, 'He needs to use this one'?",
    "answer": "The other person.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01296.mp4",
    "question_id": "01296_6",
    "clip_path": "clips/01296/01296__0000000_0041500.mp4"
  },
  {
    "timestamp": "00:00 - 00:41",
    "context": "After a brief discussion about body size and comfort, they conclude it's better to choose a slightly larger size to avoid it being too tight.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did they decide to choose a slightly larger size?",
    "answer": "To avoid it being too tight and for better comfort.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01296.mp4",
    "question_id": "01296_7",
    "clip_path": "clips/01296/01296__0000000_0041500.mp4"
  },
  {
    "timestamp": "00:00 - 00:41",
    "context": "The user ends the exchange by saying, 'Um, thank you, thanks.'",
    "question_type": "Counting",
    "question": "How many times did the user express gratitude at the end?",
    "answer": "Twice—'thank you' and 'thanks'.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01296.mp4",
    "question_id": "01296_8",
    "clip_path": "clips/01296/01296__0000000_0041500.mp4"
  },
  {
    "timestamp": "00:00 - 00:41",
    "context": "The user asks, 'How many?' then, 'Is it half price?' and later, 'Sis, what size should I wear that would be suitable?'",
    "question_type": "Counting",
    "question": "How many distinct questions did the user ask during the conversation?",
    "answer": "Three.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01296.mp4",
    "question_id": "01296_9",
    "clip_path": "clips/01296/01296__0000000_0041500.mp4"
  },
  {
    "timestamp": "00:00 - 00:05",
    "context": "[00:00 - 00:03] The user says, \"好那我去世收銀台吧\" (Okay, I'll go to the checkout counter). [00:03 - 00:05] A male employee responds, \"好來你跟我來好\" (Okay, come with me, okay) and begins to lead.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the male employee tell the user to come with him at 00:03 - 00:05?",
    "answer": "Because the user had just stated she wanted to go to the checkout counter, so he began guiding her there.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01301.mp4",
    "question_id": "01301_1",
    "clip_path": "clips/01301/01301__0000000_0005500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "In a brightly lit bakery section, the user's clear female voice says, \"好那我去世收銀台吧\" (Okay, I'll go to the checkout counter).",
    "question_type": "Sound Source Identification",
    "question": "Who spoke the line \"好那我去世收銀台吧\" at the start of the clip?",
    "answer": "The camera user, speaking in a clear female voice.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01301.mp4",
    "question_id": "01301_2",
    "clip_path": "clips/01301/01301__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:03 - 00:05",
    "context": "A male store employee in a red shirt, standing directly in front of the user, responds in a clear, moderate voice, \"好來你跟我來好\" (Okay, come with me, okay).",
    "question_type": "Sound Characteristics",
    "question": "What was the voice quality of the employee’s response at 00:03 - 00:05?",
    "answer": "Clear and moderate in volume.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01301.mp4",
    "question_id": "01301_3",
    "clip_path": "clips/01301/01301__0002500_0005500.mp4"
  },
  {
    "timestamp": "00:03 - 00:05",
    "context": "The male employee stands directly in front of the user and responds, initiating to lead the way.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the employee’s response originate relative to the camera?",
    "answer": "Directly in front of the camera user.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01301.mp4",
    "question_id": "01301_4",
    "clip_path": "clips/01301/01301__0002500_0005500.mp4"
  },
  {
    "timestamp": "00:11 - 00:13",
    "context": "After arriving at the counter, the user says in a clear, close-range voice, \"不好意思\" (Excuse me).",
    "question_type": "Temporal Information",
    "question": "When did the user say \"不好意思\"?",
    "answer": "Between 00:11 and 00:13.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01301.mp4",
    "question_id": "01301_5",
    "clip_path": "clips/01301/01301__0010500_0013500.mp4"
  },
  {
    "timestamp": "00:11 - 00:13",
    "context": "At the counter, the user says \"不好意思\" (Excuse me) to politely get employees’ attention.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the user say \"不好意思\" at the counter?",
    "answer": "To politely get the employees’ attention.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01301.mp4",
    "question_id": "01301_6",
    "clip_path": "clips/01301/01301__0010500_0013500.mp4"
  },
  {
    "timestamp": "00:13 - 00:15",
    "context": "An employee asks from a short distance in front, \"還有別的買嗎?\" (Is there anything else you'd like to buy?).",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From where relative to the camera did the question \"還有別的買嗎?\" come?",
    "answer": "From a short distance directly in front of the camera user.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01301.mp4",
    "question_id": "01301_7",
    "clip_path": "clips/01301/01301__0012500_0015500.mp4"
  },
  {
    "timestamp": "00:13 - 00:15",
    "context": "Employee: \"還有別的買嗎?\" User: \"嗯沒有了\" (No, that's all).",
    "question_type": "Counting",
    "question": "How many speakers participated in the exchange at 00:13 - 00:15?",
    "answer": "Two speakers: the employee and the user.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01301.mp4",
    "question_id": "01301_8",
    "clip_path": "clips/01301/01301__0012500_0015500.mp4"
  },
  {
    "timestamp": "00:13 - 00:15",
    "context": "The user immediately responds, \"嗯沒有了\" (No, that's all).",
    "question_type": "Sound Source Identification",
    "question": "Who said \"嗯沒有了\"?",
    "answer": "The user.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01301.mp4",
    "question_id": "01301_9",
    "clip_path": "clips/01301/01301__0012500_0015500.mp4"
  },
  {
    "timestamp": "00:13 - 00:15",
    "context": "An employee, presumably handling the transaction, asks \"還有別的買嗎?\" (Is there anything else you'd like to buy?).",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the employee ask if there was anything else to buy?",
    "answer": "To confirm whether the user wanted additional items before completing the transaction.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01301.mp4",
    "question_id": "01301_10",
    "clip_path": "clips/01301/01301__0012500_0015500.mp4"
  },
  {
    "timestamp": "00:16 - 00:18",
    "context": "The employee replies from nearby with a friendly and casual tone, \"沒事哈\" (No problem).",
    "question_type": "Sound Characteristics",
    "question": "What was the tone of the employee’s reply \"沒事哈\"?",
    "answer": "Friendly and casual.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01301.mp4",
    "question_id": "01301_11",
    "clip_path": "clips/01301/01301__0015500_0018500.mp4"
  },
  {
    "timestamp": "00:16 - 00:18",
    "context": "The employee replies from nearby, \"沒事哈\" (No problem).",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the reply \"沒事哈\" originate relative to the camera?",
    "answer": "From nearby, close to the user.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01301.mp4",
    "question_id": "01301_12",
    "clip_path": "clips/01301/01301__0015500_0018500.mp4"
  },
  {
    "timestamp": "00:16 - 00:18",
    "context": "The user says \"辛苦了\" (Thanks for your trouble) to the guiding employee; the employee answers \"沒事哈\" (No problem).",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the user say \"辛苦了\" at the end of the interaction?",
    "answer": "To thank the employee who guided them to the checkout area.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01301.mp4",
    "question_id": "01301_13",
    "clip_path": "clips/01301/01301__0015500_0018500.mp4"
  },
  {
    "timestamp": "00:00 - 00:18",
    "context": "Multiple employees speak: one guiding the user and another handling the transaction.",
    "question_type": "Counting",
    "question": "How many distinct employees spoke during the clip?",
    "answer": "Two employees.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01301.mp4",
    "question_id": "01301_14",
    "clip_path": "clips/01301/01301__0000000_0018500.mp4"
  },
  {
    "timestamp": "00:00 - 00:18",
    "context": "User speaks four times: stating intention to check out (00:00 - 00:03), saying \"不好意思\" (00:11 - 00:13), replying \"嗯沒有了\" (00:13 - 00:15), and thanking \"辛苦了\" (00:16 - 00:18).",
    "question_type": "Counting",
    "question": "How many separate times did the user speak in the clip?",
    "answer": "Four times.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01301.mp4",
    "question_id": "01301_15",
    "clip_path": "clips/01301/01301__0000000_0018500.mp4"
  },
  {
    "timestamp": "00:00 - 00:09",
    "context": "In a brightly lit supermarket, the ambient sound is a low, continuous hum mixed with distant, indistinct music and chatter from other shoppers.",
    "question_type": "Sound Characteristics",
    "question": "What are the acoustic characteristics of the supermarket's ambient sound at the start?",
    "answer": "A low, continuous hum mixed with distant, indistinct music and chatter.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01303.mp4",
    "question_id": "01303_1",
    "clip_path": "clips/01303/01303__0000000_0009500.mp4"
  },
  {
    "timestamp": "00:00 - 00:09",
    "context": "The camera holder and a companion walk towards a moving walkway escalator, the sound of their footsteps briefly audible on the tiled floor.",
    "question_type": "Temporal Information",
    "question": "Are the footsteps continuous or only briefly audible in this segment?",
    "answer": "They are briefly audible.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01303.mp4",
    "question_id": "01303_2",
    "clip_path": "clips/01303/01303__0000000_0009500.mp4"
  },
  {
    "timestamp": "00:16 - 00:26",
    "context": "The user stops and initiates a conversation with an older woman in a red jacket. The sound of their voices is clear and emanates from directly in front of the camera.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From what direction relative to the camera do the voices originate during the conversation with the older woman?",
    "answer": "Directly in front of the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01303.mp4",
    "question_id": "01303_3",
    "clip_path": "clips/01303/01303__0015500_0026500.mp4"
  },
  {
    "timestamp": "00:26 - 00:32",
    "context": "Before the woman can respond, the user's companion, standing to the left, interjects and points forward.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From which side did the companion interject at 00:26 - 00:32?",
    "answer": "From the left side of the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01303.mp4",
    "question_id": "01303_4",
    "clip_path": "clips/01303/01303__0025500_0032500.mp4"
  },
  {
    "timestamp": "00:40 - 00:54",
    "context": "Following the staff member's instructions, the user and companion step onto the moving walkway, accompanied by the onset of a continuous, low-pitched mechanical hum.",
    "question_type": "Sound Source Identification",
    "question": "What produced the continuous, low-pitched mechanical hum heard as they proceed upward?",
    "answer": "The moving walkway escalator.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01303.mp4",
    "question_id": "01303_5",
    "clip_path": "clips/01303/01303__0039500_0054500.mp4"
  },
  {
    "timestamp": "00:40 - 00:54",
    "context": "They step onto the moving walkway. This action is accompanied by the onset of a continuous, low-pitched mechanical hum from the escalator.",
    "question_type": "Temporal Information",
    "question": "When does the mechanical hum begin, and is it sustained?",
    "answer": "It begins as they step onto the moving walkway and continues as they ascend.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01303.mp4",
    "question_id": "01303_6",
    "clip_path": "clips/01303/01303__0039500_0054500.mp4"
  },
  {
    "timestamp": "00:00 - 00:09",
    "context": "Ambient sound: a low, continuous hum mixed with distant, indistinct music and chatter; footsteps briefly audible on the tiled floor.",
    "question_type": "Counting",
    "question": "How many types of non-speech environmental sounds are mentioned in this segment?",
    "answer": "Three: the low continuous hum, distant music, and their footsteps on the tiled floor.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01303.mp4",
    "question_id": "01303_7",
    "clip_path": "clips/01303/01303__0000000_0009500.mp4"
  },
  {
    "timestamp": "00:32 - 00:54",
    "context": "Staff member points toward the escalator and says sanitary products are upstairs and to go up this way. The user and companion then step onto the moving walkway.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the user and companion decide to take the moving walkway?",
    "answer": "Because the staff member told them the sanitary products were upstairs and directed them to go up that way.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01303.mp4",
    "question_id": "01303_8",
    "clip_path": "clips/01303/01303__0031500_0054500.mp4"
  },
  {
    "timestamp": "00:40 - 00:54",
    "context": "As they ascend, the staff member calls out from a distance: '慢点啊' (Be careful).",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the staff member call out '慢点啊' from a distance as they ascended?",
    "answer": "She was cautioning them to be careful as they stepped onto and rode the moving walkway.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01303.mp4",
    "question_id": "01303_9",
    "clip_path": "clips/01303/01303__0039500_0054500.mp4"
  },
  {
    "timestamp": "00:40 - 00:54",
    "context": "Their voices echo slightly in the open space while they travel up the escalator.",
    "question_type": "Sound Characteristics",
    "question": "How do the voices sound acoustically while they ascend on the moving walkway?",
    "answer": "Their voices echo slightly in the open space.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01303.mp4",
    "question_id": "01303_10",
    "clip_path": "clips/01303/01303__0039500_0054500.mp4"
  },
  {
    "timestamp": "00:16 - 00:26",
    "context": "Conversation between the user and an older woman, with a third person asking, '上电梯吗?' (Taking the elevator/escalator?).",
    "question_type": "Counting",
    "question": "How many participants are involved in the conversation during this segment?",
    "answer": "Three: the user, the older woman in the red jacket, and a third person who asks about taking the elevator/escalator.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01303.mp4",
    "question_id": "01303_11",
    "clip_path": "clips/01303/01303__0015500_0026500.mp4"
  },
  {
    "timestamp": "00:40 - 00:54",
    "context": "Stepping onto the moving walkway is accompanied by the onset of a continuous, low-pitched mechanical hum from the escalator.",
    "question_type": "Cross-Modal Reasoning",
    "question": "Using the sound of the continuous, low-pitched mechanical hum, where are the user and companion likely located in the scene at this time?",
    "answer": "On the moving walkway escalator, ascending to the upper floor.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01303.mp4",
    "question_id": "01303_12",
    "clip_path": "clips/01303/01303__0039500_0054500.mp4"
  },
  {
    "timestamp": "00:02 - 00:05",
    "context": "[00:02 - 00:05] As the escalator approaches the top, a woman directly in front of the camera warns, \"小心昂 到了\" (Be careful, we've arrived). Immediately after, her footsteps are heard as she steps off onto the tiled floor.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the woman warn, \"Be careful, we've arrived\"?",
    "answer": "Because they were reaching the top of the escalator and needed to step off safely.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01305.mp4",
    "question_id": "01305_1",
    "clip_path": "clips/01305/01305__0001500_0005500.mp4"
  },
  {
    "timestamp": "00:00 - 00:02",
    "context": "[00:00 - 00:02] The video starts on an ascending escalator producing a continuous, low-pitched mechanical hum and a rhythmic clatter.",
    "question_type": "Sound Source Identification",
    "question": "What generated the continuous hum and rhythmic clatter at the start?",
    "answer": "The ascending escalator.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01305.mp4",
    "question_id": "01305_2",
    "clip_path": "clips/01305/01305__0000000_0002500.mp4"
  },
  {
    "timestamp": "00:00 - 00:02",
    "context": "[00:00 - 00:02] The escalator produces a continuous, low-pitched mechanical hum and a rhythmic clatter.",
    "question_type": "Sound Characteristics",
    "question": "What are the acoustic qualities of the escalator's sound at the beginning?",
    "answer": "A continuous, low-pitched mechanical hum with a rhythmic clatter.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01305.mp4",
    "question_id": "01305_3",
    "clip_path": "clips/01305/01305__0000000_0002500.mp4"
  },
  {
    "timestamp": "00:00 - 00:02",
    "context": "[00:00 - 00:02] The escalator sound is described as continuous during this segment.",
    "question_type": "Temporal Information",
    "question": "Is the escalator's hum continuous or intermittent in this interval?",
    "answer": "Continuous.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01305.mp4",
    "question_id": "01305_4",
    "clip_path": "clips/01305/01305__0000000_0002500.mp4"
  },
  {
    "timestamp": "00:02 - 00:05",
    "context": "[00:02 - 00:05] A woman standing directly in front of the camera speaks a clear, mid-volume warning.",
    "question_type": "Sound Characteristics",
    "question": "What was the volume/clarity of the woman's speech?",
    "answer": "Clear, mid-volume.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01305.mp4",
    "question_id": "01305_5",
    "clip_path": "clips/01305/01305__0001500_0005500.mp4"
  },
  {
    "timestamp": "00:02 - 00:05",
    "context": "[00:02 - 00:05] The woman is standing directly in front of the camera when she speaks.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the woman's speech originate relative to the camera?",
    "answer": "Directly in front of the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01305.mp4",
    "question_id": "01305_6",
    "clip_path": "clips/01305/01305__0001500_0005500.mp4"
  },
  {
    "timestamp": "00:02 - 00:05",
    "context": "[00:02 - 00:05] Immediately after she speaks, the sound of her footsteps is heard as she steps off the escalator onto the tiled floor.",
    "question_type": "Temporal Information",
    "question": "When do the footsteps occur relative to her warning?",
    "answer": "Immediately after she speaks.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01305.mp4",
    "question_id": "01305_7",
    "clip_path": "clips/01305/01305__0001500_0005500.mp4"
  },
  {
    "timestamp": "00:05 - 00:14",
    "context": "[00:05 - 00:14] Ambient audio shifts to faint, distant mall music and indiscernible background chatter emanating from deeper within the store.",
    "question_type": "Counting",
    "question": "How many distinct background sound types are present in this segment (excluding speech and footsteps)?",
    "answer": "Two: faint, distant mall music and indiscernible background chatter.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01305.mp4",
    "question_id": "01305_8",
    "clip_path": "clips/01305/01305__0004500_0014500.mp4"
  },
  {
    "timestamp": "00:05 - 00:14",
    "context": "[00:05 - 00:14] The mall music and background chatter are described as emanating from deeper within the store.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where do the ambient mall music and chatter originate relative to the camera?",
    "answer": "From deeper within the store, farther away from the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01305.mp4",
    "question_id": "01305_9",
    "clip_path": "clips/01305/01305__0004500_0014500.mp4"
  },
  {
    "timestamp": "00:00 - 00:13",
    "context": "[00:00] A clear, moderate-volume female voice begins a monologue explaining the purpose of the journey; the user narrates as she moves forward down a well-lit hallway.",
    "question_type": "Sound Source Identification",
    "question": "Who is producing the speech heard at the start of the video?",
    "answer": "The female narrator—the user—is speaking.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01313.mp4",
    "question_id": "01313_1",
    "clip_path": "clips/01313/01313__0000000_0013500.mp4"
  },
  {
    "timestamp": "00:00 - 00:13",
    "context": "A clear, moderate-volume female voice narrates the scene while the camera moves forward.",
    "question_type": "Sound Characteristics",
    "question": "How would you describe the clarity and volume of the narrator’s voice?",
    "answer": "It is clear and moderate in volume.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01313.mp4",
    "question_id": "01313_2",
    "clip_path": "clips/01313/01313__0000000_0013500.mp4"
  },
  {
    "timestamp": "00:00 - 00:13",
    "context": "[00:00] The narration starts as the video begins, introducing the purpose of the journey.",
    "question_type": "Temporal Information",
    "question": "When does the female monologue start in this clip?",
    "answer": "Right at the beginning, at 00:00.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01313.mp4",
    "question_id": "01313_3",
    "clip_path": "clips/01313/01313__0000000_0013500.mp4"
  },
  {
    "timestamp": "00:00 - 00:13",
    "context": "The narrator says she is visually impaired and completely blind and that autumn is almost over; she notes she has never been to Wutong Avenue and went out with her white cane.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Based on her monologue, why did she decide to go out to Wutong Avenue for an autumn outing?",
    "answer": "Because autumn was almost over and she didn’t want to waste the beautiful autumn days, having never been to Wutong Avenue.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01313.mp4",
    "question_id": "01313_4",
    "clip_path": "clips/01313/01313__0000000_0013500.mp4"
  },
  {
    "timestamp": "00:00 - 00:13",
    "context": "The narrator identifies herself as visually impaired and completely blind and mentions going out with her white cane.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why is the narrator using a white cane during this journey?",
    "answer": "Because she is visually impaired and completely blind.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01313.mp4",
    "question_id": "01313_5",
    "clip_path": "clips/01313/01313__0000000_0013500.mp4"
  },
  {
    "timestamp": "00:00 - 00:13",
    "context": "The narration mentions going out with a white cane; concurrently, the lower right of the frame shows a white cane being used for navigation.",
    "question_type": "Cross-Modal Reasoning",
    "question": "What visual evidence confirms the narrator’s claim about using a white cane?",
    "answer": "In the lower right of the frame, a white cane is visible being used for navigation.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01313.mp4",
    "question_id": "01313_6",
    "clip_path": "clips/01313/01313__0000000_0013500.mp4"
  },
  {
    "timestamp": "00:00 - 00:13",
    "context": "The narrator talks about taking the subway alone; visually, the user walks past digital art displays and then enters a spacious, brightly lit subway station with a reflective floor.",
    "question_type": "Cross-Modal Reasoning",
    "question": "After the narrator mentions taking the subway, what place does the video show she enters?",
    "answer": "A spacious, brightly lit subway station with a reflective floor.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01313.mp4",
    "question_id": "01313_7",
    "clip_path": "clips/01313/01313__0000000_0013500.mp4"
  },
  {
    "timestamp": "00:00 - 00:13",
    "context": "Throughout this segment, only one clear, moderate-volume female voice is heard narrating.",
    "question_type": "Counting",
    "question": "How many distinct speakers are heard in this segment?",
    "answer": "One—the female narrator.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01313.mp4",
    "question_id": "01313_8",
    "clip_path": "clips/01313/01313__0000000_0013500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "The narrator gives a voiceover praising the subway staff: \"The subway station staff are really super enthusiastic. Not only did they bring me to the exact location, but they also told me how to give directions to my friend.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "What prompted the narrator's appreciation for the subway station staff in the voiceover?",
    "answer": "They brought her to the exact location and explained how to give directions to her friend.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01315.mp4",
    "question_id": "01315_1",
    "clip_path": "clips/01315/01315__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "An egocentric narrator provides a voiceover in a clear, appreciative female voice.",
    "question_type": "Sound Characteristics",
    "question": "What was the tone and quality of the narrator's voice in the voiceover?",
    "answer": "A clear, appreciative female voice.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01315.mp4",
    "question_id": "01315_2",
    "clip_path": "clips/01315/01315__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:06 - 00:09",
    "context": "A male volunteer nearby says, \"Now you just go to this exit side.\" The camera wearer replies, \"Okay.\" The dialogue confirms he has just finished guiding her.",
    "question_type": "Sound Source Identification",
    "question": "Who gave the instruction, \"Now you just go to this exit side\"?",
    "answer": "A male volunteer standing nearby.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01315.mp4",
    "question_id": "01315_3",
    "clip_path": "clips/01315/01315__0005500_0009500.mp4"
  },
  {
    "timestamp": "00:06 - 00:09",
    "context": "The volunteer gives final directions after guiding the camera wearer: \"Now you just go to this exit side.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the volunteer provide the instruction to go to the exit side at this moment?",
    "answer": "Because he had just finished guiding the camera wearer to her destination and was giving final directions.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01315.mp4",
    "question_id": "01315_4",
    "clip_path": "clips/01315/01315__0005500_0009500.mp4"
  },
  {
    "timestamp": "00:09 - 00:13",
    "context": "The camera wearer says, \"It's okay, I'll call him, and then he'll come.\" The volunteer acknowledges. She thanks him, and he adds, \"It's okay, you can go back first.\"",
    "question_type": "Temporal Information",
    "question": "When did the volunteer say, \"It's okay, you can go back first\"?",
    "answer": "Between 00:09 and 00:13, after the camera wearer said she would call her friend.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01315.mp4",
    "question_id": "01315_5",
    "clip_path": "clips/01315/01315__0008500_0013500.mp4"
  },
  {
    "timestamp": "00:13 - 00:17",
    "context": "As the volunteer turns and walks away, he calls back from a short distance, \"It's right here at Exit 1.\" The camera wearer replies, \"Okay, okay, okay, thank you.\"",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From what distance relative to the camera did the volunteer say, \"It's right here at Exit 1\"?",
    "answer": "From a few meters away.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01315.mp4",
    "question_id": "01315_6",
    "clip_path": "clips/01315/01315__0012500_0017500.mp4"
  },
  {
    "timestamp": "00:13 - 00:17",
    "context": "The camera wearer responds, \"Okay, okay, okay, thank you,\" after the volunteer's final information.",
    "question_type": "Counting",
    "question": "How many times did the camera wearer say \"Okay\" before thanking the volunteer?",
    "answer": "Three times.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01315.mp4",
    "question_id": "01315_7",
    "clip_path": "clips/01315/01315__0012500_0017500.mp4"
  },
  {
    "timestamp": "00:09 - 00:13",
    "context": "The camera wearer states, \"It's okay, I'll call him, and then he'll come.\" The volunteer acknowledges.",
    "question_type": "Inferential & Contextual Causality",
    "question": "What was the intended purpose of the camera wearer's statement, \"I'll call him, and then he'll come\"?",
    "answer": "To indicate she would call her friend so he would come meet her.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01315.mp4",
    "question_id": "01315_8",
    "clip_path": "clips/01315/01315__0008500_0013500.mp4"
  },
  {
    "timestamp": "00:18 - 00:26",
    "context": "Friends arrive; a brief, overlapping exchange of goodbyes occurs as they collectively thank the volunteer. The camera wearer says, \"Okay, okay,\" and then, \"Okay, bye-bye.\"",
    "question_type": "Temporal Information",
    "question": "When did the overlapping exchange of goodbyes occur, and how is it characterized?",
    "answer": "Between 00:18 and 00:26; it was brief and overlapping.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01315.mp4",
    "question_id": "01315_9",
    "clip_path": "clips/01315/01315__0017500_0026500.mp4"
  },
  {
    "timestamp": "00:18 - 00:26",
    "context": "As a man in a black jacket and others join, they collectively thank the volunteer and say goodbye.",
    "question_type": "Sound Source Identification",
    "question": "Who collectively thanked the volunteer at this point?",
    "answer": "The camera wearer and her arriving friends, including a man in a black jacket.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01315.mp4",
    "question_id": "01315_10",
    "clip_path": "clips/01315/01315__0017500_0026500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "[00:00 - 00:06] The user walks along an avenue, accompanied by the faint, continuous sound of distant traffic.",
    "question_type": "Temporal Information",
    "question": "Is the background traffic noise brief or continuous during 00:00 - 00:06?",
    "answer": "It is continuous.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01316.mp4",
    "question_id": "01316_1",
    "clip_path": "clips/01316/01316__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "[00:00 - 00:06] A female voice, originating from the user, narrates in a clear, moderate tone.",
    "question_type": "Sound Characteristics",
    "question": "What are the tone and volume characteristics of the user's voice during the initial narration?",
    "answer": "Clear, moderate tone.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01316.mp4",
    "question_id": "01316_2",
    "clip_path": "clips/01316/01316__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:11",
    "context": "[00:00 - 00:06] The user mentions being unable to see and asks how she experiences the beauty before her. [00:06 - 00:11] She says she will take out her 'secret weapon... Be My Eyes.'",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why does the user decide to use 'Be My Eyes' at 00:06 - 00:11?",
    "answer": "Because she is visually impaired and wants help experiencing or describing the beauty before her.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01316.mp4",
    "question_id": "01316_3",
    "clip_path": "clips/01316/01316__0000000_0011500.mp4"
  },
  {
    "timestamp": "00:06 - 00:11",
    "context": "[00:06 - 00:11] As the user's thumb touches the phone screen, the screen reader emits soft taps and a synthesized voice says '相机' (Camera).",
    "question_type": "Sound Source Identification",
    "question": "What generated the series of soft taps and the synthesized '相机' announcement?",
    "answer": "The smartphone’s screen reader responding to the user’s touch while launching the app.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01316.mp4",
    "question_id": "01316_4",
    "clip_path": "clips/01316/01316__0005500_0011500.mp4"
  },
  {
    "timestamp": "00:06 - 00:11",
    "context": "[00:06 - 00:11] The screen reader announces '相机' (Camera) as the user interacts with the phone.",
    "question_type": "Temporal Information",
    "question": "During which time interval did the screen reader announce '相机' (Camera)?",
    "answer": "Between 00:06 and 00:11.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01316.mp4",
    "question_id": "01316_5",
    "clip_path": "clips/01316/01316__0005500_0011500.mp4"
  },
  {
    "timestamp": "00:06 - 00:17",
    "context": "[00:06 - 00:11] The screen reader says '相机' (Camera). [00:11 - 00:17] It then announces '拍照按钮' (Photo button) and 'Be My Eyes.'",
    "question_type": "Counting",
    "question": "How many distinct screen reader phrases are explicitly heard between 00:06 and 00:17, and what are they?",
    "answer": "Three: '相机' (Camera), '拍照按钮' (Photo button), and 'Be My Eyes.'",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01316.mp4",
    "question_id": "01316_6",
    "clip_path": "clips/01316/01316__0005500_0017500.mp4"
  },
  {
    "timestamp": "00:11 - 00:17",
    "context": "[00:11 - 00:17] After pointing the phone forward and tapping the screen, a sharp, digital camera shutter sound is heard, confirming a picture was taken.",
    "question_type": "Sound Source Identification",
    "question": "What produced the sharp camera shutter sound heard toward the end of 00:11 - 00:17?",
    "answer": "The smartphone taking a photo in the app.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01316.mp4",
    "question_id": "01316_7",
    "clip_path": "clips/01316/01316__0010500_0017500.mp4"
  },
  {
    "timestamp": "00:11 - 00:17",
    "context": "[00:11 - 00:17] A sharp, digital camera shutter sound follows the user's tap.",
    "question_type": "Sound Characteristics",
    "question": "What is the quality of the camera shutter sound?",
    "answer": "Sharp and digital.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01316.mp4",
    "question_id": "01316_8",
    "clip_path": "clips/01316/01316__0010500_0017500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "[00:00 - 00:06] The avenue is accompanied by the faint, continuous sound of distant traffic.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "How far from the camera does the traffic sound seem to be?",
    "answer": "Distant and ambient, with no specific direction indicated.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01316.mp4",
    "question_id": "01316_9",
    "clip_path": "clips/01316/01316__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:11 - 00:17",
    "context": "[00:11 - 00:17] The user taps the screen and a single, sharp shutter sound is heard, confirming a photo was taken for description.",
    "question_type": "Counting",
    "question": "How many times is a camera shutter sound heard in this segment?",
    "answer": "Once.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01316.mp4",
    "question_id": "01316_10",
    "clip_path": "clips/01316/01316__0010500_0017500.mp4"
  },
  {
    "timestamp": "00:11 - 00:17",
    "context": "[00:11 - 00:17] The user lifts the phone, points it forward, taps the screen, and a shutter sound confirms a photo to get a description of the scenery.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Based on the surrounding events, why did the shutter sound occur when the user tapped the screen?",
    "answer": "Because the user took a photo to obtain a description of the avenue scenery via the app.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01316.mp4",
    "question_id": "01316_11",
    "clip_path": "clips/01316/01316__0010500_0017500.mp4"
  },
  {
    "timestamp": "00:00 - 00:12",
    "context": "[00:00 - 00:12] As they walk along a subway platform, a male asks, \"Where are we going? To Xinjiekou?\" The female confirms and they clarify they need to transfer to get to Muxuyuan.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the male voice ask, \"Where are we going? To Xinjiekou?\"",
    "answer": "He was clarifying their route because they planned to transfer trains to reach Muxuyuan.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01319.mp4",
    "question_id": "01319_1",
    "clip_path": "clips/01319/01319__0000000_0012500.mp4"
  },
  {
    "timestamp": "00:00 - 00:12",
    "context": "[00:00 - 00:12] They walk along the platform discussing their route. The sounds of their footsteps are audible.",
    "question_type": "Sound Source Identification",
    "question": "What generated the audible footsteps in this segment?",
    "answer": "The camera holder and their companion walking along the platform.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01319.mp4",
    "question_id": "01319_2",
    "clip_path": "clips/01319/01319__0000000_0012500.mp4"
  },
  {
    "timestamp": "00:12 - 00:15",
    "context": "[00:12 - 00:15] A train arrives. A high-pitched, repetitive chime sounds, immediately followed by the loud, mechanical sliding of the platform screen doors and train doors opening in front of the camera holder.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the door-opening sound originate relative to the camera holder?",
    "answer": "Directly in front of the camera holder.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01319.mp4",
    "question_id": "01319_3",
    "clip_path": "clips/01319/01319__0011500_0015500.mp4"
  },
  {
    "timestamp": "00:12 - 00:15",
    "context": "[00:12 - 00:15] A high-pitched, repetitive chime sounds, immediately followed by the loud, mechanical sliding sound of doors opening.",
    "question_type": "Temporal Information",
    "question": "What sound occurred immediately after the high-pitched, repetitive chime?",
    "answer": "The loud, mechanical sliding of the platform screen doors and train doors opening.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01319.mp4",
    "question_id": "01319_4",
    "clip_path": "clips/01319/01319__0011500_0015500.mp4"
  },
  {
    "timestamp": "00:19 - 00:23",
    "context": "[00:19 - 00:23] After boarding, the companion says, \"Okay, thank you\" to a station staff member on the platform, suggesting they received brief directions.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the companion say, \"Okay, thank you\" to the station staff member?",
    "answer": "Because they had just received brief directions before entering the train.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01319.mp4",
    "question_id": "01319_5",
    "clip_path": "clips/01319/01319__0018500_0023500.mp4"
  },
  {
    "timestamp": "00:19 - 00:23",
    "context": "[00:19 - 00:23] The interaction is quickly cut off by a distinct warning chime.",
    "question_type": "Temporal Information",
    "question": "What immediately cut off the brief interaction with the station staff member?",
    "answer": "A distinct warning chime.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01319.mp4",
    "question_id": "01319_6",
    "clip_path": "clips/01319/01319__0018500_0023500.mp4"
  },
  {
    "timestamp": "00:27 - 00:29",
    "context": "[00:27 - 00:29] A mechanical hiss followed by a soft thud indicates the train doors are closing.",
    "question_type": "Sound Characteristics",
    "question": "How are the train door-closing sounds described?",
    "answer": "A mechanical hiss followed by a soft thud.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01319.mp4",
    "question_id": "01319_7",
    "clip_path": "clips/01319/01319__0026500_0029500.mp4"
  },
  {
    "timestamp": "00:27 - 00:29",
    "context": "[00:27 - 00:29] Hiss and thud indicate closing, then the doors immediately slide open again with another mechanical sound.",
    "question_type": "Counting",
    "question": "How many distinct door-operation sound events occur in this interval, and what are they?",
    "answer": "Three: a mechanical hiss, a soft thud (closing), and another mechanical sliding sound as the doors reopen.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01319.mp4",
    "question_id": "01319_8",
    "clip_path": "clips/01319/01319__0026500_0029500.mp4"
  },
  {
    "timestamp": "00:27 - 00:29",
    "context": "[00:27 - 00:29] After closing sounds, the doors immediately slide open again, indicating a change of plans or a mistake in boarding.",
    "question_type": "Inferential & Contextual Causality",
    "question": "What did the immediate reopening of the doors indicate?",
    "answer": "A change of plans or a mistake in boarding.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01319.mp4",
    "question_id": "01319_9",
    "clip_path": "clips/01319/01319__0026500_0029500.mp4"
  },
  {
    "timestamp": "00:29 - 00:44",
    "context": "[00:29 - 00:44] A station staff member in a yellow vest approaches the open door and addresses the camera holder, noticing their white cane: \"Hey, you are visually impaired, right?\"",
    "question_type": "Cross-Modal Reasoning",
    "question": "What visual cue did the staff member reference when addressing the camera holder, and what did it imply?",
    "answer": "The staff member referenced the camera holder’s white cane, implying they are visually impaired.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01319.mp4",
    "question_id": "01319_10",
    "clip_path": "clips/01319/01319__0028500_0044167.mp4"
  },
  {
    "timestamp": "00:29 - 00:44",
    "context": "[00:29 - 00:44] The staff member confirms their destination, gives directions, and offers, \"I'll take you there.\" The camera holder disembarks and follows the staff member along the platform.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the camera holder disembark and follow the station staff member?",
    "answer": "Because the staff member offered to guide them to their transfer toward Muxuyuan.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01319.mp4",
    "question_id": "01319_11",
    "clip_path": "clips/01319/01319__0028500_0044167.mp4"
  },
  {
    "timestamp": "00:29 - 00:44",
    "context": "[00:29 - 00:44] The camera holder follows the staff member along the platform. The sound of their footsteps echoes in the station.",
    "question_type": "Sound Characteristics",
    "question": "How are the footsteps described acoustically as they walk with the staff member?",
    "answer": "The footsteps echo in the station.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01319.mp4",
    "question_id": "01319_12",
    "clip_path": "clips/01319/01319__0028500_0044167.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "[00:00 - 00:07] The visually impaired user follows a subway staff member and calls out in a clear, forward-projected voice: \"Sir... Sir, sir... hey... you hold the white cane... I'm nervous,\" to get his attention.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the user project her voice while calling out to the subway staff member?",
    "answer": "To get his attention.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01320.mp4",
    "question_id": "01320_1",
    "clip_path": "clips/01320/01320__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:07 - 00:13",
    "context": "[00:07 - 00:13] A loud, repetitive, high-pitched electronic beeping begins, originating from the subway train to the right, serving as an audible warning that the platform screen doors are about to operate.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the loud electronic beeping start between 00:07 and 00:13?",
    "answer": "It served as an audible warning that the platform screen doors were about to operate.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01320.mp4",
    "question_id": "01320_2",
    "clip_path": "clips/01320/01320__0006500_0013500.mp4"
  },
  {
    "timestamp": "00:07 - 00:13",
    "context": "[00:07 - 00:13] A loud, repetitive, high-pitched electronic beeping begins, originating from the subway train to the right.",
    "question_type": "Sound Source Identification",
    "question": "What generated the loud, repetitive high-pitched beeping?",
    "answer": "The subway train to the right.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01320.mp4",
    "question_id": "01320_3",
    "clip_path": "clips/01320/01320__0006500_0013500.mp4"
  },
  {
    "timestamp": "00:07 - 00:13",
    "context": "[00:07 - 00:13] A loud, repetitive, high-pitched electronic beeping is heard while the user continues speaking.",
    "question_type": "Sound Characteristics",
    "question": "What are the acoustic characteristics of the beeping sound?",
    "answer": "It is loud, repetitive, and high-pitched with an electronic timbre.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01320.mp4",
    "question_id": "01320_4",
    "clip_path": "clips/01320/01320__0006500_0013500.mp4"
  },
  {
    "timestamp": "00:07 - 00:13",
    "context": "[00:07 - 00:13] The beeping originates from the subway train to the right.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From which direction relative to the camera does the beeping come?",
    "answer": "From the right.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01320.mp4",
    "question_id": "01320_5",
    "clip_path": "clips/01320/01320__0006500_0013500.mp4"
  },
  {
    "timestamp": "00:07 - 00:13",
    "context": "[00:07 - 00:13] A persistent beeping begins and continues through this interval.",
    "question_type": "Temporal Information",
    "question": "When does the beeping start and how does it persist over the interval?",
    "answer": "It starts at 00:07 and persists continuously through 00:13.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01320.mp4",
    "question_id": "01320_6",
    "clip_path": "clips/01320/01320__0006500_0013500.mp4"
  },
  {
    "timestamp": "00:07 - 00:13",
    "context": "[00:07 - 00:13] Over the persistent beeping, the user continues her explanation in a calm, conversational tone.",
    "question_type": "Sound Characteristics",
    "question": "What tone does the user use while speaking over the beeping?",
    "answer": "A calm, conversational tone.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01320.mp4",
    "question_id": "01320_7",
    "clip_path": "clips/01320/01320__0006500_0013500.mp4"
  },
  {
    "timestamp": "00:00 - 00:13",
    "context": "[00:00 - 00:07] The user extends her white cane toward the staff member, indicating a preference to be guided by the cane rather than direct physical contact. [00:07 - 00:13] She explains verbally how to hold it and why.",
    "question_type": "Cross-Modal Reasoning",
    "question": "What visual action corresponds to the user's verbal request about how to guide her?",
    "answer": "She extends her white cane toward the staff member to be guided by the cane instead of direct physical contact.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01320.mp4",
    "question_id": "01320_8",
    "clip_path": "clips/01320/01320__0000000_0013500.mp4"
  },
  {
    "timestamp": "00:07 - 00:13",
    "context": "[00:07 - 00:13] Over the beeping, the user says: \"Because it gets dirty from being moved around on the ground, so it's better if you hold it like this,\" clarifying her motivation.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why does the user prefer the staff member to guide her by holding the cane rather than through direct physical contact?",
    "answer": "For hygiene reasons—the cane gets dirty from the ground, so she prefers it be held in that manner.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01320.mp4",
    "question_id": "01320_9",
    "clip_path": "clips/01320/01320__0006500_0013500.mp4"
  },
  {
    "timestamp": "00:07 - 00:13",
    "context": "[00:07 - 00:13] Two concurrent sounds are present: the persistent electronic beeping and the user's speech.",
    "question_type": "Counting",
    "question": "How many distinct sound sources are audible simultaneously during this interval?",
    "answer": "Two: the electronic beeping and the user's speech.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01320.mp4",
    "question_id": "01320_10",
    "clip_path": "clips/01320/01320__0006500_0013500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "A person walks through dry autumn leaves, creating a soft, continuous rustling. A clear, medium-volume female voice says: “哇, 我终于找到了一个可以踩叶子的地方” (Wow, I finally found a place where I can step on leaves). This statement reveals the motivation for being in this specific location.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Based on her statement, what is the likely reason for being in this wooded park location?",
    "answer": "She is there because she finally found a place where she can step on leaves and enjoy the leaf-crunching experience.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01321.mp4",
    "question_id": "01321_1",
    "clip_path": "clips/01321/01321__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "A person walks through dry, fallen autumn leaves, creating a soft, continuous rustling sound with each step.",
    "question_type": "Sound Characteristics",
    "question": "What are the acoustic qualities of the footstep-induced rustling at the start?",
    "answer": "It is soft and continuous.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01321.mp4",
    "question_id": "01321_2",
    "clip_path": "clips/01321/01321__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:03 - 00:07",
    "context": "While continuing to walk, the person says, “稍等我来蹲一下” (Wait a moment, let me squat down). As they hold a red maple leaf, they add, “我要摸一摸” (I want to touch it).",
    "question_type": "Counting",
    "question": "How many distinct spoken statements are made in this interval?",
    "answer": "Two statements.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01321.mp4",
    "question_id": "01321_3",
    "clip_path": "clips/01321/01321__0002500_0007500.mp4"
  },
  {
    "timestamp": "00:03 - 00:07",
    "context": "Their left hand, wearing a green bracelet, picks up a red maple leaf. As they hold it, a very faint, soft rustling is heard as the leaf is handled.",
    "question_type": "Sound Source Identification",
    "question": "What generated the very faint, soft rustling heard in this segment?",
    "answer": "Handling the red maple leaf (picking it up and touching it).",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01321.mp4",
    "question_id": "01321_4",
    "clip_path": "clips/01321/01321__0002500_0007500.mp4"
  },
  {
    "timestamp": "00:08 - 00:12",
    "context": "The person squats and grabs a large handful of dry leaves from the ground, producing a loud, crisp, sustained crunching sound very close to the microphone.",
    "question_type": "Sound Characteristics",
    "question": "What are the quality and loudness characteristics of the crunching produced by the grabbed leaves?",
    "answer": "It is a loud, crisp, and sustained crunching sound.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01321.mp4",
    "question_id": "01321_5",
    "clip_path": "clips/01321/01321__0007500_0012500.mp4"
  },
  {
    "timestamp": "00:08 - 00:12",
    "context": "Grabbing a large handful of dry leaves produces a loud, crisp, sustained crunching sound very close to the microphone.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the crunching originate relative to the recording device?",
    "answer": "Very close to the microphone.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01321.mp4",
    "question_id": "01321_6",
    "clip_path": "clips/01321/01321__0007500_0012500.mp4"
  },
  {
    "timestamp": "00:08 - 00:12",
    "context": "After grabbing the leaves, the person says, “这个声音特别解压” (This sound is so stress-relieving), confirming the action was performed specifically to create and hear this sound.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the person grab a large handful of dry leaves?",
    "answer": "To create and hear the stress-relieving crunching sound.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01321.mp4",
    "question_id": "01321_7",
    "clip_path": "clips/01321/01321__0007500_0012500.mp4"
  },
  {
    "timestamp": "00:12 - 00:16",
    "context": "They say, “我上一次去那个南京图书馆, 我说那个地方有叶子踩在地上” and a faint, intermittent rustling can be heard in the background as they speak.",
    "question_type": "Temporal Information",
    "question": "Is the background leaf rustling continuous or intermittent, and when is it audible?",
    "answer": "It is faint and intermittent, heard as they speak during this interval.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01321.mp4",
    "question_id": "01321_8",
    "clip_path": "clips/01321/01321__0011500_0016300.mp4"
  },
  {
    "timestamp": "00:12 - 00:16",
    "context": "As they speak, a faint, intermittent rustling can be heard, suggesting minor movements of the leaves in their hand.",
    "question_type": "Sound Source Identification",
    "question": "What is the suggested source of the faint rustling heard while they speak?",
    "answer": "Minor movements of the leaves in their hand.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01321.mp4",
    "question_id": "01321_9",
    "clip_path": "clips/01321/01321__0011500_0016300.mp4"
  },
  {
    "timestamp": "00:00 - 00:05",
    "context": "[00:00 - 00:05] A series of high-pitched melodic chimes emanates from the station's PA system, signaling a train's arrival; immediately after, a loud, low-frequency rumble and metallic screech of a train pulling in on the opposite track.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the melodic chimes play at the start of the clip?",
    "answer": "They signaled a train's arrival.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01323.mp4",
    "question_id": "01323_1",
    "clip_path": "clips/01323/01323__0000000_0005500.mp4"
  },
  {
    "timestamp": "00:00 - 00:05",
    "context": "[00:00 - 00:05] High-pitched melodic chimes are heard from the station's public address system.",
    "question_type": "Sound Source Identification",
    "question": "What system generated the melodic chimes heard at the beginning?",
    "answer": "The subway station's public address (PA) system.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01323.mp4",
    "question_id": "01323_2",
    "clip_path": "clips/01323/01323__0000000_0005500.mp4"
  },
  {
    "timestamp": "00:00 - 00:05",
    "context": "[00:00 - 00:05] A train pulls into the station with a loud, low-frequency rumble and metallic screech.",
    "question_type": "Sound Characteristics",
    "question": "How are the sounds of the arriving train described?",
    "answer": "A loud, low-frequency rumble and a metallic screech.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01323.mp4",
    "question_id": "01323_3",
    "clip_path": "clips/01323/01323__0000000_0005500.mp4"
  },
  {
    "timestamp": "00:00 - 00:05",
    "context": "[00:00 - 00:05] The user is inside a crowded train car as another train pulls in on the opposite track.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the arriving train pull in relative to the user's position?",
    "answer": "On the opposite track from the user inside the train car.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01323.mp4",
    "question_id": "01323_4",
    "clip_path": "clips/01323/01323__0000000_0005500.mp4"
  },
  {
    "timestamp": "00:00 - 00:05",
    "context": "[00:00 - 00:05] Chimes from the PA system are immediately followed by the train’s rumble and metallic screech.",
    "question_type": "Temporal Information",
    "question": "What was the timing relationship between the chimes and the train arrival sounds?",
    "answer": "The train sounds began immediately after the chimes.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01323.mp4",
    "question_id": "01323_5",
    "clip_path": "clips/01323/01323__0000000_0005500.mp4"
  },
  {
    "timestamp": "00:05 - 00:07",
    "context": "[00:05 - 00:07] A pre-recorded female voice makes an announcement in Mandarin over the station's PA system.",
    "question_type": "Temporal Information",
    "question": "When does the pre-recorded female announcement occur?",
    "answer": "Between 00:05 and 00:07.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01323.mp4",
    "question_id": "01323_6",
    "clip_path": "clips/01323/01323__0004500_0007500.mp4"
  },
  {
    "timestamp": "00:05 - 00:07",
    "context": "[00:05 - 00:07] A pre-recorded female voice announces over the station's PA system.",
    "question_type": "Sound Source Identification",
    "question": "What was the source of the female announcement?",
    "answer": "The station's PA system.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01323.mp4",
    "question_id": "01323_7",
    "clip_path": "clips/01323/01323__0004500_0007500.mp4"
  },
  {
    "timestamp": "00:05 - 00:09",
    "context": "[00:05 - 00:07] A pre-recorded female announcement is heard. [00:07 - 00:09] A male passenger near the user instructs the crowd in Mandarin.",
    "question_type": "Counting",
    "question": "How many distinct voices make announcements or instructions between 00:05 and 00:09?",
    "answer": "Two—one pre-recorded female voice and one male passenger.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01323.mp4",
    "question_id": "01323_8",
    "clip_path": "clips/01323/01323__0004500_0009367.mp4"
  },
  {
    "timestamp": "00:07 - 00:09",
    "context": "[00:07 - 00:09] A male passenger standing near the user speaks, instructing the crowd: '先下后上' (Let people get off first, then get on).",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the directive '先下后上' originate relative to the camera?",
    "answer": "From a male passenger standing near the user (close proximity).",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01323.mp4",
    "question_id": "01323_9",
    "clip_path": "clips/01323/01323__0006500_0009367.mp4"
  },
  {
    "timestamp": "00:07 - 00:09",
    "context": "[00:07 - 00:09] A male passenger near the user instructs the crowd: '先下后上' (Let people get off first, then get on).",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the male passenger issue the instruction '先下后上'?",
    "answer": "To enforce proper boarding etiquette as people prepared to exit and enter the train.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01323.mp4",
    "question_id": "01323_10",
    "clip_path": "clips/01323/01323__0006500_0009367.mp4"
  },
  {
    "timestamp": "00:07 - 00:09",
    "context": "[00:07 - 00:09] The male passenger speaks in a clear, assertive voice.",
    "question_type": "Sound Characteristics",
    "question": "What were the acoustic qualities of the male passenger’s speech?",
    "answer": "Clear and assertive.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01323.mp4",
    "question_id": "01323_11",
    "clip_path": "clips/01323/01323__0006500_0009367.mp4"
  },
  {
    "timestamp": "00:00 - 00:09",
    "context": "[00:00 - 00:09] A security guard, standing directly in front of the camera, says: \"Next time you come, enter through this door. Then you'll be right in. Just come straight in. Don't use the southeast door.\" He then turns and leads the user down the hallway.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the guard advise using a specific door instead of the southeast door?",
    "answer": "So the user could come straight in and be “right in,” making entry more direct and avoiding the southeast door.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01331.mp4",
    "question_id": "01331_1",
    "clip_path": "clips/01331/01331__0000000_0009500.mp4"
  },
  {
    "timestamp": "00:00 - 00:09",
    "context": "[00:00 - 00:09] A security guard, standing directly in front of the camera in a brightly lit hallway, speaks clearly and gives instructions.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the guard’s clear instructions originate relative to the camera?",
    "answer": "Directly in front of the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01331.mp4",
    "question_id": "01331_2",
    "clip_path": "clips/01331/01331__0000000_0009500.mp4"
  },
  {
    "timestamp": "00:09 - 00:15",
    "context": "[00:09 - 00:15] The guard uses his left hand to push aside a thick, transparent plastic curtain, which produces a soft, continuous rustling sound.",
    "question_type": "Sound Source Identification",
    "question": "What object produced the rustling sound at the doorway?",
    "answer": "The thick, transparent plastic curtain being pushed aside.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01331.mp4",
    "question_id": "01331_3",
    "clip_path": "clips/01331/01331__0008500_0015500.mp4"
  },
  {
    "timestamp": "00:09 - 00:15",
    "context": "[00:09 - 00:15] Pushing aside the plastic curtain produces a soft, continuous rustling sound.",
    "question_type": "Sound Characteristics",
    "question": "How is the curtain’s rustling sound described?",
    "answer": "Soft and continuous.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01331.mp4",
    "question_id": "01331_4",
    "clip_path": "clips/01331/01331__0008500_0015500.mp4"
  },
  {
    "timestamp": "00:15 - 00:33",
    "context": "[00:15 - 00:33] Throughout this segment, a faint, high-pitched jingling sound is intermittently audible, originating from a small bell on a bunny keychain attached to the user's white cane.",
    "question_type": "Temporal Information",
    "question": "Over this interval, is the jingling continuous or intermittent?",
    "answer": "Intermittent throughout 00:15–00:33.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01331.mp4",
    "question_id": "01331_5",
    "clip_path": "clips/01331/01331__0014500_0033500.mp4"
  },
  {
    "timestamp": "00:15 - 00:33",
    "context": "[00:15 - 00:33] A faint, high-pitched jingling is intermittently audible, originating from a small bell on a bunny keychain attached to the user's white cane.",
    "question_type": "Sound Source Identification",
    "question": "What generated the faint jingling heard in this segment?",
    "answer": "A small bell on a bunny keychain attached to the user's white cane.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01331.mp4",
    "question_id": "01331_6",
    "clip_path": "clips/01331/01331__0014500_0033500.mp4"
  },
  {
    "timestamp": "00:15 - 00:33",
    "context": "[00:15 - 00:33] The jingling is described as faint and high-pitched.",
    "question_type": "Sound Characteristics",
    "question": "What are the acoustic qualities of the jingling sound?",
    "answer": "Faint and high-pitched.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01331.mp4",
    "question_id": "01331_7",
    "clip_path": "clips/01331/01331__0014500_0033500.mp4"
  },
  {
    "timestamp": "00:15 - 00:33",
    "context": "[00:15 - 00:33] From a distance, a female staff member greets with \"Hello,\" and the user replies, \"Hello.\"",
    "question_type": "Counting",
    "question": "How many times is the word \"Hello\" spoken in this exchange?",
    "answer": "Twice—once by the staff member and once by the user.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01331.mp4",
    "question_id": "01331_8",
    "clip_path": "clips/01331/01331__0014500_0033500.mp4"
  },
  {
    "timestamp": "00:00 - 00:15",
    "context": "[00:00 - 00:09] The guard gives door instructions and then leads the user down the hallway. [00:09 - 00:15] He reaches a doorway, holds a plastic curtain aside, and says, \"You can go in here.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the guard lead the user down the hallway after giving instructions?",
    "answer": "To guide the user to the doorway of the room they could enter.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01331.mp4",
    "question_id": "01331_9",
    "clip_path": "clips/01331/01331__0000000_0015500.mp4"
  },
  {
    "timestamp": "00:09 - 00:33",
    "context": "[00:09 - 00:15] The user says, \"Okay, thank you.\" [00:15 - 00:33] After entering the room, the user thanks the guard again.",
    "question_type": "Counting",
    "question": "How many times did the user thank the guard?",
    "answer": "Twice.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01331.mp4",
    "question_id": "01331_10",
    "clip_path": "clips/01331/01331__0008500_0033500.mp4"
  },
  {
    "timestamp": "00:33 - 00:55",
    "context": "[00:33 - 00:55] The staff member offers to issue a library card for free and then asks for the user's disability certificate and ID card.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the staff member request the user’s disability certificate and ID card?",
    "answer": "To process the registration and issue a free library card.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01331.mp4",
    "question_id": "01331_11",
    "clip_path": "clips/01331/01331__0032500_0055500.mp4"
  },
  {
    "timestamp": "00:33 - 00:55",
    "context": "[00:33 - 00:55] As the user prepares their documents at the reception desk, the sound of rustling paper is heard.",
    "question_type": "Sound Source Identification",
    "question": "What caused the rustling paper sound near the end of the clip?",
    "answer": "The user preparing their documents.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01331.mp4",
    "question_id": "01331_12",
    "clip_path": "clips/01331/01331__0032500_0055500.mp4"
  },
  {
    "timestamp": "00:15 - 00:33",
    "context": "[00:15 - 00:33] From a distance, a female staff member greets them with \"Hello.\"",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the female staff member’s greeting come from relative to the camera?",
    "answer": "From a distance, away from the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01331.mp4",
    "question_id": "01331_13",
    "clip_path": "clips/01331/01331__0014500_0033500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "[00:00 - 00:07] A person's hand slides across the spines of several old, cardboard-bound files on a shelf, producing a soft, continuous rustling sound.",
    "question_type": "Sound Source Identification",
    "question": "What action generated the soft, continuous rustling sound at the start?",
    "answer": "A hand sliding across the spines of old, cardboard-bound files on a shelf.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01333.mp4",
    "question_id": "01333_1",
    "clip_path": "clips/01333/01333__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "[00:00 - 00:07] A male voice from nearby provides information, to which the user replies with surprise: \"Oh, 1,000 volumes.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the user exclaim, \"Oh, 1,000 volumes.\"?",
    "answer": "It was a reaction to information provided by the nearby male speaker.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01333.mp4",
    "question_id": "01333_2",
    "clip_path": "clips/01333/01333__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "[00:00 - 00:07] The user uses both hands to pull a specific, thick, cardboard-bound book from the shelf, creating a scraping sound as it rubs against the adjacent files.",
    "question_type": "Sound Source Identification",
    "question": "What produced the scraping sound as the book was removed?",
    "answer": "Pulling a thick, cardboard-bound book from the shelf so it rubbed against adjacent files.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01333.mp4",
    "question_id": "01333_3",
    "clip_path": "clips/01333/01333__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:07 - 00:12",
    "context": "[00:07 - 00:12] The user opens the book, producing a crisp, distinct rustling sound as the old paper of the cover and first page are turned.",
    "question_type": "Sound Characteristics",
    "question": "What were the acoustic qualities of the sound when the book was opened?",
    "answer": "A crisp, distinct rustling from the old paper of the cover and first page.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01333.mp4",
    "question_id": "01333_4",
    "clip_path": "clips/01333/01333__0006500_0012500.mp4"
  },
  {
    "timestamp": "00:12 - 00:19",
    "context": "[00:12 - 00:19] The user agrees to instructions provided by the off-screen male speaker.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Was the instructing male speaker on-screen or off-screen during this interval?",
    "answer": "Off-screen.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01333.mp4",
    "question_id": "01333_5",
    "clip_path": "clips/01333/01333__0011500_0019500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "[00:00 - 00:07] The hand sliding across file spines produces a soft, continuous rustling sound.",
    "question_type": "Temporal Information",
    "question": "Was the rustling sound while sliding across the file spines brief or continuous?",
    "answer": "Continuous.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01333.mp4",
    "question_id": "01333_6",
    "clip_path": "clips/01333/01333__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:12 - 00:25",
    "context": "[00:12 - 00:19] The user says, \"Understood\" and \"Okay, okay.\" [00:19 - 00:25] The user responds, \"Okay, no problem.\"",
    "question_type": "Counting",
    "question": "How many times did the user say \"okay\" between 00:12 and 00:25?",
    "answer": "Three times: twice in \"Okay, okay\" and once in \"Okay, no problem.\"",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01333.mp4",
    "question_id": "01333_7",
    "clip_path": "clips/01333/01333__0011500_0025500.mp4"
  },
  {
    "timestamp": "00:27 - 00:31",
    "context": "[00:27 - 00:31] The user closes the book, creating a soft thud as the covers meet.",
    "question_type": "Sound Characteristics",
    "question": "What was the quality of the sound when the book was closed?",
    "answer": "A soft thud as the covers met.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01333.mp4",
    "question_id": "01333_8",
    "clip_path": "clips/01333/01333__0026500_0030667.mp4"
  },
  {
    "timestamp": "00:27 - 00:31",
    "context": "[00:27 - 00:31] The user closes the book, then reads the title from the cover aloud: \"Let's go see the sea.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Based on the subsequent action, why did the user close the book at this time?",
    "answer": "They closed it to read the title from the cover, which they immediately read aloud.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01333.mp4",
    "question_id": "01333_9",
    "clip_path": "clips/01333/01333__0026500_0030667.mp4"
  },
  {
    "timestamp": "00:00 - 00:08",
    "context": "[00:00 - 00:08] A male narrator begins speaking in a calm, explanatory tone: “打车和普通人打车,我们都是一样的操作。怎么点手机界面,以前我演示过盲人怎么使用手机,在这里不再赘述。” The caption notes this establishes a tutorial context on hailing a cab, likely from a visually impaired perspective.",
    "question_type": "Inferential & Contextual Causality",
    "question": "What is the inferred purpose of the narrator beginning his monologue at the start of the video?",
    "answer": "To provide a tutorial on how to hail a cab, likely from the perspective of a visually impaired person.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01335.mp4",
    "question_id": "01335_1",
    "clip_path": "clips/01335/01335__0000000_0008500.mp4"
  },
  {
    "timestamp": "00:00 - 00:08",
    "context": "[00:00 - 00:08] The narrator says he won’t go into detail about tapping the phone interface because he previously demonstrated how blind people use their phones.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why does the narrator say he will not elaborate on how to tap the phone interface?",
    "answer": "Because he has demonstrated before how blind people use their phones.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01335.mp4",
    "question_id": "01335_2",
    "clip_path": "clips/01335/01335__0000000_0008500.mp4"
  },
  {
    "timestamp": "00:00 - 00:08",
    "context": "[00:00 - 00:08] A male narrator begins speaking in a calm, explanatory tone.",
    "question_type": "Sound Characteristics",
    "question": "What is the tone of the narrator’s speech at the beginning?",
    "answer": "Calm and explanatory.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01335.mp4",
    "question_id": "01335_3",
    "clip_path": "clips/01335/01335__0000000_0008500.mp4"
  },
  {
    "timestamp": "00:00 - 00:15",
    "context": "[00:00 - 00:08] The narrator begins speaking and explains the context. [00:08 - 00:15] He continues explaining how to hail a cab, and the audio cuts off abruptly as he is about to explain the next step.",
    "question_type": "Temporal Information",
    "question": "Describe the continuity and ending of the narrator’s speech across 00:00 to 00:15.",
    "answer": "It continues from the start and then cuts off abruptly at around 00:15.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01335.mp4",
    "question_id": "01335_4",
    "clip_path": "clips/01335/01335__0000000_0015167.mp4"
  },
  {
    "timestamp": "00:00 - 00:08",
    "context": "[00:00 - 00:08] A male narrator begins speaking.",
    "question_type": "Sound Source Identification",
    "question": "Who is the source of the speech heard at the beginning of the video?",
    "answer": "A male narrator.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01335.mp4",
    "question_id": "01335_5",
    "clip_path": "clips/01335/01335__0000000_0008500.mp4"
  },
  {
    "timestamp": "00:08 - 00:15",
    "context": "[00:08 - 00:15] The narrator continues: “我们只是说一下怎么打车。站在路边,选一个合适的位置,然后用手机...,” before the audio cuts off.",
    "question_type": "Temporal Information",
    "question": "When does the narrator give the instruction to stand by the roadside, choose a suitable spot, and then use the phone?",
    "answer": "Between 00:08 and 00:15.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01335.mp4",
    "question_id": "01335_6",
    "clip_path": "clips/01335/01335__0007500_0015167.mp4"
  },
  {
    "timestamp": "00:10 - 00:12",
    "context": "[00:10 - 00:12] As the car slows and pulls over to the right side of a city street, a male passenger in the front seat says, \"好，谢谢师傅啊\" (Okay, thank you, driver), signaling the end of the ride and expressing gratitude.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the passenger say \"好，谢谢师傅啊\" at 00:10–00:12?",
    "answer": "He was signaling the end of the ride and expressing gratitude as the car pulled over.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01337.mp4",
    "question_id": "01337_1",
    "clip_path": "clips/01337/01337__0009500_0012500.mp4"
  },
  {
    "timestamp": "00:12 - 00:18",
    "context": "[00:12 - 00:18] A brief, polite conversation continues. The driver seemingly offers help, and the passenger replies, \"没事没事，我自己我自己\" (It's okay, it's okay, I'll do it myself), followed by \"谢啊\" (Thanks).",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the passenger say \"没事没事，我自己我自己\" during 00:12–00:18?",
    "answer": "He was responding to the driver's apparent offer of help, indicating he would exit on his own.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01337.mp4",
    "question_id": "01337_2",
    "clip_path": "clips/01337/01337__0011500_0018500.mp4"
  },
  {
    "timestamp": "00:18 - 00:23",
    "context": "[00:18] The passenger pulls the right-side door handle, producing a soft, distinct click as the latch mechanism unlatches.",
    "question_type": "Sound Source Identification",
    "question": "What generated the soft, distinct click at around 00:18?",
    "answer": "The car door’s latch mechanism unlatching when the right-side door handle was pulled.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01337.mp4",
    "question_id": "01337_3",
    "clip_path": "clips/01337/01337__0017500_0023500.mp4"
  },
  {
    "timestamp": "00:18 - 00:23",
    "context": "[After 00:18] After the passenger exits, the car door is shut from outside, creating a solid, moderately loud thud from the immediate right of the camera.",
    "question_type": "Sound Source Identification",
    "question": "What produced the solid thud heard after the passenger exited?",
    "answer": "The car door being shut from outside.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01337.mp4",
    "question_id": "01337_4",
    "clip_path": "clips/01337/01337__0017500_0023500.mp4"
  },
  {
    "timestamp": "00:10 - 00:12",
    "context": "[00:10 - 00:12] The male passenger speaks to the driver at a clear, conversational volume.",
    "question_type": "Sound Characteristics",
    "question": "What was the volume of the passenger’s speech at 00:10–00:12?",
    "answer": "Clear, conversational volume.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01337.mp4",
    "question_id": "01337_5",
    "clip_path": "clips/01337/01337__0009500_0012500.mp4"
  },
  {
    "timestamp": "00:18 - 00:23",
    "context": "[After 00:18] The car door is shut from the outside, creating a solid, moderately loud thud.",
    "question_type": "Sound Characteristics",
    "question": "How is the door-closing sound described in terms of quality and volume?",
    "answer": "A solid, moderately loud thud.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01337.mp4",
    "question_id": "01337_6",
    "clip_path": "clips/01337/01337__0017500_0023500.mp4"
  },
  {
    "timestamp": "00:18 - 00:23",
    "context": "[After 00:18] The door-closing thud originates from the immediate right of the camera.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the door-closing thud originate relative to the camera?",
    "answer": "From the immediate right of the camera at close range.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01337.mp4",
    "question_id": "01337_7",
    "clip_path": "clips/01337/01337__0017500_0023500.mp4"
  },
  {
    "timestamp": "00:18 - 00:23",
    "context": "[00:18] A soft click occurs as the door unlatches. [After exit] A solid thud follows when the door is shut from outside.",
    "question_type": "Temporal Information",
    "question": "In what order did the door-related sounds occur between 00:18 and 00:23?",
    "answer": "First the soft unlatching click, then after the passenger exited, the solid door-closing thud.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01337.mp4",
    "question_id": "01337_8",
    "clip_path": "clips/01337/01337__0017500_0023500.mp4"
  },
  {
    "timestamp": "00:12 - 00:18",
    "context": "[00:12 - 00:18] A brief, polite conversation continues between the two men inside the car.",
    "question_type": "Temporal Information",
    "question": "How long did the brief interior conversation last?",
    "answer": "About 6 seconds, from 00:12 to 00:18.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01337.mp4",
    "question_id": "01337_9",
    "clip_path": "clips/01337/01337__0011500_0018500.mp4"
  },
  {
    "timestamp": "00:18 - 00:23",
    "context": "[00:18] Door handle click as the latch unlatches. [After exit] Door-closing thud from the right.",
    "question_type": "Counting",
    "question": "How many distinct door-related sounds are described in 00:18–00:23?",
    "answer": "Two: the unlatching click and the closing thud.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01337.mp4",
    "question_id": "01337_10",
    "clip_path": "clips/01337/01337__0017500_0023500.mp4"
  },
  {
    "timestamp": "00:18 - 00:23",
    "context": "[After 00:18] The door is shut from outside with a moderately loud thud from the immediate right, confirming the completion of the drop-off.",
    "question_type": "Cross-Modal Reasoning",
    "question": "After the door-closing thud from the right, what does it indicate visually about the situation?",
    "answer": "It confirms the passenger has exited and the drop-off is complete.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01337.mp4",
    "question_id": "01337_11",
    "clip_path": "clips/01337/01337__0017500_0023500.mp4"
  },
  {
    "timestamp": "00:00 - 00:36",
    "context": "A clear male voice explains: he prefers the manual assistance gate so staff can use a walkie-talkie to call priority service (e.g., Nanjing '158'; Beijing West '036'; Beijing South 'Zhang Runqiu'). He notes he is at Nanjing South and is visually impaired, seeking assistance.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why does the speaker want to find the manual assistance gate despite the line not being long?",
    "answer": "Because staff there can call the station’s priority passenger service via walkie-talkie to assist him as a visually impaired traveler.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01341.mp4",
    "question_id": "01341_1",
    "clip_path": "clips/01341/01341__0000000_0036500.mp4"
  },
  {
    "timestamp": "00:37 - 00:41",
    "context": "From very close range, the user asks a female officer to call 158. She replies from the front-right, “Okay, okay. Please wait a moment.”",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the officer’s initial reply originate relative to the camera?",
    "answer": "From the front-right at very close range.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01341.mp4",
    "question_id": "01341_2",
    "clip_path": "clips/01341/01341__0036500_0041500.mp4"
  },
  {
    "timestamp": "00:42 - 00:56",
    "context": "The officer, speaking from the right, asks, “Have you bought a ticket?” and “Show me your ID card,” then checks the ID as faint scanner beeps are heard.",
    "question_type": "Counting",
    "question": "How many questions did the officer ask during the procedural check in this segment?",
    "answer": "Two.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01341.mp4",
    "question_id": "01341_3",
    "clip_path": "clips/01341/01341__0041500_0056500.mp4"
  },
  {
    "timestamp": "00:42 - 00:56",
    "context": "After the user hands over his ID card, faint electronic beeps from a scanner are heard in the background as the officer checks it.",
    "question_type": "Sound Source Identification",
    "question": "What generated the faint electronic beeps heard during the ID check?",
    "answer": "An ID scanner used by the officer.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01341.mp4",
    "question_id": "01341_4",
    "clip_path": "clips/01341/01341__0041500_0056500.mp4"
  },
  {
    "timestamp": "01:00 - 01:17",
    "context": "The officer picks up her shoulder walkie-talkie; a distinct crackling sound is heard as the radio activates.",
    "question_type": "Sound Characteristics",
    "question": "What was the characteristic activation sound of the walkie-talkie?",
    "answer": "A distinct, crackling sound.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01341.mp4",
    "question_id": "01341_5",
    "clip_path": "clips/01341/01341__0059500_0077500.mp4"
  },
  {
    "timestamp": "01:00 - 01:17",
    "context": "Speaking over the radio, the officer’s voice has the compressed quality of radio transmission; a reply comes through with static confirming staff is on the way.",
    "question_type": "Sound Characteristics",
    "question": "How did the radio-transmitted voices sound in this segment?",
    "answer": "They had a compressed radio quality and were accompanied by static.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01341.mp4",
    "question_id": "01341_6",
    "clip_path": "clips/01341/01341__0059500_0077500.mp4"
  },
  {
    "timestamp": "01:00 - 01:17",
    "context": "The officer radios, “Is 158 service available? … At entrance 6, there is a blind person, his train is at 10:43,” and receives a confirming response.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the officer use the walkie-talkie at this time?",
    "answer": "To contact the ‘158’ service and arrange assistance for the user, as requested.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01341.mp4",
    "question_id": "01341_7",
    "clip_path": "clips/01341/01341__0059500_0077500.mp4"
  },
  {
    "timestamp": "01:00 - 01:17",
    "context": "A response comes through the walkie-talkie, accompanied by static, confirming that staff is on the way.",
    "question_type": "Temporal Information",
    "question": "During which time interval was the confirmation over the radio received?",
    "answer": "Between 01:00 and 01:17.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01341.mp4",
    "question_id": "01341_8",
    "clip_path": "clips/01341/01341__0059500_0077500.mp4"
  },
  {
    "timestamp": "01:28 - 01:33",
    "context": "The officer tells the user, “158 has been called,” returns his ID card, and instructs, “Okay, you can move forward.”",
    "question_type": "Temporal Information",
    "question": "When did the officer verbally confirm that 158 had been called?",
    "answer": "Between 01:28 and 01:33.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01341.mp4",
    "question_id": "01341_9",
    "clip_path": "clips/01341/01341__0087500_0093500.mp4"
  },
  {
    "timestamp": "01:28 - 01:33",
    "context": "After confirming the call and returning the ID card, the officer says, “Okay, you can move forward,” and the user passes the checkpoint.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the officer tell the user he could move forward?",
    "answer": "Because the assistance request had been handled and his ID was returned, allowing him to proceed past the checkpoint.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01341.mp4",
    "question_id": "01341_10",
    "clip_path": "clips/01341/01341__0087500_0093500.mp4"
  },
  {
    "timestamp": "01:35 - 01:51",
    "context": "The user narrates the next steps “over background music,” describing baggage check, being seated to wait, and the assistance staff arriving shortly after.",
    "question_type": "Temporal Information",
    "question": "During what time range is background music audible under the user’s monologue?",
    "answer": "From 01:35 to 01:51.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01341.mp4",
    "question_id": "01341_11",
    "clip_path": "clips/01341/01341__0094500_0111500.mp4"
  },
  {
    "timestamp": "01:35 - 01:51",
    "context": "The user says staff arranged a seat for him to wait, and shortly after, the assistance lady arrived.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why was a seat arranged for the user after baggage security check?",
    "answer": "So he could wait for the assistance staff to arrive.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01341.mp4",
    "question_id": "01341_12",
    "clip_path": "clips/01341/01341__0094500_0111500.mp4"
  },
  {
    "timestamp": "01:52 - 02:09",
    "context": "A new female voice, presumably the ‘158’ staff, is heard from nearby asking about the train time and commenting on the backpack’s weight.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "What was the approximate distance of the new female voice relative to the camera?",
    "answer": "It was nearby (close range).",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01341.mp4",
    "question_id": "01341_13",
    "clip_path": "clips/01341/01341__0111500_0129500.mp4"
  },
  {
    "timestamp": "01:52 - 02:09",
    "context": "The staff member says, “Your backpack is quite heavy,” and the user replies, “There’s a laptop inside the backpack.”",
    "question_type": "Inferential & Contextual Causality",
    "question": "What reason is given for the backpack being heavy?",
    "answer": "It contains a laptop.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01341.mp4",
    "question_id": "01341_14",
    "clip_path": "clips/01341/01341__0111500_0129500.mp4"
  },
  {
    "timestamp": "00:00 - 00:36",
    "context": "In his opening monologue the user cites examples of station services: Nanjing Station’s ‘158’, Beijing West’s ‘036’, and Beijing South Station’s ‘Zhang Runqiu’.",
    "question_type": "Counting",
    "question": "How many example station service identifiers/names does the speaker mention?",
    "answer": "Three.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01341.mp4",
    "question_id": "01341_15",
    "clip_path": "clips/01341/01341__0000000_0036500.mp4"
  },
  {
    "timestamp": "00:14 - 00:29",
    "context": "[00:14] A large red bus approaches and pulls up to the stop on the left, with a low-frequency engine rumble and a distinct, brief hiss of its air brakes.",
    "question_type": "Sound Source Identification",
    "question": "What produced the distinct, brief hiss heard as the red bus pulled up?",
    "answer": "The bus’s air brakes.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01342.mp4",
    "question_id": "01342_1",
    "clip_path": "clips/01342/01342__0013500_0029500.mp4"
  },
  {
    "timestamp": "00:14 - 00:29",
    "context": "[00:14 - 00:29] As the red bus arrives, its engine is audible as a low-frequency rumble.",
    "question_type": "Sound Characteristics",
    "question": "How is the arriving red bus's engine sound described?",
    "answer": "As a low-frequency rumble.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01342.mp4",
    "question_id": "01342_2",
    "clip_path": "clips/01342/01342__0013500_0029500.mp4"
  },
  {
    "timestamp": "00:14 - 00:29",
    "context": "[00:14 - 00:29] The red bus approaches from the front and pulls up to the stop on the left.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From which side relative to the camera did the red bus’s arrival sounds (engine and air brakes) come when it pulled up?",
    "answer": "From the left side.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01342.mp4",
    "question_id": "01342_3",
    "clip_path": "clips/01342/01342__0013500_0029500.mp4"
  },
  {
    "timestamp": "00:14 - 00:29",
    "context": "[00:14 - 00:29] The arrival includes a distinct, brief hiss of the bus’s air brakes.",
    "question_type": "Temporal Information",
    "question": "Was the air brake hiss during the red bus’s arrival brief or prolonged?",
    "answer": "It was brief.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01342.mp4",
    "question_id": "01342_4",
    "clip_path": "clips/01342/01342__0013500_0029500.mp4"
  },
  {
    "timestamp": "00:29 - 00:37",
    "context": "[00:29 - 00:37] As the red bus departs, the sound of its engine gradually fades into the distance, mixing with the moderate sound of other passing cars.",
    "question_type": "Temporal Information",
    "question": "During which interval did the red bus’s engine sound fade, and how did its volume change?",
    "answer": "Between 00:29 and 00:37, it gradually faded into the distance.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01342.mp4",
    "question_id": "01342_5",
    "clip_path": "clips/01342/01342__0028500_0037500.mp4"
  },
  {
    "timestamp": "00:29 - 00:37",
    "context": "[00:29 - 00:37] The departing bus’s engine fades while other passing cars are heard at a moderate level.",
    "question_type": "Counting",
    "question": "How many distinct vehicle sound types are audible in this interval?",
    "answer": "Two: the fading bus engine and other passing cars.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01342.mp4",
    "question_id": "01342_6",
    "clip_path": "clips/01342/01342__0028500_0037500.mp4"
  },
  {
    "timestamp": "00:29 - 00:37",
    "context": "[00:29 - 00:37] Other passing cars are audible along with the fading bus engine.",
    "question_type": "Sound Characteristics",
    "question": "What is the volume level of the other passing cars during this segment?",
    "answer": "Moderate.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01342.mp4",
    "question_id": "01342_7",
    "clip_path": "clips/01342/01342__0028500_0037500.mp4"
  },
  {
    "timestamp": "00:29 - 00:37",
    "context": "[00:29 - 00:37] The red bus departs, and its engine sound gradually fades into the distance.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the red bus’s engine sound gradually fade during this interval?",
    "answer": "Because the bus was departing and moving away from the stop.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01342.mp4",
    "question_id": "01342_8",
    "clip_path": "clips/01342/01342__0028500_0037500.mp4"
  },
  {
    "timestamp": "00:37 - 00:49",
    "context": "[00:37 - 00:49] A second white-and-red bus arrives on the left, producing a low engine hum and the sound of air brakes.",
    "question_type": "Sound Source Identification",
    "question": "What generated the low engine hum and air brake sound when the second bus arrived?",
    "answer": "The white-and-red bus’s engine and air brakes.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01342.mp4",
    "question_id": "01342_9",
    "clip_path": "clips/01342/01342__0036500_0049400.mp4"
  },
  {
    "timestamp": "00:37 - 00:49",
    "context": "[00:37 - 00:49] The second bus arrives on the left.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From which side did the second bus’s arrival sounds originate relative to the camera?",
    "answer": "From the left.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01342.mp4",
    "question_id": "01342_10",
    "clip_path": "clips/01342/01342__0036500_0049400.mp4"
  },
  {
    "timestamp": "00:37 - 00:49",
    "context": "[00:37 - 00:49] The user says they didn’t hear any station announcements and decides to find someone to ask.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the user decide to find someone to ask during this segment?",
    "answer": "Because they couldn’t find bus 510 and hadn’t heard any station announcements.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01342.mp4",
    "question_id": "01342_11",
    "clip_path": "clips/01342/01342__0036500_0049400.mp4"
  },
  {
    "timestamp": "00:14 - 00:29, 00:37 - 00:49",
    "context": "[00:14 - 00:29] Air brake hiss as the red bus arrives. [00:37 - 00:49] Air brake sound again as the second bus arrives.",
    "question_type": "Counting",
    "question": "How many times is an air brake hiss heard in the video?",
    "answer": "Twice.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01342.mp4",
    "question_id": "01342_12",
    "clip_path": "clips/01342/01342__0013500_0049400.mp4"
  },
  {
    "timestamp": "00:00 - 00:14",
    "context": "[00:00 - 00:14] The user provides a continuous, calm monologue about their intention for the day.",
    "question_type": "Sound Characteristics",
    "question": "How is the user’s monologue characterized at the start of the video?",
    "answer": "It is continuous and calm.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01342.mp4",
    "question_id": "01342_13",
    "clip_path": "clips/01342/01342__0000000_0014500.mp4"
  },
  {
    "timestamp": "00:00 - 00:38",
    "context": "[00:00 - 00:09] The user asks a nearby woman, “Auntie, please do me a favor, see if there's a car with 8358 around me.” She replies there isn’t. [00:17 - 00:38] The user calls the driver and says, “Where are you? Honk the horn. I'm right at the location,” indicating he’s trying to find his ride.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the user ask the nearby woman to check for a car with '8358'?",
    "answer": "He was trying to locate his ride associated with '8358.'",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01347.mp4",
    "question_id": "01347_1",
    "clip_path": "clips/01347/01347__0000000_0038500.mp4"
  },
  {
    "timestamp": "00:00 - 00:09",
    "context": "From a first-person perspective, the user speaks to a nearby woman in a clear, moderate voice.",
    "question_type": "Sound Characteristics",
    "question": "What was the quality and volume of the user’s voice during the initial conversation?",
    "answer": "Clear and moderate.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01347.mp4",
    "question_id": "01347_2",
    "clip_path": "clips/01347/01347__0000000_0009500.mp4"
  },
  {
    "timestamp": "00:00 - 00:09",
    "context": "The entire exchange between the user and the woman is described as a direct, nearby conversation.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the initial conversation occur relative to the camera?",
    "answer": "At close range, as a direct, nearby conversation.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01347.mp4",
    "question_id": "01347_3",
    "clip_path": "clips/01347/01347__0000000_0009500.mp4"
  },
  {
    "timestamp": "00:00 - 00:09",
    "context": "The user and a nearby woman converse about a car with '8358.'",
    "question_type": "Counting",
    "question": "How many speakers participated in the initial exchange?",
    "answer": "Two speakers: the user and the woman.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01347.mp4",
    "question_id": "01347_4",
    "clip_path": "clips/01347/01347__0000000_0009500.mp4"
  },
  {
    "timestamp": "00:00 - 00:09",
    "context": "The conversation between the user and the woman begins at the start of the clip and ends by 00:09.",
    "question_type": "Temporal Information",
    "question": "When did the initial conversation occur and how long did it last?",
    "answer": "Between 00:00 and 00:09, lasting about 9 seconds.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01347.mp4",
    "question_id": "01347_5",
    "clip_path": "clips/01347/01347__0000000_0009500.mp4"
  },
  {
    "timestamp": "00:17 - 00:38",
    "context": "The user speaks into a phone: “Hello driver, can you hear my voice? Where are you? Honk the horn. I'm right at the location.” His voice is directed forward for the phone’s microphone.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "During the call attempt, in what direction was the user’s speech addressed relative to the camera?",
    "answer": "Forward, directed toward the phone’s microphone.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01347.mp4",
    "question_id": "01347_6",
    "clip_path": "clips/01347/01347__0016500_0038500.mp4"
  },
  {
    "timestamp": "00:17 - 00:38",
    "context": "The user repeatedly asks the driver to honk and states he’s at the location, with no visible response from the driver.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the user ask the driver to honk the horn?",
    "answer": "To help locate the driver’s vehicle since the driver wasn’t visible or responding.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01347.mp4",
    "question_id": "01347_7",
    "clip_path": "clips/01347/01347__0016500_0038500.mp4"
  },
  {
    "timestamp": "00:17 - 00:38",
    "context": "The user repeatedly calls out to the driver and receives no spoken response.",
    "question_type": "Temporal Information",
    "question": "When did the user’s phone call attempt to reach the driver take place?",
    "answer": "Between 00:17 and 00:38.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01347.mp4",
    "question_id": "01347_8",
    "clip_path": "clips/01347/01347__0016500_0038500.mp4"
  },
  {
    "timestamp": "00:46 - 01:12",
    "context": "A male narrator explains: after the driver picked up, he could hear their ambient sound and that the driver was rubbing the microphone with his hand, but the driver never spoke.",
    "question_type": "Sound Source Identification",
    "question": "According to the narration, what produced the rubbing sound heard on the call?",
    "answer": "The driver rubbing the phone’s microphone with his hand.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01347.mp4",
    "question_id": "01347_9",
    "clip_path": "clips/01347/01347__0045500_0071843.mp4"
  },
  {
    "timestamp": "00:46 - 01:12",
    "context": "The narrator states that during the call the other party never spoke.",
    "question_type": "Counting",
    "question": "How many times did the driver speak during the call described by the narrator?",
    "answer": "Zero; the driver never spoke.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01347.mp4",
    "question_id": "01347_10",
    "clip_path": "clips/01347/01347__0045500_0071843.mp4"
  },
  {
    "timestamp": "00:17 - 01:12",
    "context": "A soft, melodic background music track begins during 00:17 - 00:38 and continues under the narrator from 00:46 - 01:12.",
    "question_type": "Temporal Information",
    "question": "When did the background music begin, and did it continue under the narration?",
    "answer": "It began during 00:17 - 00:38 and continued under the narration from 00:46 to 01:12.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01347.mp4",
    "question_id": "01347_11",
    "clip_path": "clips/01347/01347__0016500_0071843.mp4"
  },
  {
    "timestamp": "00:17 - 01:12",
    "context": "A soft, melodic background music track plays during the call attempt and continues into the narration.",
    "question_type": "Sound Characteristics",
    "question": "What are the characteristics of the background music?",
    "answer": "Soft and melodic.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01347.mp4",
    "question_id": "01347_12",
    "clip_path": "clips/01347/01347__0016500_0071843.mp4"
  },
  {
    "timestamp": "00:46 - 01:12",
    "context": "A male narrator’s voice, added in post-production, explains the situation over the continuing music.",
    "question_type": "Sound Source Identification",
    "question": "Whose voice is heard explaining the situation from 00:46 to 01:12?",
    "answer": "A male narrator’s voice added in post-production.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01347.mp4",
    "question_id": "01347_13",
    "clip_path": "clips/01347/01347__0045500_0071843.mp4"
  },
  {
    "timestamp": "00:46 - 01:12",
    "context": "The narrator says he heard ambient sounds and microphone rubbing after the call connected, but the other party never spoke, which he found unprecedented. He speculates the driver’s phone might have been stolen.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Based on the narrated details, why did he speculate the driver’s phone might have been stolen?",
    "answer": "Because after the call connected he heard ambient audio and mic rubbing but no speech, an unusual situation, leading him to guess the phone might not be with the driver.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01347.mp4",
    "question_id": "01347_14",
    "clip_path": "clips/01347/01347__0045500_0071843.mp4"
  },
  {
    "timestamp": "00:00 - 00:09",
    "context": "The woman replies there is no car with '8358' around. The user confirms, and she agrees before he thanks her.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the user thank the woman at the end of the initial exchange?",
    "answer": "Because she confirmed there was no car with '8358' nearby.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01347.mp4",
    "question_id": "01347_15",
    "clip_path": "clips/01347/01347__0000000_0009500.mp4"
  },
  {
    "timestamp": "00:00 - 01:12",
    "context": "Voices heard include the user, the nearby woman, and later a male narrator. The driver never speaks.",
    "question_type": "Counting",
    "question": "How many distinct human voices are heard across the clip?",
    "answer": "Three: the user, the woman, and the male narrator.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01347.mp4",
    "question_id": "01347_16",
    "clip_path": "clips/01347/01347__0000000_0071843.mp4"
  },
  {
    "timestamp": "00:07 - 00:26",
    "context": "The driver, in a calm and explanatory tone, details how visually impaired individuals use smartphones. The passenger listens.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why is the driver speaking in a calm, explanatory tone during this segment?",
    "answer": "He is explaining how visually impaired individuals use smartphones to help the passenger understand.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01360.mp4",
    "question_id": "01360_1",
    "clip_path": "clips/01360/01360__0006500_0026500.mp4"
  },
  {
    "timestamp": "00:07 - 00:26",
    "context": "The passenger listens and occasionally agrees with affirmative sounds and says, \"Yeah, yeah, yeah, yeah,\" confirming his understanding.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why does the passenger say \"Yeah, yeah, yeah, yeah\"?",
    "answer": "To confirm his understanding and agreement with the driver's explanation.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01360.mp4",
    "question_id": "01360_2",
    "clip_path": "clips/01360/01360__0006500_0026500.mp4"
  },
  {
    "timestamp": "00:07 - 00:26",
    "context": "The conversation is accompanied by the low, continuous hum of the car engine.",
    "question_type": "Sound Source Identification",
    "question": "What generates the background hum heard during the conversation?",
    "answer": "The car engine.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01360.mp4",
    "question_id": "01360_3",
    "clip_path": "clips/01360/01360__0006500_0026500.mp4"
  },
  {
    "timestamp": "00:07 - 00:26",
    "context": "The conversation is accompanied by the low, continuous hum of the car engine.",
    "question_type": "Sound Characteristics",
    "question": "What is the quality of the background car sound during the conversation?",
    "answer": "A low, continuous hum.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01360.mp4",
    "question_id": "01360_4",
    "clip_path": "clips/01360/01360__0006500_0026500.mp4"
  },
  {
    "timestamp": "00:07 - 00:26",
    "context": "The camera operator is seated in the passenger seat; the driver is on the left.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From which direction relative to the camera does the driver's speech originate?",
    "answer": "From the left side.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01360.mp4",
    "question_id": "01360_5",
    "clip_path": "clips/01360/01360__0006500_0026500.mp4"
  },
  {
    "timestamp": "00:07 - 00:26",
    "context": "The camera operator and the driver have a conversation during this interval.",
    "question_type": "Temporal Information",
    "question": "When does the conversation between the passenger and driver occur?",
    "answer": "From 00:07 to 00:26.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01360.mp4",
    "question_id": "01360_6",
    "clip_path": "clips/01360/01360__0006500_0026500.mp4"
  },
  {
    "timestamp": "00:07 - 00:26",
    "context": "The conversation is accompanied by the low, continuous hum of the car engine.",
    "question_type": "Temporal Information",
    "question": "What is the temporal relationship between the engine hum and the conversation?",
    "answer": "The engine hum accompanies the conversation throughout the segment.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01360.mp4",
    "question_id": "01360_7",
    "clip_path": "clips/01360/01360__0006500_0026500.mp4"
  },
  {
    "timestamp": "00:07 - 00:26",
    "context": "The passenger says, \"Yeah, yeah, yeah, yeah,\" confirming his understanding.",
    "question_type": "Counting",
    "question": "How many times does the passenger repeat the word \"yeah\" in the quoted response?",
    "answer": "Four times.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01360.mp4",
    "question_id": "01360_8",
    "clip_path": "clips/01360/01360__0006500_0026500.mp4"
  },
  {
    "timestamp": "00:07 - 00:26",
    "context": "The camera operator (passenger) and the driver converse.",
    "question_type": "Counting",
    "question": "How many speakers are engaged in the conversation?",
    "answer": "Two speakers.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01360.mp4",
    "question_id": "01360_9",
    "clip_path": "clips/01360/01360__0006500_0026500.mp4"
  },
  {
    "timestamp": "00:00 - 00:12",
    "context": "[00:00 - 00:12] A man, directly in front of the camera, speaks in a clear, instructional tone and uses the camera holder’s white cane to show a safe path along the curb and how to bypass an electric bike. The camera holder is visually impaired.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the man use the camera holder’s white cane to demonstrate the path?",
    "answer": "To physically show the clear path and how to bypass the parked electric bike for the visually impaired camera holder.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01361.mp4",
    "question_id": "01361_1",
    "clip_path": "clips/01361/01361__0000000_0012500.mp4"
  },
  {
    "timestamp": "00:12 - 00:13",
    "context": "[00:12 - 00:13] The man walks away and responds kindly to the thanks, saying, “You’re welcome, you’re welcome,” from a short distance.",
    "question_type": "Sound Source Identification",
    "question": "Who said, “You’re welcome, you’re welcome”?",
    "answer": "The man who had been giving directions.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01361.mp4",
    "question_id": "01361_2",
    "clip_path": "clips/01361/01361__0011500_0013500.mp4"
  },
  {
    "timestamp": "00:00 - 00:12",
    "context": "[00:00 - 00:12] The man provides navigational instructions, speaking in a clear, instructional tone.",
    "question_type": "Sound Characteristics",
    "question": "What was the tone of the man’s speech while giving directions?",
    "answer": "Clear and instructional.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01361.mp4",
    "question_id": "01361_3",
    "clip_path": "clips/01361/01361__0000000_0012500.mp4"
  },
  {
    "timestamp": "00:00 - 00:12",
    "context": "[00:00 - 00:12] The man is positioned directly in front of the camera while giving instructions.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where was the man relative to the camera while delivering the initial instructions?",
    "answer": "Directly in front of the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01361.mp4",
    "question_id": "01361_4",
    "clip_path": "clips/01361/01361__0000000_0012500.mp4"
  },
  {
    "timestamp": "00:12 - 00:13",
    "context": "[00:12 - 00:13] As he walks away, the man says, “You’re welcome, you’re welcome,” from a short distance.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From what distance did the “You’re welcome, you’re welcome” originate?",
    "answer": "From a short distance away.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01361.mp4",
    "question_id": "01361_5",
    "clip_path": "clips/01361/01361__0011500_0013500.mp4"
  },
  {
    "timestamp": "00:12 - 00:13",
    "context": "[00:12 - 00:13] The man walks away and says, “You’re welcome, you’re welcome.”",
    "question_type": "Temporal Information",
    "question": "When did the man respond with “You’re welcome, you’re welcome”?",
    "answer": "Between 00:12 and 00:13.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01361.mp4",
    "question_id": "01361_6",
    "clip_path": "clips/01361/01361__0011500_0013500.mp4"
  },
  {
    "timestamp": "00:13 - 00:22",
    "context": "[00:13 - 00:22] The camera holder asks, “Sir, can I take a picture of you?” The man, from a few meters away, says “No.” The camera holder explains they used to be a photographer and offers to take a picture for him. The man then says, “Alright, go ahead.”",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the man change from declining the photo to agreeing?",
    "answer": "After hearing the camera holder’s explanation that they used to be a photographer and would take a picture for him, he reconsidered and agreed.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01361.mp4",
    "question_id": "01361_7",
    "clip_path": "clips/01361/01361__0012500_0022500.mp4"
  },
  {
    "timestamp": "00:13 - 00:22",
    "context": "[00:13 - 00:22] The man, from a few meters away, initially declines the request with a simple “No.”",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "How far from the camera was the man when he initially said “No” to the photo request?",
    "answer": "A few meters away.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01361.mp4",
    "question_id": "01361_8",
    "clip_path": "clips/01361/01361__0012500_0022500.mp4"
  },
  {
    "timestamp": "00:13 - 00:22",
    "context": "[00:13 - 00:22] The camera holder calls out, “Sir, can I take a picture of you?”",
    "question_type": "Temporal Information",
    "question": "During what time interval did the camera holder ask to take a picture?",
    "answer": "00:13 to 00:22.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01361.mp4",
    "question_id": "01361_9",
    "clip_path": "clips/01361/01361__0012500_0022500.mp4"
  },
  {
    "timestamp": "00:00 - 00:12",
    "context": "[00:00 - 00:12] The camera holder expresses gratitude: “Okay, thank you, thank you.”",
    "question_type": "Counting",
    "question": "How many times did the camera holder say “thank you”?",
    "answer": "Twice.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01361.mp4",
    "question_id": "01361_10",
    "clip_path": "clips/01361/01361__0000000_0012500.mp4"
  },
  {
    "timestamp": "00:12 - 00:13",
    "context": "[00:12 - 00:13] The man replies, “You’re welcome, you’re welcome.”",
    "question_type": "Counting",
    "question": "How many times did the man say “You’re welcome”?",
    "answer": "Twice.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01361.mp4",
    "question_id": "01361_11",
    "clip_path": "clips/01361/01361__0011500_0013500.mp4"
  },
  {
    "timestamp": "00:23 - 00:25",
    "context": "[00:23 - 00:25] Preparing to take the photo, the camera holder asks, “Where are you? How far are you from me?”",
    "question_type": "Counting",
    "question": "How many questions did the camera holder ask while preparing to frame the photo?",
    "answer": "Two.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01361.mp4",
    "question_id": "01361_12",
    "clip_path": "clips/01361/01361__0022500_0025500.mp4"
  },
  {
    "timestamp": "00:23 - 00:25",
    "context": "[00:23 - 00:25] The camera holder, who is visually impaired, asks for help framing: “Where are you? How far are you from me?”",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the camera holder ask “Where are you? How far are you from me?” while preparing to take the photo?",
    "answer": "Because they are visually impaired and needed assistance locating and framing the subject.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01361.mp4",
    "question_id": "01361_13",
    "clip_path": "clips/01361/01361__0022500_0025500.mp4"
  },
  {
    "timestamp": "00:13 - 00:22",
    "context": "[00:13 - 00:22] The camera holder calls out to the man: “Sir, can I take a picture of you?”",
    "question_type": "Sound Source Identification",
    "question": "Who asked, “Sir, can I take a picture of you?”",
    "answer": "The camera holder.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01361.mp4",
    "question_id": "01361_14",
    "clip_path": "clips/01361/01361__0012500_0022500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "As they are about to walk into a yellow and black striped pole, a man standing to their front-right, approximately 1-2 meters away, warns them in a clear, medium-volume voice, stating, '前面是柱子' (There's a pole in front).",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the man warn, 'There's a pole in front'?",
    "answer": "Because the camera-wearer was about to walk into a yellow and black striped pole.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01362.mp4",
    "question_id": "01362_1",
    "clip_path": "clips/01362/01362__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "A man standing to their front-right, approximately 1-2 meters away, warns them in a clear, medium-volume voice.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the warning voice originate relative to the camera?",
    "answer": "From the front-right, approximately 1–2 meters away.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01362.mp4",
    "question_id": "01362_2",
    "clip_path": "clips/01362/01362__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "The man warns them in a clear, medium-volume voice, stating, '前面是柱子' (There's a pole in front).",
    "question_type": "Sound Characteristics",
    "question": "What were the clarity and volume characteristics of the man's warning?",
    "answer": "It was clear and medium-volume.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01362.mp4",
    "question_id": "01362_3",
    "clip_path": "clips/01362/01362__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "Simultaneously, the tip of the white cane makes a light, sharp tapping sound as it makes contact with the base of the metal pole.",
    "question_type": "Sound Source Identification",
    "question": "What generated the light, sharp tapping sound heard at this moment?",
    "answer": "The tip of the white cane contacting the base of the metal pole.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01362.mp4",
    "question_id": "01362_4",
    "clip_path": "clips/01362/01362__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "The tip of the white cane makes a light, sharp tapping sound as it makes contact with the base of the metal pole.",
    "question_type": "Sound Characteristics",
    "question": "How is the cane's tapping sound described?",
    "answer": "As a light, sharp tapping sound.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01362.mp4",
    "question_id": "01362_5",
    "clip_path": "clips/01362/01362__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "A man warns '前面是柱子' while the cane simultaneously makes a light, sharp tapping sound on the pole's base.",
    "question_type": "Temporal Information",
    "question": "Did the cane's tapping occur before, after, or at the same time as the warning?",
    "answer": "At the same time (simultaneously).",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01362.mp4",
    "question_id": "01362_6",
    "clip_path": "clips/01362/01362__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "The man warns; in response, the camera-wearer acknowledges with '哦' and then says '谢谢谢谢'.",
    "question_type": "Counting",
    "question": "How many different speakers are involved in the exchange during this segment?",
    "answer": "Two speakers: the man and the camera-wearer.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01362.mp4",
    "question_id": "01362_7",
    "clip_path": "clips/01362/01362__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:03 - 00:07",
    "context": "Following the initial warning, the same man kindly asks, '你要去哪?' (Where are you going?).",
    "question_type": "Temporal Information",
    "question": "Did the man's question 'Where are you going?' occur before or after the initial warning?",
    "answer": "After the initial warning.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01362.mp4",
    "question_id": "01362_8",
    "clip_path": "clips/01362/01362__0002500_0007267.mp4"
  },
  {
    "timestamp": "00:03 - 00:07",
    "context": "The camera-wearer replies, '我要去全家 (超市)' and upon hearing this, the man says, '全家在对面。我带你过去'.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the man offer, 'I'll take you there'?",
    "answer": "Because after hearing the destination was FamilyMart, which he indicated is across the street, he offered to guide them there.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01362.mp4",
    "question_id": "01362_9",
    "clip_path": "clips/01362/01362__0002500_0007267.mp4"
  },
  {
    "timestamp": "00:03 - 00:07",
    "context": "The man asks 'Where are you going?', the camera-wearer states their destination, and the man offers assistance.",
    "question_type": "Counting",
    "question": "How many speaking turns occur in this exchange?",
    "answer": "Three speaking turns: the man's question, the camera-wearer's reply, and the man's offer.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01362.mp4",
    "question_id": "01362_10",
    "clip_path": "clips/01362/01362__0002500_0007267.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "As they approach the curb, the helper provides a cautionary warning, saying, \"Slow down, there are steps here,\" to prevent a fall.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the helper say, \"Slow down, there are steps here\"?",
    "answer": "Because they were approaching the curb and he wanted to prevent a fall.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01365.mp4",
    "question_id": "01365_1",
    "clip_path": "clips/01365/01365__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "The man's clear voice is heard from directly in front, saying, \"Come, come, come, I'll take you over. Go sit on the steps over there.\"",
    "question_type": "Sound Source Identification",
    "question": "Who produced the clear, guiding voice heard at the start of the clip?",
    "answer": "The man in a gray hoodie directly in front of the camera operator.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01365.mp4",
    "question_id": "01365_2",
    "clip_path": "clips/01365/01365__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "This entire interaction is set against the faint, ambient sound of distant city traffic.",
    "question_type": "Sound Characteristics",
    "question": "How would you describe the volume and character of the background city traffic?",
    "answer": "Faint and ambient.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01365.mp4",
    "question_id": "01365_3",
    "clip_path": "clips/01365/01365__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "The man's clear voice is heard from directly in front.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where relative to the camera did the helper's speech originate?",
    "answer": "Directly in front of the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01365.mp4",
    "question_id": "01365_4",
    "clip_path": "clips/01365/01365__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "This entire interaction (00:00–00:07) is set against the faint, ambient sound of distant city traffic.",
    "question_type": "Temporal Information",
    "question": "Is the background city traffic brief or continuous during 00:00–00:07?",
    "answer": "It is continuous throughout the interaction.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01365.mp4",
    "question_id": "01365_5",
    "clip_path": "clips/01365/01365__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "The operator says, \"Oh, okay, thank you.\" Later: \"Okay. Thank you, thank you, thank you.\"",
    "question_type": "Counting",
    "question": "How many times does the operator say \"thank you\" in total?",
    "answer": "Four times.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01365.mp4",
    "question_id": "01365_6",
    "clip_path": "clips/01365/01365__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:00 - 00:13",
    "context": "A female voice says with a light, amused tone: \"The chick is drawn a bit small. Haha... It's a round head, an oval body, and a small mouth.\" Another person confirms, \"Yes, just like that. Haha.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the female voice laugh while describing the chick at the beginning?",
    "answer": "Because she found the simple, small stick-figure chick design amusing.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01371.mp4",
    "question_id": "01371_1",
    "clip_path": "clips/01371/01371__0000000_0013500.mp4"
  },
  {
    "timestamp": "00:00 - 00:13",
    "context": "Another person, likely an instructor or friend, confirms the design, saying, \"Yes, just like that. Haha.\"",
    "question_type": "Sound Source Identification",
    "question": "Who said, \"Yes, just like that. Haha\"?",
    "answer": "Another person, likely an instructor or friend.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01371.mp4",
    "question_id": "01371_2",
    "clip_path": "clips/01371/01371__0000000_0013500.mp4"
  },
  {
    "timestamp": "00:00 - 00:13",
    "context": "A female voice speaks with a light, amused tone about the chick design.",
    "question_type": "Sound Characteristics",
    "question": "What was the tone of the female voice when describing the chick?",
    "answer": "Light and amused, including a laugh.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01371.mp4",
    "question_id": "01371_3",
    "clip_path": "clips/01371/01371__0000000_0013500.mp4"
  },
  {
    "timestamp": "00:00 - 00:13",
    "context": "The female voice says \"Haha,\" and the other person also ends with \"Haha.\"",
    "question_type": "Counting",
    "question": "How many times is laughter (\"Haha\") heard in this segment?",
    "answer": "Twice.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01371.mp4",
    "question_id": "01371_4",
    "clip_path": "clips/01371/01371__0000000_0013500.mp4"
  },
  {
    "timestamp": "00:14 - 00:36",
    "context": "The instructor guides in a calm, instructional tone: \"Just brush it casually... It's pure color anyway... Dip in some water, dip in some paint.\"",
    "question_type": "Sound Characteristics",
    "question": "What is the tone of the instructor while giving glazing instructions?",
    "answer": "Calm and instructional.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01371.mp4",
    "question_id": "01371_5",
    "clip_path": "clips/01371/01371__0013500_0036500.mp4"
  },
  {
    "timestamp": "00:14 - 00:36",
    "context": "The operator responds \"Okay.\" and later affirms \"Mmm.\" while the instructor continues to guide.",
    "question_type": "Counting",
    "question": "How many times does the operator verbally acknowledge the instructions in this interval?",
    "answer": "Twice: once with \"Okay\" and once with \"Mmm.\"",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01371.mp4",
    "question_id": "01371_6",
    "clip_path": "clips/01371/01371__0013500_0036500.mp4"
  },
  {
    "timestamp": "00:37 - 00:46",
    "context": "The instructor physically guides the operator's hand while painting. The operator says, \"I'm afraid you'll get tired.\" The instructor replies, \"No, no, no... It's fine.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the operator express concern that the instructor might get tired?",
    "answer": "Because the instructor was physically guiding the operator’s hand during the painting, which could be tiring.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01371.mp4",
    "question_id": "01371_7",
    "clip_path": "clips/01371/01371__0036500_0046500.mp4"
  },
  {
    "timestamp": "00:47 - 01:03",
    "context": "The operator asks, \"Do we still need to add some glaze?\" The instructor clarifies, \"This is the glaze.\"",
    "question_type": "Sound Source Identification",
    "question": "Who clarified, \"This is the glaze\"?",
    "answer": "The instructor.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01371.mp4",
    "question_id": "01371_8",
    "clip_path": "clips/01371/01371__0046500_0063500.mp4"
  },
  {
    "timestamp": "00:47 - 01:03",
    "context": "Impressed, the operator exclaims, \"Wow... The glaze is smooth...\"",
    "question_type": "Sound Characteristics",
    "question": "What was the emotional tone of the operator’s exclamation \"Wow\"?",
    "answer": "Impressed.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01371.mp4",
    "question_id": "01371_9",
    "clip_path": "clips/01371/01371__0046500_0063500.mp4"
  },
  {
    "timestamp": "00:47 - 01:03",
    "context": "The operator says, \"Oh... Okay, okay,\" and later adds another \"Okay,\" signaling completion.",
    "question_type": "Counting",
    "question": "How many times does the operator say \"Okay\" in this interval?",
    "answer": "Three times.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01371.mp4",
    "question_id": "01371_10",
    "clip_path": "clips/01371/01371__0046500_0063500.mp4"
  },
  {
    "timestamp": "00:14 - 00:36",
    "context": "While saying, \"Dip in some water, dip in some paint,\" the instructor dips a long, thin paintbrush into a white substance and demonstrates on the cup.",
    "question_type": "Cross-Modal Reasoning",
    "question": "When the instructor says, \"Dip in some water, dip in some paint,\" what concurrent action is shown?",
    "answer": "They dip a long, thin paintbrush into a white substance and demonstrate on the cup.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01371.mp4",
    "question_id": "01371_11",
    "clip_path": "clips/01371/01371__0013500_0036500.mp4"
  },
  {
    "timestamp": "01:03 - 01:09",
    "context": "The instructor takes the cup and brush: \"Let me check... Okay,\" and appears to do a quick touch-up.",
    "question_type": "Cross-Modal Reasoning",
    "question": "When the instructor says, \"Let me check... Okay,\" what do they do?",
    "answer": "They take the cup and brush to inspect the work and perform a quick touch-up.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01371.mp4",
    "question_id": "01371_12",
    "clip_path": "clips/01371/01371__0062500_0069500.mp4"
  },
  {
    "timestamp": "01:03 - 01:09",
    "context": "The instructor inspects and \"appears to do a quick touch-up.\"",
    "question_type": "Temporal Information",
    "question": "Was the inspection and touch-up brief or prolonged?",
    "answer": "Brief; it was a quick touch-up.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01371.mp4",
    "question_id": "01371_13",
    "clip_path": "clips/01371/01371__0062500_0069500.mp4"
  },
  {
    "timestamp": "01:03 - 01:09",
    "context": "The instructor concludes, \"In a moment, I'll take you to wash your hands... There's hot water now.\" The operator agrees, \"Okay.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the instructor suggest washing hands next?",
    "answer": "Because they had finished the glazing work and were preparing for the next step, with hot water available for cleaning.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01371.mp4",
    "question_id": "01371_14",
    "clip_path": "clips/01371/01371__0062500_0069500.mp4"
  },
  {
    "timestamp": "00:01 - 00:13",
    "context": "[00:01 - 00:13] A person passes a small ceramic animal figurine. The audio is a close-range, conversational-volume dialogue as they try to identify the animal by touch, guessing cat, then rabbit, before concluding it's a mouse.",
    "question_type": "Sound Characteristics",
    "question": "What were the proximity and volume characteristics of the dialogue during the figurine identification?",
    "answer": "Close-range and at conversational volume.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01372.mp4",
    "question_id": "01372_1",
    "clip_path": "clips/01372/01372__0000500_0013500.mp4"
  },
  {
    "timestamp": "00:01 - 00:13",
    "context": "[00:01 - 00:13] They discuss whether the figurine is a cat or a rabbit while feeling it, then conclude it's a mouse resembling a Disney character.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why were the two people debating whether the figurine was a cat or a rabbit?",
    "answer": "They were trying to identify the animal by touch after the figurine was passed to the camera wearer.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01372.mp4",
    "question_id": "01372_2",
    "clip_path": "clips/01372/01372__0000500_0013500.mp4"
  },
  {
    "timestamp": "00:13 - 00:26",
    "context": "[00:13 - 00:26] A small ceramic cup is handed over. In a clear, nearby voice, she introduces it as a cup and hints at a hidden feature while the camera wearer begins to feel its surface.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the woman's introduction about the cup originate relative to the camera?",
    "answer": "From nearby, at close range.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01372.mp4",
    "question_id": "01372_3",
    "clip_path": "clips/01372/01372__0012500_0026500.mp4"
  },
  {
    "timestamp": "00:26 - 00:43",
    "context": "[00:26 - 00:43] The camera wearer's fingers explore a raised feature on the cup; the other person guides their fingers. The wearer exclaims, “Ah, I know, it’s a small cat!” and the other confirms it’s lying on the cup.",
    "question_type": "Inferential & Contextual Causality",
    "question": "What prompted the camera wearer to exclaim, “Ah, I know, it’s a small cat!”?",
    "answer": "Guided tactile exploration of the raised feature on the cup led to realizing the hidden cat design.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01372.mp4",
    "question_id": "01372_4",
    "clip_path": "clips/01372/01372__0025500_0043500.mp4"
  },
  {
    "timestamp": "00:43 - 00:49",
    "context": "[00:43 - 00:49] A man with a deeper voice asks to feel the cup. The camera wearer passes it to her left; this pass is accompanied by soft rustling of sleeves.",
    "question_type": "Sound Source Identification",
    "question": "What generated the soft rustling sound during the pass?",
    "answer": "Sleeves rubbing as the cup was passed to the left.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01372.mp4",
    "question_id": "01372_5",
    "clip_path": "clips/01372/01372__0042500_0049500.mp4"
  },
  {
    "timestamp": "00:43 - 00:49",
    "context": "[00:43 - 00:49] The cup is passed to the camera wearer's left, out of frame, accompanied by soft sleeve rustling.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From which direction relative to the camera did the sleeve rustling during the handoff occur?",
    "answer": "To the left of the camera, out of frame.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01372.mp4",
    "question_id": "01372_6",
    "clip_path": "clips/01372/01372__0042500_0049500.mp4"
  },
  {
    "timestamp": "00:49 - 00:54",
    "context": "[00:49 - 00:54] After feeling the cup, the man exclaims “Wow!” in an impressed tone, praising its artistic quality.",
    "question_type": "Sound Characteristics",
    "question": "What was the tone quality of the man's exclamation “Wow!”?",
    "answer": "An impressed tone.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01372.mp4",
    "question_id": "01372_7",
    "clip_path": "clips/01372/01372__0048500_0054500.mp4"
  },
  {
    "timestamp": "00:49 - 00:54",
    "context": "[00:49 - 00:54] The camera wearer agrees and claps her hands together twice, producing two soft, distinct claps.",
    "question_type": "Counting",
    "question": "How many clapping sounds were produced?",
    "answer": "Two.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01372.mp4",
    "question_id": "01372_8",
    "clip_path": "clips/01372/01372__0048500_0054500.mp4"
  },
  {
    "timestamp": "00:49 - 00:54",
    "context": "[00:49 - 00:54] The camera wearer says it's amazing and then claps twice.",
    "question_type": "Sound Source Identification",
    "question": "Who produced the clapping sounds heard at the end of the clip?",
    "answer": "The camera wearer clapped her hands.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01372.mp4",
    "question_id": "01372_9",
    "clip_path": "clips/01372/01372__0048500_0054500.mp4"
  },
  {
    "timestamp": "00:49 - 00:54",
    "context": "[00:49 - 00:54] The man says “Wow!” and praises the cup; the camera wearer responds enthusiastically and claps twice to show appreciation.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the camera wearer clap her hands?",
    "answer": "To show appreciation for the craftsmanship of the cup.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01372.mp4",
    "question_id": "01372_10",
    "clip_path": "clips/01372/01372__0048500_0054500.mp4"
  },
  {
    "timestamp": "00:01 - 00:13",
    "context": "[00:01 - 00:13] They first guess cat, then rabbit, before concluding the figurine is a mouse.",
    "question_type": "Counting",
    "question": "How many initial animal guesses were made before concluding it was a mouse?",
    "answer": "Two guesses: cat and rabbit.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01372.mp4",
    "question_id": "01372_11",
    "clip_path": "clips/01372/01372__0000500_0013500.mp4"
  },
  {
    "timestamp": "00:49 - 00:54",
    "context": "[00:49 - 00:54] After feeling the cup, the man exclaims “Wow!”; afterward, the camera wearer claps twice.",
    "question_type": "Temporal Information",
    "question": "What was the sequence between the man's “Wow!” and the clapping?",
    "answer": "The clapping occurred after the man's exclamation.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01372.mp4",
    "question_id": "01372_12",
    "clip_path": "clips/01372/01372__0048500_0054500.mp4"
  },
  {
    "timestamp": "00:08 - 00:12",
    "context": "[00:08 - 00:12] A woman's voice from the left says, \"Okay, let's use hot water.\" Another woman replies, \"Okay.\" The first woman adds, \"I'll let it run for a bit,\" and turns on the faucet, starting a moderate-volume stream of water.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From which direction relative to the camera did the woman suggest, \"Okay, let's use hot water\"?",
    "answer": "From the left.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01374.mp4",
    "question_id": "01374_1",
    "clip_path": "clips/01374/01374__0007500_0012500.mp4"
  },
  {
    "timestamp": "00:08 - 00:12",
    "context": "[00:08 - 00:12] The first woman says, \"I'll let it run for a bit,\" while turning on the faucet to use hot water.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the woman say, \"I'll let it run for a bit\"?",
    "answer": "Because they intended to use hot water and she wanted to let it run to reach the desired temperature.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01374.mp4",
    "question_id": "01374_2",
    "clip_path": "clips/01374/01374__0007500_0012500.mp4"
  },
  {
    "timestamp": "00:08 - 00:12",
    "context": "[00:08 - 00:12] Her hand reaches out and turns on the faucet, initiating a moderate-volume stream of water.",
    "question_type": "Cross-Modal Reasoning",
    "question": "When her hand turns on the faucet, what sound begins?",
    "answer": "A moderate-volume stream of running water.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01374.mp4",
    "question_id": "01374_3",
    "clip_path": "clips/01374/01374__0007500_0012500.mp4"
  },
  {
    "timestamp": "00:12 - 00:21",
    "context": "[00:12 - 00:21] The user places their hands under the running water, creating a loud and continuous splashing sound as the stream hits their hands and the metal sink.",
    "question_type": "Sound Characteristics",
    "question": "How is the splashing sound described while the user rinses their hands?",
    "answer": "Loud and continuous.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01374.mp4",
    "question_id": "01374_4",
    "clip_path": "clips/01374/01374__0011500_0021500.mp4"
  },
  {
    "timestamp": "00:12 - 00:21",
    "context": "[00:12 - 00:21] The loud and continuous splashing occurs as the stream hits the user's hands and the metal sink.",
    "question_type": "Sound Source Identification",
    "question": "What specifically generated the loud, continuous splashing sound?",
    "answer": "The stream of water hitting the user's hands and the metal sink.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01374.mp4",
    "question_id": "01374_5",
    "clip_path": "clips/01374/01374__0011500_0021500.mp4"
  },
  {
    "timestamp": "00:12 - 00:21",
    "context": "[00:12 - 00:21] The woman on the left, also at the sink, says \"It's okay\" as the user begins washing, and later says, \"Okay, it's done.\"",
    "question_type": "Counting",
    "question": "How many times does the woman speak to guide or signal progress during the rinsing?",
    "answer": "Twice: she says \"It's okay\" and later \"Okay, it's done.\"",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01374.mp4",
    "question_id": "01374_6",
    "clip_path": "clips/01374/01374__0011500_0021500.mp4"
  },
  {
    "timestamp": "00:12 - 00:21",
    "context": "[00:12 - 00:21] The woman on the left speaks as the user begins washing: \"It's okay.\"",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From which side did the voice saying \"It's okay\" originate?",
    "answer": "From the left side at the sink.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01374.mp4",
    "question_id": "01374_7",
    "clip_path": "clips/01374/01374__0011500_0021500.mp4"
  },
  {
    "timestamp": "00:12 - 00:21",
    "context": "[00:12 - 00:21] The user thoroughly rinses their hands for several seconds before the woman says, \"Okay, it's done.\"",
    "question_type": "Temporal Information",
    "question": "Was the hand rinsing brief or did it last several seconds?",
    "answer": "It lasted several seconds.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01374.mp4",
    "question_id": "01374_8",
    "clip_path": "clips/01374/01374__0011500_0021500.mp4"
  },
  {
    "timestamp": "00:21 - 00:26",
    "context": "[00:12 - 00:21] The woman says, \"Okay, it's done.\" [00:23 - 00:26] After turning off the water, the user turns away from the sink.",
    "question_type": "Temporal Information",
    "question": "When was the water turned off relative to the statement \"Okay, it's done\"?",
    "answer": "Shortly after that statement; the water is off by 00:23 as the next action begins.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01374.mp4",
    "question_id": "01374_9",
    "clip_path": "clips/01374/01374__0020500_0026500.mp4"
  },
  {
    "timestamp": "00:23 - 00:26",
    "context": "[00:23 - 00:26] After turning off the water, an instructor approaches, gestures toward the user's wet hands, and says, \"Let me help you with that.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the instructor say, \"Let me help you with that\"?",
    "answer": "Because the user's hands were wet after handwashing, and the instructor was offering further assistance.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01374.mp4",
    "question_id": "01374_10",
    "clip_path": "clips/01374/01374__0022500_0026500.mp4"
  },
  {
    "timestamp": "00:23 - 00:26",
    "context": "[00:23 - 00:26] After turning off the water, an instructor approaches, gestures toward the user's wet hands, and speaks directly to them.",
    "question_type": "Temporal Information",
    "question": "When did the instructor speak relative to the water being turned off?",
    "answer": "Immediately after the water was turned off, as the user turned away from the sink.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01374.mp4",
    "question_id": "01374_11",
    "clip_path": "clips/01374/01374__0022500_0026500.mp4"
  },
  {
    "timestamp": "00:28 - 00:29",
    "context": "[00:28 - 00:29] As the user moves back into the studio, a voice from the background is heard clearly saying, \"Thanks,\" expressing gratitude for assistance.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Was the \"Thanks\" utterance close to the camera or from the background?",
    "answer": "From the background.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01374.mp4",
    "question_id": "01374_12",
    "clip_path": "clips/01374/01374__0027500_0029500.mp4"
  },
  {
    "timestamp": "00:28 - 00:29",
    "context": "[00:28 - 00:29] A voice from the background says, \"Thanks,\" expressing gratitude for assistance.",
    "question_type": "Inferential & Contextual Causality",
    "question": "What was the likely reason for the background voice saying \"Thanks\"?",
    "answer": "It was expressing gratitude for assistance.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01374.mp4",
    "question_id": "01374_13",
    "clip_path": "clips/01374/01374__0027500_0029500.mp4"
  },
  {
    "timestamp": "00:01 - 00:07",
    "context": "[00:01] A woman's hand gently places a small, white, unfired clay cup onto a wooden table, producing a very soft, low-volume thud.",
    "question_type": "Sound Source Identification",
    "question": "What generated the very soft, low-volume thud at the beginning?",
    "answer": "The unfired clay cup being gently placed onto the wooden table.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01379.mp4",
    "question_id": "01379_1",
    "clip_path": "clips/01379/01379__0000500_0007500.mp4"
  },
  {
    "timestamp": "00:01 - 00:07",
    "context": "[00:01] The placement of the unfired clay cup produces a very soft, low-volume thud that underscores its fragility.",
    "question_type": "Sound Characteristics",
    "question": "What were the acoustic qualities of the thud made when the cup was set down?",
    "answer": "A very soft, low-volume thud.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01379.mp4",
    "question_id": "01379_2",
    "clip_path": "clips/01379/01379__0000500_0007500.mp4"
  },
  {
    "timestamp": "00:01 - 00:07",
    "context": "[00:01 - 00:07] A female voice, originating from the front-right, instructs the camera wearer.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From what direction did the instructor’s voice originate during the initial instructions?",
    "answer": "From the front-right of the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01379.mp4",
    "question_id": "01379_3",
    "clip_path": "clips/01379/01379__0000500_0007500.mp4"
  },
  {
    "timestamp": "00:07 - 00:11",
    "context": "[00:07 - 00:11] The wearer gently touches the cup and lets out a soft, amazed 'Wow.'",
    "question_type": "Temporal Information",
    "question": "When did the wearer say 'Wow' in admiration?",
    "answer": "Between 00:07 and 00:11.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01379.mp4",
    "question_id": "01379_4",
    "clip_path": "clips/01379/01379__0006500_0011500.mp4"
  },
  {
    "timestamp": "00:07 - 00:11",
    "context": "[00:07 - 00:11] She lets out a soft, amazed 'Wow.'",
    "question_type": "Sound Characteristics",
    "question": "What was the volume of the wearer's 'Wow'?",
    "answer": "Soft.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01379.mp4",
    "question_id": "01379_5",
    "clip_path": "clips/01379/01379__0006500_0011500.mp4"
  },
  {
    "timestamp": "00:18 - 00:33",
    "context": "[00:18 - 00:33] The camera wearer calls to Nan-ge; the man's voice responds from the left.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From which direction did the man's voice come when he responded to the wearer?",
    "answer": "From the left side of the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01379.mp4",
    "question_id": "01379_6",
    "clip_path": "clips/01379/01379__0017500_0033500.mp4"
  },
  {
    "timestamp": "00:18 - 00:33",
    "context": "[00:18 - 00:33] The wearer repeatedly warns the man to be very gentle and not to pinch the fragile clay.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the wearer repeatedly warn the man not to pinch the cup?",
    "answer": "Because the cup is still unfired and soft, making it fragile and easy to damage.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01379.mp4",
    "question_id": "01379_7",
    "clip_path": "clips/01379/01379__0017500_0033500.mp4"
  },
  {
    "timestamp": "00:33 - 00:46",
    "context": "[00:33 - 00:46] The man questions if the object is a cup since its shape reminds him of a jar; the wearer laughs softly.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the wearer laugh softly after the man questioned whether it was a cup?",
    "answer": "She was amused by his playful doubt and questioning of her creation.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01379.mp4",
    "question_id": "01379_8",
    "clip_path": "clips/01379/01379__0032500_0046500.mp4"
  },
  {
    "timestamp": "00:55 - 01:09",
    "context": "[00:55 - 01:09] The man thinks he sees a crack; the wearer becomes alarmed and playfully accuses him of damaging it.",
    "question_type": "Inferential & Contextual Causality",
    "question": "What prompted the wearer's alarm and playful accusation toward the man?",
    "answer": "His remark about a possible crack made her think he had damaged the fragile cup.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01379.mp4",
    "question_id": "01379_9",
    "clip_path": "clips/01379/01379__0054500_0069500.mp4"
  },
  {
    "timestamp": "01:10 - 01:21",
    "context": "[01:10 - 01:21] The man discovers the 'crack' is a paper sticker; the wearer says 'OK' and takes back her cup.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the wearer take back her cup after saying 'OK'?",
    "answer": "Because the supposed crack was revealed to be just a serial number sticker, resolving her concern.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01379.mp4",
    "question_id": "01379_10",
    "clip_path": "clips/01379/01379__0069500_0081500.mp4"
  },
  {
    "timestamp": "01:23 - 01:32",
    "context": "[01:23 - 01:32] A third person, likely staff, speaks from the front-right to explain next steps.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From where did the staff member's voice originate during the transition explanation?",
    "answer": "From the front-right of the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01379.mp4",
    "question_id": "01379_11",
    "clip_path": "clips/01379/01379__0082500_0092500.mp4"
  },
  {
    "timestamp": "01:34 - 01:43",
    "context": "[01:34 - 01:43] The staff brings over a plate; the wearer infers it is for the paint.",
    "question_type": "Cross-Modal Reasoning",
    "question": "When the staff brought over a plate, what was it most likely for according to the wearer's inference?",
    "answer": "For holding paint.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01379.mp4",
    "question_id": "01379_12",
    "clip_path": "clips/01379/01379__0093500_0103500.mp4"
  },
  {
    "timestamp": "01:46 - 02:08",
    "context": "[01:46 - 02:08] The staff places a white bowl on the table with a soft clink.",
    "question_type": "Sound Source Identification",
    "question": "What object produced the soft clink when placed on the table?",
    "answer": "A white bowl.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01379.mp4",
    "question_id": "01379_13",
    "clip_path": "clips/01379/01379__0105500_0128500.mp4"
  },
  {
    "timestamp": "02:09 - 02:28",
    "context": "[02:09 - 02:28] The staff places another white cup on the table with a gentle thud and explains which surfaces to paint.",
    "question_type": "Sound Characteristics",
    "question": "What was the sound quality when the additional white cup was set down?",
    "answer": "A gentle thud.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01379.mp4",
    "question_id": "01379_14",
    "clip_path": "clips/01379/01379__0128500_0148500.mp4"
  },
  {
    "timestamp": "02:09 - 02:28",
    "context": "[02:09 - 02:28] The staff places another white cup and explains the bottom should not be painted, but the inside and outside should be fully coated.",
    "question_type": "Cross-Modal Reasoning",
    "question": "Why did the staff place another white cup on the table before explaining painting instructions?",
    "answer": "To use it as a visual aid while explaining which surfaces to paint and which to leave unpainted.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01379.mp4",
    "question_id": "01379_15",
    "clip_path": "clips/01379/01379__0128500_0148500.mp4"
  },
  {
    "timestamp": "01:46 - 02:28",
    "context": "[01:46] A white bowl is placed with a soft clink. [02:09] Another white cup is placed with a gentle thud.",
    "question_type": "Counting",
    "question": "How many distinct object-placement sounds on the table are described between 01:46 and 02:28?",
    "answer": "Two.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01379.mp4",
    "question_id": "01379_16",
    "clip_path": "clips/01379/01379__0105500_0148500.mp4"
  },
  {
    "timestamp": "02:45 - 03:14",
    "context": "[02:45 - 03:14] The staff prepares the paint palette and brush, making soft scraping and clinking sounds on the table.",
    "question_type": "Temporal Information",
    "question": "During what time interval are the soft scraping and clinking sounds from paint preparation heard?",
    "answer": "From 02:45 to 03:14.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01379.mp4",
    "question_id": "01379_17",
    "clip_path": "clips/01379/01379__0164500_0194500.mp4"
  },
  {
    "timestamp": "02:45 - 03:14",
    "context": "[02:45 - 03:14] Soft scraping and clinking sounds occur as the staff prepares the paint palette and brush.",
    "question_type": "Sound Source Identification",
    "question": "What actions generated the soft scraping and clinking sounds during this segment?",
    "answer": "Preparing the paint palette and brush on the table.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01379.mp4",
    "question_id": "01379_18",
    "clip_path": "clips/01379/01379__0164500_0194500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "[00:00 - 00:06] Inside a crowded elevator, the camera holder says, \"Hey, could someone please press the 7th floor for us, thank you?\" The narration notes this request is motivated by a need for assistance, likely because her hands are full or she cannot reach the button panel.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the camera holder ask someone to press the 7th-floor button?",
    "answer": "She needed assistance pressing the button, likely because her hands were full or she could not reach the panel.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01389.mp4",
    "question_id": "01389_1",
    "clip_path": "clips/01389/01389__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "[00:00 - 00:06] The audio captures her voice as the primary sound source as she makes the request.",
    "question_type": "Sound Source Identification",
    "question": "What was the primary sound source in the clip?",
    "answer": "The camera holder’s voice.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01389.mp4",
    "question_id": "01389_2",
    "clip_path": "clips/01389/01389__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "[00:00 - 00:06] The person holding the camera speaks in a clear, polite tone when making the request.",
    "question_type": "Sound Characteristics",
    "question": "What was the tone quality of the camera holder’s request?",
    "answer": "Clear and polite.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01389.mp4",
    "question_id": "01389_3",
    "clip_path": "clips/01389/01389__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "[00:00 - 00:06] The request is spoken by the person holding the camera.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the primary speech originate relative to the camera?",
    "answer": "From the camera holder at the camera’s position, very close to the microphone.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01389.mp4",
    "question_id": "01389_4",
    "clip_path": "clips/01389/01389__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "[00:00 - 00:06] The spoken request occurs at the start of the clip and is delivered as a single sentence.",
    "question_type": "Temporal Information",
    "question": "When does the initial request occur in the clip, and is it a single utterance?",
    "answer": "At the start of the clip (00:00–00:06), as a single, brief sentence.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01389.mp4",
    "question_id": "01389_5",
    "clip_path": "clips/01389/01389__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "[00:00 - 00:06] After someone agrees, the camera holder replies, \"Ah, okay, thank you,\" acknowledging the help.",
    "question_type": "Temporal Information",
    "question": "What speech immediately follows the agreement to the request?",
    "answer": "The camera holder says, \"Ah, okay, thank you.\"",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01389.mp4",
    "question_id": "01389_6",
    "clip_path": "clips/01389/01389__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "[00:00 - 00:06] The camera holder first makes the request, someone agrees, and then she expresses thanks.",
    "question_type": "Counting",
    "question": "How many separate utterances by the camera holder are captured?",
    "answer": "Two: the initial request and the subsequent “Ah, okay, thank you.”",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01389.mp4",
    "question_id": "01389_7",
    "clip_path": "clips/01389/01389__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "[00:00 - 00:06] The caption notes that someone agrees to the request, in addition to the camera holder speaking.",
    "question_type": "Counting",
    "question": "How many distinct speakers are heard in this exchange?",
    "answer": "Two: the camera holder and another person who agrees.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01389.mp4",
    "question_id": "01389_8",
    "clip_path": "clips/01389/01389__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "[00:00 - 00:06] The camera pans to reveal a woman in a red uniform and a child near the doors; following the request, someone agrees, and the camera holder thanks them.",
    "question_type": "Cross-Modal Reasoning",
    "question": "After the camera pans to people near the elevator doors, what does the audio suggest is about to happen?",
    "answer": "Someone will press the 7th-floor button for them.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01389.mp4",
    "question_id": "01389_9",
    "clip_path": "clips/01389/01389__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "A nurse and a doctor speak nearby about escorting a patient. The doctor says, \"He can probably see a little, but he can't see it himself... Just let me take him to the ultrasound room.\" The nurse replies, \"Okay, thank you.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the nurse thank the doctor at the end of their exchange?",
    "answer": "Because he offered to take the patient, who has limited vision, to the ultrasound room.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01391.mp4",
    "question_id": "01391_1",
    "clip_path": "clips/01391/01391__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:07 - 00:12",
    "context": "As the camera holder begins walking, a female voice from the camera's perspective says, \"So thoughtful!\"",
    "question_type": "Sound Source Identification",
    "question": "Who said the phrase \"So thoughtful!\" heard while walking down the corridor?",
    "answer": "The camera holder's female voice.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01391.mp4",
    "question_id": "01391_2",
    "clip_path": "clips/01391/01391__0006500_0012500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "The nurse and doctor converse nearby about escorting a patient, spoken at a normal, clear volume from close proximity.",
    "question_type": "Sound Characteristics",
    "question": "What were the volume and clarity of the nurse–doctor conversation at the start?",
    "answer": "It was spoken at a normal, clear volume from close proximity.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01391.mp4",
    "question_id": "01391_3",
    "clip_path": "clips/01391/01391__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:07 - 00:12",
    "context": "After the camera holder speaks, the nurse's voice is heard asking, \"Sister-in-law, did you bring the ID card?\" from slightly behind.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From where relative to the camera did the nurse ask about the ID card?",
    "answer": "From slightly behind the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01391.mp4",
    "question_id": "01391_4",
    "clip_path": "clips/01391/01391__0006500_0012500.mp4"
  },
  {
    "timestamp": "00:13 - 00:17",
    "context": "A non-diegetic, female text-to-speech voiceover plays: \"The experience is so good. Not nervous at all.\"",
    "question_type": "Temporal Information",
    "question": "When does the non-diegetic voiceover occur, and is it brief or continuous?",
    "answer": "Between 00:13 and 00:17, and it is brief, spanning that 4-second interval.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01391.mp4",
    "question_id": "01391_5",
    "clip_path": "clips/01391/01391__0012500_0017500.mp4"
  },
  {
    "timestamp": "00:07 - 00:12",
    "context": "Camera holder: \"So thoughtful!\" Nurse: \"Did you bring the ID card?\" Camera holder: \"Ah, okay, thank you,\" followed by \"Mm-hmm.\"",
    "question_type": "Counting",
    "question": "How many separate utterances did the camera holder make in this interval?",
    "answer": "Three utterances.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01391.mp4",
    "question_id": "01391_6",
    "clip_path": "clips/01391/01391__0006500_0012500.mp4"
  },
  {
    "timestamp": "00:07 - 00:12",
    "context": "The camera holder praises the doctor with, \"So thoughtful!\" immediately after his offer to escort the patient.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the camera holder exclaim \"So thoughtful!\"?",
    "answer": "She was reacting to the doctor’s kindness in offering to escort the patient.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01391.mp4",
    "question_id": "01391_7",
    "clip_path": "clips/01391/01391__0006500_0012500.mp4"
  },
  {
    "timestamp": "00:13 - 00:17",
    "context": "A non-diegetic, female text-to-speech voiceover clearly states: \"The experience is so good. Not nervous at all.\"",
    "question_type": "Sound Source Identification",
    "question": "What generated the voice that summarizes the positive experience near the end?",
    "answer": "A non-diegetic female text-to-speech voiceover.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01391.mp4",
    "question_id": "01391_8",
    "clip_path": "clips/01391/01391__0012500_0017500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "The doctor walks into frame and says, \"He can probably see a little... Just let me take him to the ultrasound room.\"",
    "question_type": "Temporal Information",
    "question": "During which time window does the doctor state he will take the patient to the ultrasound room?",
    "answer": "00:00 to 00:07.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01391.mp4",
    "question_id": "01391_9",
    "clip_path": "clips/01391/01391__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:00 - 00:17",
    "context": "Voices heard include a nurse, a male doctor, the camera holder’s female voice, and a non-diegetic female TTS voiceover.",
    "question_type": "Counting",
    "question": "How many distinct speaking voices are heard across the entire clip?",
    "answer": "Four voices: the nurse, the doctor, the camera holder, and a female text-to-speech voiceover.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01391.mp4",
    "question_id": "01391_10",
    "clip_path": "clips/01391/01391__0000000_0017500.mp4"
  },
  {
    "timestamp": "00:00 - 00:32",
    "context": "[00:00 - 00:32] Two individuals converse in front of the elevator call panel in a moderate, clear tone, with faint music from a nearby screen audible.",
    "question_type": "Sound Characteristics",
    "question": "How is the tone of the conversation described during the wait?",
    "answer": "Moderate and clear.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01392.mp4",
    "question_id": "01392_1",
    "clip_path": "clips/01392/01392__0000000_0032500.mp4"
  },
  {
    "timestamp": "00:00 - 00:32",
    "context": "[00:00 - 00:32] Faint music is heard in the background while the two individuals talk, noted as coming from a nearby screen.",
    "question_type": "Sound Source Identification",
    "question": "What is the source of the faint background music during the conversation?",
    "answer": "A nearby screen.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01392.mp4",
    "question_id": "01392_2",
    "clip_path": "clips/01392/01392__0000000_0032500.mp4"
  },
  {
    "timestamp": "00:00 - 00:32",
    "context": "[00:00 - 00:32] The conversation continues while the elevator is shown ascending on the panel.",
    "question_type": "Temporal Information",
    "question": "How long does the conversation last before the elevator arrives?",
    "answer": "32 seconds.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01392.mp4",
    "question_id": "01392_3",
    "clip_path": "clips/01392/01392__0000000_0032500.mp4"
  },
  {
    "timestamp": "00:00 - 00:32",
    "context": "[00:00 - 00:32] One speaker says, \"When I come back, I can come back by myself... It's congenital... We generally do things ourselves if we can.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why does the speaker say, \"We generally do things ourselves if we can\"?",
    "answer": "To emphasize their independence despite a congenital condition.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01392.mp4",
    "question_id": "01392_4",
    "clip_path": "clips/01392/01392__0000000_0032500.mp4"
  },
  {
    "timestamp": "00:00 - 00:32",
    "context": "[00:00 - 00:32] The red digital floor indicator changes from 4 to 5 to 6 to 7, showing the elevator is approaching.",
    "question_type": "Cross-Modal Reasoning",
    "question": "What visual cue indicated that an arrival sound would happen soon?",
    "answer": "The floor indicator ascending from 4 to 7 signaled the elevator was approaching, foreshadowing the arrival chime.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01392.mp4",
    "question_id": "01392_5",
    "clip_path": "clips/01392/01392__0000000_0032500.mp4"
  },
  {
    "timestamp": "00:32 - 00:35",
    "context": "[00:32 - 00:35] A loud, distinct \"ding\" chime signals the elevator's arrival.",
    "question_type": "Sound Characteristics",
    "question": "What is the quality and volume of the elevator’s arrival sound?",
    "answer": "A loud, distinct \"ding\".",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01392.mp4",
    "question_id": "01392_6",
    "clip_path": "clips/01392/01392__0031500_0035500.mp4"
  },
  {
    "timestamp": "00:32 - 00:35",
    "context": "[00:32 - 00:35] The \"ding\" chime emanates from directly in front.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the chime originate relative to the camera?",
    "answer": "Directly in front of the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01392.mp4",
    "question_id": "01392_7",
    "clip_path": "clips/01392/01392__0031500_0035500.mp4"
  },
  {
    "timestamp": "00:32 - 00:35",
    "context": "[00:32 - 00:35] Immediately after the chime, a person exclaims, \"电梯来啦 (The elevator is here)!\"",
    "question_type": "Temporal Information",
    "question": "What speech immediately followed the arrival chime?",
    "answer": "A person exclaimed, \"电梯来啦 (The elevator is here)!\"",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01392.mp4",
    "question_id": "01392_8",
    "clip_path": "clips/01392/01392__0031500_0035500.mp4"
  },
  {
    "timestamp": "00:32 - 00:35",
    "context": "[00:32 - 00:35] The chime sounds and the stainless steel elevator doors slide open, revealing two nurses inside (one next to a wheeled utility cart).",
    "question_type": "Cross-Modal Reasoning",
    "question": "After the chime, what visual event does the audio cue correspond to?",
    "answer": "The doors open, revealing two nurses inside, one beside a large wheeled utility cart.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01392.mp4",
    "question_id": "01392_9",
    "clip_path": "clips/01392/01392__0031500_0035500.mp4"
  },
  {
    "timestamp": "00:35 - 00:42",
    "context": "[00:35 - 00:42] As the nurses begin to exit, the utility cart’s wheels produce a moderate rattling sound on the floor.",
    "question_type": "Sound Source Identification",
    "question": "What produced the moderate rattling sound during the nurses’ exit?",
    "answer": "The wheels of the utility cart on the floor.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01392.mp4",
    "question_id": "01392_10",
    "clip_path": "clips/01392/01392__0034500_0042184.mp4"
  },
  {
    "timestamp": "00:35 - 00:42",
    "context": "[00:35 - 00:42] The utility cart’s wheels are heard with a moderate rattling as the nurses exit.",
    "question_type": "Sound Characteristics",
    "question": "How is the utility cart’s wheel noise described?",
    "answer": "A moderate rattling sound.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01392.mp4",
    "question_id": "01392_11",
    "clip_path": "clips/01392/01392__0034500_0042184.mp4"
  },
  {
    "timestamp": "00:35 - 00:42",
    "context": "[00:35 - 00:42] One nurse asks the filmer, \"你到哪 (Where are you going?)\" while coordinating movement in the doorway.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why does the nurse ask, \"Where are you going?\"",
    "answer": "To coordinate movements as they exit while the filmer intends to enter.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01392.mp4",
    "question_id": "01392_12",
    "clip_path": "clips/01392/01392__0034500_0042184.mp4"
  },
  {
    "timestamp": "00:35 - 00:42",
    "context": "[00:35 - 00:42] After the nurse speaks, the camera holder moves forward, crossing the threshold to enter the now-empty elevator.",
    "question_type": "Cross-Modal Reasoning",
    "question": "Following the nurse’s question, what action indicates the camera holder’s intention?",
    "answer": "They move forward and enter the now-empty elevator, indicating they plan to ride it.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01392.mp4",
    "question_id": "01392_13",
    "clip_path": "clips/01392/01392__0034500_0042184.mp4"
  },
  {
    "timestamp": "00:32 - 00:42",
    "context": "[00:32 - 00:42] Two spoken lines are heard: the exclamation \"The elevator is here!\" and the nurse’s question \"Where are you going?\"",
    "question_type": "Counting",
    "question": "How many distinct spoken utterances are heard after the elevator arrives?",
    "answer": "Two.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01392.mp4",
    "question_id": "01392_14",
    "clip_path": "clips/01392/01392__0031500_0042184.mp4"
  },
  {
    "timestamp": "00:32 - 00:35",
    "context": "[00:32 - 00:35] A single, loud \"ding\" signals the elevator’s arrival.",
    "question_type": "Counting",
    "question": "How many chime sounds are heard signaling the elevator’s arrival?",
    "answer": "One.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01392.mp4",
    "question_id": "01392_15",
    "clip_path": "clips/01392/01392__0031500_0035500.mp4"
  },
  {
    "timestamp": "00:00 - 00:05",
    "context": "[00:00 - 00:05] A clear, female voice begins to speak from the user's perspective, saying: \"其实盲道上的障碍不光只有车\". Light, ambient city traffic is faintly audible in the background.",
    "question_type": "Sound Source Identification",
    "question": "Who produced the clear speech heard at the start of the video?",
    "answer": "A clear, female speaker (the narrator) speaking from the user's perspective.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01396.mp4",
    "question_id": "01396_1",
    "clip_path": "clips/01396/01396__0000000_0005500.mp4"
  },
  {
    "timestamp": "00:00 - 00:05",
    "context": "[00:00 - 00:05] Light, ambient city traffic is faintly audible in the background while the speaker talks.",
    "question_type": "Sound Characteristics",
    "question": "What is the volume and character of the ambient city traffic heard at the start?",
    "answer": "It is faint and light background traffic noise.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01396.mp4",
    "question_id": "01396_2",
    "clip_path": "clips/01396/01396__0000000_0005500.mp4"
  },
  {
    "timestamp": "00:05 - 00:21",
    "context": "[00:05 - 00:21] The user's white cane enters from the bottom right and makes soft tapping sounds as it touches the ground.",
    "question_type": "Sound Source Identification",
    "question": "What generated the soft tapping sounds between 00:05 and 00:21?",
    "answer": "The user's white cane contacting the ground.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01396.mp4",
    "question_id": "01396_3",
    "clip_path": "clips/01396/01396__0004500_0021500.mp4"
  },
  {
    "timestamp": "00:05 - 00:21",
    "context": "[00:05 - 00:21] The cane enters the frame from the bottom right and taps the ground.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From which direction relative to the camera did the cane’s tapping originate when it first appeared?",
    "answer": "From the bottom right relative to the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01396.mp4",
    "question_id": "01396_4",
    "clip_path": "clips/01396/01396__0004500_0021500.mp4"
  },
  {
    "timestamp": "00:05 - 00:21",
    "context": "[00:05 - 00:21] The speaker points out a manhole cover interrupting the tactile path and explains a right–left–left–right sequence to rejoin the path.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the speaker describe a sequence of turns (right, left, left, right)?",
    "answer": "Because a manhole cover interrupts the tactile path, requiring a detour to get back on the path.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01396.mp4",
    "question_id": "01396_5",
    "clip_path": "clips/01396/01396__0004500_0021500.mp4"
  },
  {
    "timestamp": "00:05 - 00:21",
    "context": "[00:05 - 00:21] The cane continues to tap the ground, audibly tracing the path she describes while explaining the detour.",
    "question_type": "Temporal Information",
    "question": "Is the cane tapping brief or continuous during the speaker’s detour explanation?",
    "answer": "It is continuous throughout this interval.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01396.mp4",
    "question_id": "01396_6",
    "clip_path": "clips/01396/01396__0004500_0021500.mp4"
  },
  {
    "timestamp": "00:21 - 00:27",
    "context": "[00:21 - 00:27] While walking, the camera reveals another manhole cover further down the path, and she says, \"又有一个井盖.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why does the speaker remark, \"又有一个井盖\" during this segment?",
    "answer": "Because another manhole cover becomes visible further down the path.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01396.mp4",
    "question_id": "01396_7",
    "clip_path": "clips/01396/01396__0020500_0027500.mp4"
  },
  {
    "timestamp": "00:05 - 00:21",
    "context": "[00:05 - 00:21] The speaker says: \"我应该先右转, 然后再左转, 然后再左转, 然后再右转.\"",
    "question_type": "Counting",
    "question": "How many directional turns does the speaker say are needed to rejoin the tactile path?",
    "answer": "Four turns.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01396.mp4",
    "question_id": "01396_8",
    "clip_path": "clips/01396/01396__0004500_0021500.mp4"
  },
  {
    "timestamp": "00:21 - 00:27",
    "context": "[00:05 - 00:21] A manhole cover interrupts the tactile path. [00:21 - 00:27] The camera reveals another one, and she says, \"又有一个井盖.\"",
    "question_type": "Counting",
    "question": "By the end of 00:27, how many manhole covers have been mentioned or shown as obstacles?",
    "answer": "Two manhole covers.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01396.mp4",
    "question_id": "01396_9",
    "clip_path": "clips/01396/01396__0020500_0027500.mp4"
  },
  {
    "timestamp": "00:27 - 00:31",
    "context": "[00:27 - 00:31] The user continues forward, with the white cane tapping rhythmically on the tactile paving along a narrow walkway.",
    "question_type": "Sound Characteristics",
    "question": "How is the cane’s tapping characterized in this segment?",
    "answer": "It is rhythmic tapping on the tactile paving.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01396.mp4",
    "question_id": "01396_10",
    "clip_path": "clips/01396/01396__0026500_0031500.mp4"
  },
  {
    "timestamp": "00:21 - 00:27",
    "context": "[00:21 - 00:27] The speaker says: \"但是在绕这个井盖的过程中, 我自己前进的方向可能就已经变了.\"",
    "question_type": "Temporal Information",
    "question": "When does the speaker mention that her sense of forward direction might have changed?",
    "answer": "During 00:21 to 00:27.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01396.mp4",
    "question_id": "01396_11",
    "clip_path": "clips/01396/01396__0020500_0027500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "[00:00 - 00:04] A clear female voice from the user's position explains the tactile paving. Footsteps are audible as a white cane appears, tapping the ground.",
    "question_type": "Sound Source Identification",
    "question": "What generated the tapping sound heard at the start of the video?",
    "answer": "The white cane tapping the ground.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01398.mp4",
    "question_id": "01398_1",
    "clip_path": "clips/01398/01398__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "[00:00 - 00:04] A clear female voice from the user's position explains the function of the tactile paving.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the clear female voice originate relative to the camera?",
    "answer": "From the user's position, close to the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01398.mp4",
    "question_id": "01398_2",
    "clip_path": "clips/01398/01398__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:04 - 00:11",
    "context": "[00:04 - 00:11] The cane encounters a metal manhole cover and produces a series of three distinct, sharp, metallic tapping sounds.",
    "question_type": "Counting",
    "question": "How many distinct metallic tapping sounds occurred when the cane struck the manhole cover?",
    "answer": "Three.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01398.mp4",
    "question_id": "01398_3",
    "clip_path": "clips/01398/01398__0003500_0011500.mp4"
  },
  {
    "timestamp": "00:04 - 00:11",
    "context": "[00:04 - 00:11] The cane strikes a metal manhole cover, producing distinct, sharp, metallic tapping sounds.",
    "question_type": "Sound Characteristics",
    "question": "What were the characteristics of the tapping sounds made when the cane struck the manhole cover?",
    "answer": "They were distinct, sharp, and metallic.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01398.mp4",
    "question_id": "01398_4",
    "clip_path": "clips/01398/01398__0003500_0011500.mp4"
  },
  {
    "timestamp": "00:04 - 00:11",
    "context": "[00:04 - 00:11] After the cane produces metallic taps on the cover, the user says it seems to be the sound of a manhole cover.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the user conclude that it seemed to be a manhole cover?",
    "answer": "Because the cane produced sharp, metallic tapping when striking it, indicating a metal manhole cover.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01398.mp4",
    "question_id": "01398_5",
    "clip_path": "clips/01398/01398__0003500_0011500.mp4"
  },
  {
    "timestamp": "00:12 - 00:23",
    "context": "[00:12 - 00:23] The user continuously probes the tactile path; the cane creates a rhythmic, scraping and tapping sound as it sweeps from side to side.",
    "question_type": "Temporal Information",
    "question": "During 00:12 - 00:23, was the cane's scraping and tapping sound continuous or intermittent?",
    "answer": "Continuous and rhythmic throughout the interval.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01398.mp4",
    "question_id": "01398_6",
    "clip_path": "clips/01398/01398__0011500_0023500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "[00:00 - 00:04] Footsteps are audible as the cane taps the ground.",
    "question_type": "Counting",
    "question": "Excluding speech, how many types of sounds are audible simultaneously at the start?",
    "answer": "Two: footsteps and cane tapping.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01398.mp4",
    "question_id": "01398_7",
    "clip_path": "clips/01398/01398__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:35 - 00:45",
    "context": "[00:35 - 00:45] The user reflects: the tactile path bypasses several manhole covers to avoid navigating around each one and getting flustered.",
    "question_type": "Inferential & Contextual Causality",
    "question": "According to the user's reflection, what is the purpose of the tactile path bypassing the manhole covers?",
    "answer": "To prevent users from having to navigate around each cover individually and to avoid getting flustered.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01398.mp4",
    "question_id": "01398_8",
    "clip_path": "clips/01398/01398__0034500_0045300.mp4"
  },
  {
    "timestamp": "00:00 - 00:02",
    "context": "[00:00 - 00:02] She audibly extends a telescopic white cane, producing a series of soft, sliding metallic clicks, and says: “我这边不太熟，所以我要拿盲杖” (I'm not very familiar with this place, so I need to use my white cane).",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did she say she needed to use her white cane at the start?",
    "answer": "Because she was not very familiar with the place.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01404.mp4",
    "question_id": "01404_1",
    "clip_path": "clips/01404/01404__0000000_0002500.mp4"
  },
  {
    "timestamp": "00:00 - 00:02",
    "context": "[00:00 - 00:02] She extends a telescopic white cane, producing a series of soft, sliding metallic clicks.",
    "question_type": "Sound Source Identification",
    "question": "What generated the series of soft, sliding metallic clicks at the start?",
    "answer": "The telescopic white cane being extended.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01404.mp4",
    "question_id": "01404_2",
    "clip_path": "clips/01404/01404__0000000_0002500.mp4"
  },
  {
    "timestamp": "00:00 - 00:02",
    "context": "[00:00 - 00:02] A series of soft, sliding metallic clicks is heard as the cane is extended.",
    "question_type": "Sound Characteristics",
    "question": "What were the acoustic qualities of the cane-extension sounds?",
    "answer": "They were soft, sliding metallic clicks occurring in a series.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01404.mp4",
    "question_id": "01404_3",
    "clip_path": "clips/01404/01404__0000000_0002500.mp4"
  },
  {
    "timestamp": "00:02 - 00:06",
    "context": "[00:02 - 00:06] She touches a pack of coffee filters with her left hand, creating a distinct, soft rustling sound, while saying: “滤纸、咖啡豆、手冲壶”.",
    "question_type": "Sound Source Identification",
    "question": "What action produced the distinct, soft rustling sound?",
    "answer": "Touching a pack of coffee filters with her left hand.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01404.mp4",
    "question_id": "01404_4",
    "clip_path": "clips/01404/01404__0001500_0006500.mp4"
  },
  {
    "timestamp": "00:02 - 00:06",
    "context": "[00:02 - 00:06] A distinct, soft rustling is heard as her hand contacts the filter pack.",
    "question_type": "Sound Characteristics",
    "question": "How is the rustling sound from the coffee filters described?",
    "answer": "Distinct and soft.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01404.mp4",
    "question_id": "01404_5",
    "clip_path": "clips/01404/01404__0001500_0006500.mp4"
  },
  {
    "timestamp": "00:06 - 00:10",
    "context": "[00:06 - 00:10] She taps a black electric kettle; the tapping produces a light, dull thud on plastic surfaces while she says: “这个是烧水壶... 我的细口壶”.",
    "question_type": "Sound Characteristics",
    "question": "What was the quality of the sound produced when she tapped the electric kettle?",
    "answer": "A light, dull thud on plastic surfaces.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01404.mp4",
    "question_id": "01404_6",
    "clip_path": "clips/01404/01404__0005500_0010500.mp4"
  },
  {
    "timestamp": "00:10 - 00:13",
    "context": "[00:10 - 00:13] She adjusts her grip on the cane and gestures toward a red coffee grinder, saying: “有点挡住... 这个是磨豆机”.",
    "question_type": "Cross-Modal Reasoning",
    "question": "When she gestured toward the red device, what object did she identify?",
    "answer": "A coffee grinder.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01404.mp4",
    "question_id": "01404_7",
    "clip_path": "clips/01404/01404__0009500_0013500.mp4"
  },
  {
    "timestamp": "00:14 - 00:17",
    "context": "[00:14 - 00:17] A white Samoyed dog walks into frame from the right; its paws make very faint, soft padding sounds. She says: “这边有张桌子，还有狗子”.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From which direction relative to the camera did the dog's approach—and thus the paw sounds—come?",
    "answer": "From the right side.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01404.mp4",
    "question_id": "01404_8",
    "clip_path": "clips/01404/01404__0013500_0017500.mp4"
  },
  {
    "timestamp": "00:14 - 00:17",
    "context": "[00:14 - 00:17] The dog's paws make very faint, soft padding sounds as it walks in.",
    "question_type": "Sound Characteristics",
    "question": "What are the volume and texture of the dog's footstep sounds?",
    "answer": "Very faint, soft padding sounds.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01404.mp4",
    "question_id": "01404_9",
    "clip_path": "clips/01404/01404__0013500_0017500.mp4"
  },
  {
    "timestamp": "00:18 - 00:22",
    "context": "[00:18 - 00:22] Approaching a white desk streaming setup, the tip of her white cane makes a few light, sharp tapping sounds on the floor.",
    "question_type": "Counting",
    "question": "Were the cane-tip taps a single strike or multiple strikes?",
    "answer": "Multiple—a few light, sharp taps.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01404.mp4",
    "question_id": "01404_10",
    "clip_path": "clips/01404/01404__0017500_0022500.mp4"
  },
  {
    "timestamp": "00:18 - 00:22",
    "context": "[00:18 - 00:22] The tip of the white cane contacts the floor in front of the desk, producing light, sharp taps.",
    "question_type": "Sound Source Identification",
    "question": "What produced the light, sharp tapping sounds near the desk?",
    "answer": "The tip of her white cane contacting the floor in front of the desk.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01404.mp4",
    "question_id": "01404_11",
    "clip_path": "clips/01404/01404__0017500_0022500.mp4"
  },
  {
    "timestamp": "00:18 - 00:22",
    "context": "[00:18 - 00:22] As she approaches the desk, a few light, sharp taps from the cane are heard.",
    "question_type": "Temporal Information",
    "question": "When did the cane-tip tapping occur relative to her movement toward the desk?",
    "answer": "Between 00:18 and 00:22, as she approached the desk.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01404.mp4",
    "question_id": "01404_12",
    "clip_path": "clips/01404/01404__0017500_0022500.mp4"
  },
  {
    "timestamp": "00:22 - 00:27",
    "context": "[00:22 - 00:27] Her left hand makes contact with the microphone stand, creating a very subtle brushing sound, while she identifies desk items.",
    "question_type": "Sound Source Identification",
    "question": "What contact produced the brushing sound at the desk?",
    "answer": "Her left hand contacting the microphone stand.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01404.mp4",
    "question_id": "01404_13",
    "clip_path": "clips/01404/01404__0021500_0027500.mp4"
  },
  {
    "timestamp": "00:22 - 00:27",
    "context": "[00:22 - 00:27] A very subtle brushing sound is heard as her hand touches the microphone stand.",
    "question_type": "Sound Characteristics",
    "question": "How is the brushing sound described when she touches the microphone stand?",
    "answer": "Very subtle.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01404.mp4",
    "question_id": "01404_14",
    "clip_path": "clips/01404/01404__0021500_0027500.mp4"
  },
  {
    "timestamp": "00:04 - 00:06",
    "context": "While audibly searching and repeating, \"Toilet, toilet,\" the user places their left hand on a white wall under a staircase, producing soft, rhythmic patting sounds as their hand slides along the wall, suggesting they are using touch to navigate.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why was the user patting the wall and repeating \"Toilet, toilet\" during 00:04 - 00:06?",
    "answer": "They were audibly searching and using touch to navigate toward the toilet.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01406.mp4",
    "question_id": "01406_1",
    "clip_path": "clips/01406/01406__0003500_0006500.mp4"
  },
  {
    "timestamp": "00:04 - 00:06",
    "context": "The user places their left hand on a white wall under a staircase, producing a series of soft, rhythmic patting sounds as their hand slides along the wall.",
    "question_type": "Sound Source Identification",
    "question": "What generated the soft, rhythmic patting sounds between 00:04 and 00:06?",
    "answer": "The user's left hand sliding and patting against the wall under the staircase.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01406.mp4",
    "question_id": "01406_2",
    "clip_path": "clips/01406/01406__0003500_0006500.mp4"
  },
  {
    "timestamp": "00:04 - 00:06",
    "context": "Soft, rhythmic patting sounds are produced as the user's hand slides along the wall while searching.",
    "question_type": "Sound Characteristics",
    "question": "What is the acoustic quality of the patting sounds made during 00:04 - 00:06?",
    "answer": "They are soft and rhythmic.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01406.mp4",
    "question_id": "01406_3",
    "clip_path": "clips/01406/01406__0003500_0006500.mp4"
  },
  {
    "timestamp": "00:04 - 00:06",
    "context": "A series of soft, rhythmic patting sounds occurs as the user's hand moves along the wall while searching.",
    "question_type": "Temporal Information",
    "question": "When do the patting sounds occur and how long do they last?",
    "answer": "They occur between 00:04 and 00:06 and persist throughout that interval.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01406.mp4",
    "question_id": "01406_4",
    "clip_path": "clips/01406/01406__0003500_0006500.mp4"
  },
  {
    "timestamp": "00:07 - 00:09",
    "context": "The user's hand continues to slide along the wall, producing a soft rubbing sound.",
    "question_type": "Sound Characteristics",
    "question": "What is the quality of the rubbing sound heard at 00:07 - 00:09?",
    "answer": "It is a soft rubbing sound.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01406.mp4",
    "question_id": "01406_5",
    "clip_path": "clips/01406/01406__0006500_0009500.mp4"
  },
  {
    "timestamp": "00:07 - 00:09",
    "context": "Upon realizing they have passed the correct turn, the user says, \"This is the stairs, I've gone too far,\" acknowledging their navigational error.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the user say, \"I've gone too far,\" at 00:07 - 00:09?",
    "answer": "They realized they had reached the stairs and had passed the correct turn for the bathroom.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01406.mp4",
    "question_id": "01406_6",
    "clip_path": "clips/01406/01406__0006500_0009500.mp4"
  },
  {
    "timestamp": "00:04 - 00:06",
    "context": "While searching, the user repeats, \"Toilet, toilet,\" as they move along the wall.",
    "question_type": "Counting",
    "question": "How many times did the user say the word \"toilet\" while searching between 00:04 and 00:06?",
    "answer": "Twice.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01406.mp4",
    "question_id": "01406_7",
    "clip_path": "clips/01406/01406__0003500_0006500.mp4"
  },
  {
    "timestamp": "00:11 - 00:13",
    "context": "After turning back, the user approaches a wooden door and pushes it open with their left hand, which makes a faint rustling sound upon contact.",
    "question_type": "Sound Source Identification",
    "question": "What produced the faint rustling sound at 00:11 - 00:13?",
    "answer": "The wooden door making contact with the user's left hand as it was pushed open.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01406.mp4",
    "question_id": "01406_8",
    "clip_path": "clips/01406/01406__0010500_0013500.mp4"
  },
  {
    "timestamp": "00:11 - 00:13",
    "context": "The user pushes the wooden door open with their left hand, producing a faint rustling sound upon contact.",
    "question_type": "Temporal Information",
    "question": "When did the faint rustling sound occur, and was it brief or sustained?",
    "answer": "It occurred between 00:11 and 00:13 upon contact with the door and was brief.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01406.mp4",
    "question_id": "01406_9",
    "clip_path": "clips/01406/01406__0010500_0013500.mp4"
  },
  {
    "timestamp": "00:11 - 00:13",
    "context": "Upon seeing the bathroom inside after opening the door, the user says, \"It's here.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the user say, \"It's here,\" at 00:11 - 00:13?",
    "answer": "They had opened the door, seen the bathroom inside, and were confirming they found it.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01406.mp4",
    "question_id": "01406_10",
    "clip_path": "clips/01406/01406__0010500_0013500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "The user speaks near the entryway and upbeat background music is audible.",
    "question_type": "Sound Characteristics",
    "question": "What is the character of the background music heard at 00:00 - 00:03?",
    "answer": "It is upbeat background music.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01406.mp4",
    "question_id": "01406_11",
    "clip_path": "clips/01406/01406__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:00 - 00:02",
    "context": "A series of two sharp, metallic clinking sounds originate from directly in front of the camera as the user adjusts a long, silver pole in preparation for ascending the stairs.",
    "question_type": "Counting",
    "question": "How many sharp, metallic clinking sounds were heard at the start?",
    "answer": "Two.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01409.mp4",
    "question_id": "01409_1",
    "clip_path": "clips/01409/01409__0000000_0002500.mp4"
  },
  {
    "timestamp": "00:00 - 00:02",
    "context": "A series of two sharp, metallic clinking sounds originate from directly in front of the camera as the user adjusts a long, silver pole in preparation for ascending the stairs.",
    "question_type": "Sound Source Identification",
    "question": "What generated the clinking sounds at 00:00–00:02?",
    "answer": "The long, silver pole the user was adjusting.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01409.mp4",
    "question_id": "01409_2",
    "clip_path": "clips/01409/01409__0000000_0002500.mp4"
  },
  {
    "timestamp": "00:00 - 00:02",
    "context": "The clinking sounds originate from directly in front of the camera.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the clinking originate relative to the camera?",
    "answer": "Directly in front of the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01409.mp4",
    "question_id": "01409_3",
    "clip_path": "clips/01409/01409__0000000_0002500.mp4"
  },
  {
    "timestamp": "00:02 - 00:08",
    "context": "Soft, rhythmic thuds are produced by the user's footsteps on the wooden stairs. Concurrently, a female voice says, \"Slowly, slowly,\" as a self-reminder to be careful.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the speaker say \"Slowly, slowly\" while walking upstairs?",
    "answer": "As a self-reminder to be careful while going up the stairs.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01409.mp4",
    "question_id": "01409_4",
    "clip_path": "clips/01409/01409__0001500_0008500.mp4"
  },
  {
    "timestamp": "00:02 - 00:08",
    "context": "Soft, rhythmic thuds are produced by the user's footsteps on the wooden stairs.",
    "question_type": "Sound Characteristics",
    "question": "How are the user's footsteps described while going upstairs?",
    "answer": "As soft, rhythmic thuds on the wooden stairs.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01409.mp4",
    "question_id": "01409_5",
    "clip_path": "clips/01409/01409__0001500_0008500.mp4"
  },
  {
    "timestamp": "00:02 - 00:08",
    "context": "A female voice says, \"Slowly, slowly,\" as a self-reminder.",
    "question_type": "Counting",
    "question": "How many times is the word \"slowly\" said?",
    "answer": "Twice.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01409.mp4",
    "question_id": "01409_6",
    "clip_path": "clips/01409/01409__0001500_0008500.mp4"
  },
  {
    "timestamp": "00:10 - 00:12",
    "context": "After reaching the top of the stairs, the user calls out in a clear, downward-directed voice, \"Yuki,\" attempting to get the attention of someone or a pet downstairs.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the user call out \"Yuki\" at 00:10–00:12?",
    "answer": "To get the attention of someone or a pet downstairs.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01409.mp4",
    "question_id": "01409_7",
    "clip_path": "clips/01409/01409__0009500_0012500.mp4"
  },
  {
    "timestamp": "00:13 - 00:18",
    "context": "In response to the call, a rapid pitter-patter of light footsteps is heard approaching from the stairs below, made by a white Samoyed dog named Yuki running up the stairs. The user lets out a soft, amused chuckle upon seeing the dog.",
    "question_type": "Sound Source Identification",
    "question": "What produced the rapid pitter-patter of light footsteps?",
    "answer": "The white Samoyed dog named Yuki running up the stairs.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01409.mp4",
    "question_id": "01409_8",
    "clip_path": "clips/01409/01409__0012500_0018500.mp4"
  },
  {
    "timestamp": "00:13 - 00:18",
    "context": "A rapid pitter-patter of light footsteps is heard approaching from the stairs below.",
    "question_type": "Temporal Information",
    "question": "When did the rapid pitter-patter start and about how long did it last?",
    "answer": "It was heard between 00:13 and 00:18, lasting about five seconds.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01409.mp4",
    "question_id": "01409_9",
    "clip_path": "clips/01409/01409__0012500_0018500.mp4"
  },
  {
    "timestamp": "00:13 - 00:18",
    "context": "The footsteps occur in response to the user's earlier call of \"Yuki.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "What prompted the dog's approach up the stairs?",
    "answer": "It was in response to the user's call of \"Yuki\" moments earlier.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01409.mp4",
    "question_id": "01409_10",
    "clip_path": "clips/01409/01409__0012500_0018500.mp4"
  },
  {
    "timestamp": "00:13 - 00:18",
    "context": "A rapid pitter-patter of light footsteps approaches from below after the user calls \"Yuki.\"",
    "question_type": "Cross-Modal Reasoning",
    "question": "Based on the approaching pitter-patter sound, what was happening visually?",
    "answer": "Yuki was running up the stairs toward the user.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01409.mp4",
    "question_id": "01409_11",
    "clip_path": "clips/01409/01409__0012500_0018500.mp4"
  },
  {
    "timestamp": "00:19 - 00:28",
    "context": "Distinct, light tapping sounds occur as the tip of the pole makes contact with a wooden side table and then the frame of a white lounge chair.",
    "question_type": "Sound Source Identification",
    "question": "Which objects did the pole tip tap to produce the distinct sounds?",
    "answer": "First a wooden side table, then the frame of a white lounge chair.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01409.mp4",
    "question_id": "01409_12",
    "clip_path": "clips/01409/01409__0018500_0028500.mp4"
  },
  {
    "timestamp": "00:19 - 00:28",
    "context": "Distinct, light tapping sounds come from the pole tip contacting furniture; a soft rustling sound occurs when the user touches the footrest with their foot.",
    "question_type": "Sound Characteristics",
    "question": "What were the qualities of the tapping and the footrest contact sounds?",
    "answer": "Distinct, light tapping from the pole contacts, and a soft rustling against fabric when the foot touched the footrest.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01409.mp4",
    "question_id": "01409_13",
    "clip_path": "clips/01409/01409__0018500_0028500.mp4"
  },
  {
    "timestamp": "00:19 - 00:28",
    "context": "The user touches the footrest with their foot, creating a soft rustling against the fabric, and says, \"Oh, here it is,\" confirming its location through touch.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the user say, \"Oh, here it is\"?",
    "answer": "Touching the footrest and hearing the soft rustling confirmed its location.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01409.mp4",
    "question_id": "01409_14",
    "clip_path": "clips/01409/01409__0018500_0028500.mp4"
  },
  {
    "timestamp": "00:34 - 00:39",
    "context": "Realizing they've moved away from their intended path, the user says, \"I went the wrong way. Actually, it's this way. You can see downstairs,\" and moves back to the glass railing.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the user say, \"I went the wrong way. Actually, it's this way\"?",
    "answer": "They realized they had strayed from their intended path and corrected course back to the glass railing to show the view downstairs.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01409.mp4",
    "question_id": "01409_15",
    "clip_path": "clips/01409/01409__0033500_0039500.mp4"
  },
  {
    "timestamp": "00:40 - 00:42",
    "context": "While looking down over the railing, the user calls out again, \"Yuki,\" to get the attention of the dog, which is now in the living room on the lower level.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where was Yuki relative to the camera when the user called out again?",
    "answer": "In the living room on the lower level, below the camera's position.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01409.mp4",
    "question_id": "01409_16",
    "clip_path": "clips/01409/01409__0039500_0042500.mp4"
  },
  {
    "timestamp": "00:43 - 00:44",
    "context": "A soft, light-hearted chuckle is heard from the user, expressing amusement as they watch the dog downstairs.",
    "question_type": "Inferential & Contextual Causality",
    "question": "What prompted the user's soft, light-hearted chuckle here?",
    "answer": "Amusement while watching the dog downstairs.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01409.mp4",
    "question_id": "01409_17",
    "clip_path": "clips/01409/01409__0042500_0044500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "A loud and continuous pre-recorded announcement from a nearby speaker can be heard, advertising the watermelons: \"Genuine watermelon, fifty cents, fifty cents, guaranteed ripe and sweet, taste before you buy.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why was the pre-recorded announcement playing in the background at 00:00-00:04?",
    "answer": "It was advertising the watermelons, promoting their price and quality and encouraging customers to taste before buying.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01415.mp4",
    "question_id": "01415_1",
    "clip_path": "clips/01415/01415__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "A loud and continuous pre-recorded announcement from a nearby speaker can be heard, advertising the watermelons.",
    "question_type": "Sound Source Identification",
    "question": "What produced the loud background speech during 00:00-00:04?",
    "answer": "A nearby speaker playing a pre-recorded sales announcement.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01415.mp4",
    "question_id": "01415_2",
    "clip_path": "clips/01415/01415__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "A loud and continuous pre-recorded announcement from a nearby speaker can be heard.",
    "question_type": "Sound Characteristics",
    "question": "What were the volume and continuity characteristics of the background sales announcement at 00:00-00:04?",
    "answer": "It was loud and continuous.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01415.mp4",
    "question_id": "01415_3",
    "clip_path": "clips/01415/01415__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "A woman places a large watermelon onto a digital scale on the ground, which produces a soft thud.",
    "question_type": "Sound Characteristics",
    "question": "How would you describe the sound made when the watermelon was placed on the scale?",
    "answer": "A soft thud.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01415.mp4",
    "question_id": "01415_4",
    "clip_path": "clips/01415/01415__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:13 - 00:17",
    "context": "The camera holder, a woman, speaks directly to the vendor from a close distance. Her voice is clear above the loud background announcement.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "What was the distance of the woman's speech relative to the camera when she addressed the vendor?",
    "answer": "Close distance.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01415.mp4",
    "question_id": "01415_5",
    "clip_path": "clips/01415/01415__0012500_0017500.mp4"
  },
  {
    "timestamp": "00:13 - 00:17",
    "context": "She says: \"The sweetest one... the sweetest watermelon,\" asking the vendor to select the sweetest watermelon for her purchase.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the woman say, \"The sweetest one... the sweetest watermelon\" at 00:13-00:17?",
    "answer": "She was requesting that the vendor select the sweetest watermelon for her purchase.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01415.mp4",
    "question_id": "01415_6",
    "clip_path": "clips/01415/01415__0012500_0017500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "The announcement says: \"... fifty cents, fifty cents ...\"",
    "question_type": "Counting",
    "question": "How many times is the phrase \"fifty cents\" spoken in the quoted announcement at 00:00-00:04?",
    "answer": "Twice.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01415.mp4",
    "question_id": "01415_7",
    "clip_path": "clips/01415/01415__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:18 - 00:22",
    "context": "The loud, repetitive sales announcement continues to dominate the audio.",
    "question_type": "Temporal Information",
    "question": "What is the temporal behavior of the sales announcement between 00:18 and 00:22?",
    "answer": "It continues throughout this interval and is repetitive, dominating the audio.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01415.mp4",
    "question_id": "01415_8",
    "clip_path": "clips/01415/01415__0017500_0022500.mp4"
  },
  {
    "timestamp": "00:13 - 00:17",
    "context": "The camera holder, a woman, speaks directly to the vendor from a close distance.",
    "question_type": "Counting",
    "question": "How many people are speaking during 00:13-00:17?",
    "answer": "One—the camera-holding woman.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01415.mp4",
    "question_id": "01415_9",
    "clip_path": "clips/01415/01415__0012500_0017500.mp4"
  },
  {
    "timestamp": "00:00 - 00:09",
    "context": "A female voice in Mandarin narrates: \"现在我们已经到达深圳宝安机场... 某人已经把欲哭无泪写到脸上了\" as the camera pans to the man being followed.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why does the camera pan to the man's face right after the playful comment about someone 'wanting to cry but having no tears'?",
    "answer": "To show the person being referenced and highlight the contrast between the comment and his actual expression.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01419.mp4",
    "question_id": "01419_1",
    "clip_path": "clips/01419/01419__0000000_0009500.mp4"
  },
  {
    "timestamp": "00:00 - 00:09",
    "context": "The narrator jokes that someone looks miserable, but when the camera reveals the man in the white t-shirt, he is smiling broadly.",
    "question_type": "Cross-Modal Reasoning",
    "question": "What creates the humorous contrast between the audio narration and the visuals?",
    "answer": "The narration suggests the man looks upset, but the visual shows him smiling broadly.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01419.mp4",
    "question_id": "01419_2",
    "clip_path": "clips/01419/01419__0000000_0009500.mp4"
  },
  {
    "timestamp": "00:00 - 00:09",
    "context": "A female voice, speaking in Mandarin, narrates their arrival in the airport.",
    "question_type": "Sound Source Identification",
    "question": "Who is narrating their arrival and in what language?",
    "answer": "A female voice speaking in Mandarin.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01419.mp4",
    "question_id": "01419_3",
    "clip_path": "clips/01419/01419__0000000_0009500.mp4"
  },
  {
    "timestamp": "00:00 - 00:09",
    "context": "Background pop music and the low hum of the airport are audible during the narration.",
    "question_type": "Temporal Information",
    "question": "Are the background pop music and airport hum brief or continuous during this segment?",
    "answer": "They are continuous in the background throughout the segment.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01419.mp4",
    "question_id": "01419_4",
    "clip_path": "clips/01419/01419__0000000_0009500.mp4"
  },
  {
    "timestamp": "00:00 - 00:09",
    "context": "The sound of rolling luggage wheels can be heard faintly as travelers move through the terminal.",
    "question_type": "Sound Source Identification",
    "question": "What action generates the faint rolling sound heard in the terminal?",
    "answer": "Rolling luggage wheels moving as travelers walk.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01419.mp4",
    "question_id": "01419_5",
    "clip_path": "clips/01419/01419__0000000_0009500.mp4"
  },
  {
    "timestamp": "00:00 - 00:09",
    "context": "The rolling luggage wheels are described as being heard faintly.",
    "question_type": "Sound Characteristics",
    "question": "What is the volume of the rolling luggage wheel sounds?",
    "answer": "Faint.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01419.mp4",
    "question_id": "01419_6",
    "clip_path": "clips/01419/01419__0000000_0009500.mp4"
  },
  {
    "timestamp": "00:00 - 00:09",
    "context": "The airport ambience is described as a low hum accompanying background pop music.",
    "question_type": "Sound Characteristics",
    "question": "How is the airport background noise characterized?",
    "answer": "As a low hum in the background.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01419.mp4",
    "question_id": "01419_7",
    "clip_path": "clips/01419/01419__0000000_0009500.mp4"
  },
  {
    "timestamp": "00:00 - 00:09",
    "context": "During the walk through the terminal, background pop music, a low airport hum, and faint rolling luggage sounds are mentioned.",
    "question_type": "Counting",
    "question": "How many distinct non-speech background sounds are explicitly mentioned?",
    "answer": "Three: background pop music, the low hum of the airport, and rolling luggage wheels.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01419.mp4",
    "question_id": "01419_8",
    "clip_path": "clips/01419/01419__0000000_0009500.mp4"
  },
  {
    "timestamp": "00:00 - 00:09",
    "context": "The rolling luggage wheels are heard faintly as people move through the space.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Relative to the camera, is the rolling luggage sound close and prominent or ambient and distant?",
    "answer": "Ambient and distant (faint), not close to the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01419.mp4",
    "question_id": "01419_9",
    "clip_path": "clips/01419/01419__0000000_0009500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "As the person holding the camera walks along a stone path, a male voice, originating directly from the camera's position, speaks in a clear, conversational tone: \"哎呀, 这个地方有很浓重的桂花香味啊\".",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the first male speaker make the comment at 00:00 - 00:04?",
    "answer": "He was commenting on the surrounding environment after noticing a strong osmanthus fragrance.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01422.mp4",
    "question_id": "01422_1",
    "clip_path": "clips/01422/01422__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "A male voice originates directly from the camera's position as the camera holder walks along the path.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the first male voice originate relative to the camera?",
    "answer": "Directly from the camera's position (center, essentially at the camera).",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01422.mp4",
    "question_id": "01422_2",
    "clip_path": "clips/01422/01422__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "The first speaker's voice is described as clear and conversational.",
    "question_type": "Sound Characteristics",
    "question": "What is the tone/quality of the first male speaker's voice?",
    "answer": "Clear and conversational.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01422.mp4",
    "question_id": "01422_3",
    "clip_path": "clips/01422/01422__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "The initial comment about the osmanthus fragrance occurs within the 00:00 - 00:04 interval.",
    "question_type": "Temporal Information",
    "question": "When does the first spoken comment occur?",
    "answer": "Between 00:00 and 00:04.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01422.mp4",
    "question_id": "01422_4",
    "clip_path": "clips/01422/01422__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:04 - 00:07",
    "context": "In direct response to the first speaker, a second male voice replies: \"对, 现在到处都是桂花... 嗯... 八月桂花香嘛\".",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the second male voice speak at 00:04 - 00:07?",
    "answer": "To agree with the first observation and provide seasonal context for the smell (August osmanthus fragrance).",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01422.mp4",
    "question_id": "01422_5",
    "clip_path": "clips/01422/01422__0003500_0007500.mp4"
  },
  {
    "timestamp": "00:04 - 00:07",
    "context": "The second male voice is located nearby and slightly to the right of the camera.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "What is the location of the second male voice relative to the camera?",
    "answer": "Nearby and slightly to the right.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01422.mp4",
    "question_id": "01422_6",
    "clip_path": "clips/01422/01422__0003500_0007500.mp4"
  },
  {
    "timestamp": "00:04 - 00:07",
    "context": "A second male voice replies in a clear, conversational tone.",
    "question_type": "Sound Characteristics",
    "question": "What is the tone/quality of the second male voice's reply?",
    "answer": "Clear and conversational.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01422.mp4",
    "question_id": "01422_7",
    "clip_path": "clips/01422/01422__0003500_0007500.mp4"
  },
  {
    "timestamp": "00:04 - 00:07",
    "context": "The reply occurs after the first comment, within 00:04 - 00:07.",
    "question_type": "Temporal Information",
    "question": "Does the second speaker's reply overlap with the first comment or follow it?",
    "answer": "It follows the first comment, occurring from 00:04 to 00:07 in direct response.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01422.mp4",
    "question_id": "01422_8",
    "clip_path": "clips/01422/01422__0003500_0007500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "Two distinct male voices are heard: the first from the camera's position (00:00 - 00:04), and the second nearby to the right (00:04 - 00:07).",
    "question_type": "Counting",
    "question": "How many distinct male speakers are present in the audio?",
    "answer": "Two.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01422.mp4",
    "question_id": "01422_9",
    "clip_path": "clips/01422/01422__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:04 - 00:07",
    "context": "While the second male voice replies, the camera continues forward, capturing other visitors, including one person posing for a photo on a large rock.",
    "question_type": "Cross-Modal Reasoning",
    "question": "While the second male voice is replying, what is seen in the video?",
    "answer": "Other visitors in the park, including someone posing for a photo on a large rock.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01422.mp4",
    "question_id": "01422_10",
    "clip_path": "clips/01422/01422__0003500_0007500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "A male voice originates directly from the camera's position as the person holding the camera walks along the path.",
    "question_type": "Cross-Modal Reasoning",
    "question": "Based on the audio origin and the visual context, who is most likely speaking the first line?",
    "answer": "The person holding the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01422.mp4",
    "question_id": "01422_11",
    "clip_path": "clips/01422/01422__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:00 - 00:01",
    "context": "[00:00 - 00:01] The camera operator, while walking briskly on a path covered in dry leaves, shouts in a loud, high-pitched voice at a person in a black jacket walking ahead: \"老王别跑\" (\"Old Wang, don't run!\").",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the camera operator shout \"Old Wang, don't run!\" at the person in the black jacket?",
    "answer": "To tell the person walking ahead not to run.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01424.mp4",
    "question_id": "01424_1",
    "clip_path": "clips/01424/01424__0000000_0001500.mp4"
  },
  {
    "timestamp": "00:01 - 00:03",
    "context": "[00:01 - 00:03] Immediately after shouting, the camera operator trips and falls; the view becomes extremely shaky and blurry as it tumbles toward the ground.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the camera view become extremely shaky and blurry between 00:01 and 00:03?",
    "answer": "Because the operator tripped and fell, causing the camera to tumble toward the ground.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01424.mp4",
    "question_id": "01424_2",
    "clip_path": "clips/01424/01424__0000500_0003500.mp4"
  },
  {
    "timestamp": "00:01 - 00:03",
    "context": "[00:01 - 00:03] The event is marked by a sudden, sharp yelp (\"哎呀\"). The fall is accompanied by a soft thud upon impact.",
    "question_type": "Sound Source Identification",
    "question": "What produced the soft thud upon impact heard during the fall?",
    "answer": "The impact with the ground during the operator’s fall.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01424.mp4",
    "question_id": "01424_3",
    "clip_path": "clips/01424/01424__0000500_0003500.mp4"
  },
  {
    "timestamp": "00:01 - 00:03",
    "context": "[00:01 - 00:03] The event is marked by a sudden, sharp yelp (\"哎呀\") from the operator.",
    "question_type": "Sound Source Identification",
    "question": "Who produced the sudden, sharp yelp (\"哎呀\") during the fall?",
    "answer": "The camera operator.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01424.mp4",
    "question_id": "01424_4",
    "clip_path": "clips/01424/01424__0000500_0003500.mp4"
  },
  {
    "timestamp": "00:00 - 00:01",
    "context": "[00:00 - 00:01] The camera operator shouts in a loud, high-pitched voice.",
    "question_type": "Sound Characteristics",
    "question": "What were the pitch and volume characteristics of the operator’s shout at 00:00-00:01?",
    "answer": "It was loud and high-pitched.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01424.mp4",
    "question_id": "01424_5",
    "clip_path": "clips/01424/01424__0000000_0001500.mp4"
  },
  {
    "timestamp": "00:01 - 00:03",
    "context": "[00:01 - 00:03] The fall is accompanied by the loud, crisp sound of rustling leaves.",
    "question_type": "Sound Characteristics",
    "question": "How is the rustling leaves sound characterized during the fall?",
    "answer": "It is loud and crisp.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01424.mp4",
    "question_id": "01424_6",
    "clip_path": "clips/01424/01424__0000500_0003500.mp4"
  },
  {
    "timestamp": "00:03 - 00:05",
    "context": "[00:03 - 00:05] The sound of leaves rustling faintly persists for a moment as the operator settles after the fall.",
    "question_type": "Sound Characteristics",
    "question": "What is the volume of the rustling leaves sound after the camera comes to rest?",
    "answer": "Faint.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01424.mp4",
    "question_id": "01424_7",
    "clip_path": "clips/01424/01424__0002500_0005500.mp4"
  },
  {
    "timestamp": "00:01 - 00:03",
    "context": "[00:01 - 00:03] A faint male voice in the distance can be heard making a short sound of surprise.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the short sound of surprise originate relative to the camera?",
    "answer": "From a faint male voice in the distance, far from the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01424.mp4",
    "question_id": "01424_8",
    "clip_path": "clips/01424/01424__0000500_0003500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "[00:00 - 00:01] The operator shouts. [00:01 - 00:03] Immediately after shouting, the operator trips and falls.",
    "question_type": "Temporal Information",
    "question": "What is the temporal relationship between the shout and the trip/fall?",
    "answer": "The trip and fall occur immediately after the shout.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01424.mp4",
    "question_id": "01424_9",
    "clip_path": "clips/01424/01424__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "[00:00 - 00:01] The operator shouts \"老王别跑\". [00:01 - 00:03] The operator yelps \"哎呀\" as they fall, and a faint male voice in the distance makes a short sound of surprise.",
    "question_type": "Counting",
    "question": "How many distinct vocalizations are heard from 00:00 to 00:03?",
    "answer": "Three: the operator’s shout, the operator’s yelp, and a faint male voice’s short sound of surprise.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01424.mp4",
    "question_id": "01424_10",
    "clip_path": "clips/01424/01424__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:01 - 00:03",
    "context": "[00:01 - 00:03] The fall is accompanied by the loud, crisp sound of rustling leaves and a soft thud upon impact.",
    "question_type": "Counting",
    "question": "How many distinct non-vocal sound events accompany the fall between 00:01 and 00:03?",
    "answer": "Two: the rustling leaves and the soft thud on impact.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01424.mp4",
    "question_id": "01424_11",
    "clip_path": "clips/01424/01424__0000500_0003500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "[00:00 - 00:04] A staff member arrives to assist. The user says, \"Someone's here... Okay, thank you... Uh-huh, okay, thank you.\" The station's acoustics create a faint echo, and the user's voice is clear and close.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the user say 'thank you' during 00:00 - 00:04?",
    "answer": "Because a staff member had arrived to assist them.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01428.mp4",
    "question_id": "01428_1",
    "clip_path": "clips/01428/01428__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:04 - 00:08",
    "context": "[00:04 - 00:08] The staff member asks, \"To where?\" The user replies, \"To that Yuzui Park.\" The staff member confirms, \"Yuzui Park?\" and the user affirms, \"Yuzui Park.\"",
    "question_type": "Sound Source Identification",
    "question": "Who asked, \"To where?\"",
    "answer": "The female staff member.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01428.mp4",
    "question_id": "01428_2",
    "clip_path": "clips/01428/01428__0003500_0008500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "[00:00 - 00:04] The user's voice is clear and close, while the station's acoustics create a faint echo.",
    "question_type": "Sound Characteristics",
    "question": "What was the character of the environmental acoustics during the user's initial speech?",
    "answer": "The station's acoustics created a faint echo.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01428.mp4",
    "question_id": "01428_3",
    "clip_path": "clips/01428/01428__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "[00:00 - 00:04] The user's voice is described as clear and close.",
    "question_type": "Sound Characteristics",
    "question": "How would you describe the user's voice quality and proximity during the opening lines?",
    "answer": "The user's voice was clear and close.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01428.mp4",
    "question_id": "01428_4",
    "clip_path": "clips/01428/01428__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "[00:00 - 00:04] The user's voice is clear and close.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Did the user's speech originate close to or far from the camera during 00:00 - 00:04?",
    "answer": "Close to the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01428.mp4",
    "question_id": "01428_5",
    "clip_path": "clips/01428/01428__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:08 - 00:13",
    "context": "[00:08 - 00:13] The staff member clarifies the route, stating \"Line 2,\" and asks, \"Take the elevator down?\" The user agrees to be escorted.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the staff member suggest taking the elevator down after mentioning Line 2?",
    "answer": "To determine how the user wanted to proceed along the clarified route.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01428.mp4",
    "question_id": "01428_6",
    "clip_path": "clips/01428/01428__0007500_0013500.mp4"
  },
  {
    "timestamp": "00:13 - 00:16",
    "context": "[00:13 - 00:16] The staff member says, \"Okay. Okay, you can hold on to me.\" The user replies, \"Okay, thank you.\" They begin walking together.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the staff member say, \"You can hold on to me\"?",
    "answer": "To provide physical guidance as they began walking together.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01428.mp4",
    "question_id": "01428_7",
    "clip_path": "clips/01428/01428__0012500_0016500.mp4"
  },
  {
    "timestamp": "00:16 - 00:24",
    "context": "[00:16 - 00:24] As they walk, the staff member says, \"Help me call the elevator... Elevator No. 2... Help me call the front elevator.\" Then, \"I'll take her down.\"",
    "question_type": "Temporal Information",
    "question": "When did the staff member begin preparing to call the elevator?",
    "answer": "Between 00:16 and 00:24 while they were walking.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01428.mp4",
    "question_id": "01428_8",
    "clip_path": "clips/01428/01428__0015500_0024500.mp4"
  },
  {
    "timestamp": "00:00 - 00:04",
    "context": "[00:00 - 00:04] The user says, \"Okay, thank you... Uh-huh, okay, thank you.\"",
    "question_type": "Counting",
    "question": "How many times did the user say \"thank you\" during 00:00 - 00:04?",
    "answer": "Twice.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01428.mp4",
    "question_id": "01428_9",
    "clip_path": "clips/01428/01428__0000000_0004500.mp4"
  },
  {
    "timestamp": "00:04 - 00:08",
    "context": "[00:04 - 00:08] User: \"To that Yuzui Park.\" Staff: \"Yuzui Park?\" User: \"Yuzui Park.\"",
    "question_type": "Counting",
    "question": "How many times was the destination name \"Yuzui Park\" spoken in this segment?",
    "answer": "Three times.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01428.mp4",
    "question_id": "01428_10",
    "clip_path": "clips/01428/01428__0003500_0008500.mp4"
  },
  {
    "timestamp": "00:04 - 00:07",
    "context": "[00:04 - 00:07] Upon exiting, a male staff member on the platform asks loudly and clearly, '到几号线?' to provide directions.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the male staff member on the platform ask '到几号线?'",
    "answer": "He asked to provide directions to the user.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01431.mp4",
    "question_id": "01431_1",
    "clip_path": "clips/01431/01431__0003500_0007500.mp4"
  },
  {
    "timestamp": "00:02 - 00:04",
    "context": "[00:02 - 00:04] The doors open with a hiss and chime as the user prepares to disembark and says, '好, 谢谢你啊' to the helper.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Based on the preceding events, why did the user say '好, 谢谢你啊' at this moment?",
    "answer": "They were expressing gratitude to the person who had offered help as they prepared to disembark.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01431.mp4",
    "question_id": "01431_2",
    "clip_path": "clips/01431/01431__0001500_0004500.mp4"
  },
  {
    "timestamp": "00:00 - 00:02",
    "context": "[00:00 - 00:02] The subway train is in motion, producing a low, continuous rumble.",
    "question_type": "Sound Source Identification",
    "question": "What generated the low, continuous rumble at the start?",
    "answer": "The moving subway train.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01431.mp4",
    "question_id": "01431_3",
    "clip_path": "clips/01431/01431__0000000_0002500.mp4"
  },
  {
    "timestamp": "00:02 - 00:04",
    "context": "[00:02 - 00:04] The subway doors open with a sharp pneumatic hiss and a brief electronic chime.",
    "question_type": "Sound Source Identification",
    "question": "What produced the sharp pneumatic hiss and brief electronic chime?",
    "answer": "The subway doors opening.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01431.mp4",
    "question_id": "01431_4",
    "clip_path": "clips/01431/01431__0001500_0004500.mp4"
  },
  {
    "timestamp": "00:11 - 00:15",
    "context": "[00:11 - 00:15] On the upward-moving escalator, the distinct, continuous mechanical whirring and clanking becomes the dominant sound.",
    "question_type": "Sound Characteristics",
    "question": "How is the escalator's sound described during this segment?",
    "answer": "A distinct, continuous mechanical whirring and clanking that dominates the audio.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01431.mp4",
    "question_id": "01431_5",
    "clip_path": "clips/01431/01431__0010500_0015500.mp4"
  },
  {
    "timestamp": "00:15 - 00:17",
    "context": "[00:15 - 00:17] While on the escalator, the user's cane makes a single, sharp metallic clank as it strikes the metal siding.",
    "question_type": "Sound Characteristics",
    "question": "What was the acoustic quality of the cane's impact sound?",
    "answer": "A single, sharp metallic clank.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01431.mp4",
    "question_id": "01431_6",
    "clip_path": "clips/01431/01431__0014500_0017500.mp4"
  },
  {
    "timestamp": "00:00 - 00:02",
    "context": "[00:00 - 00:02] A female passenger from the front right offers assistance, speaking clearly at a moderate volume.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From what direction relative to the camera did the assisting woman's voice originate?",
    "answer": "From the front right.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01431.mp4",
    "question_id": "01431_7",
    "clip_path": "clips/01431/01431__0000000_0002500.mp4"
  },
  {
    "timestamp": "00:11 - 00:15",
    "context": "[00:11 - 00:15] From the front, a male helper confirms their direction by saying, '2号线'.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where relative to the camera did the male helper who said '2号线' speak from?",
    "answer": "From the front.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01431.mp4",
    "question_id": "01431_8",
    "clip_path": "clips/01431/01431__0010500_0015500.mp4"
  },
  {
    "timestamp": "00:11 - 00:15",
    "context": "[00:11 - 00:15] The user is guided onto an escalator and its mechanical whirring and clanking becomes dominant.",
    "question_type": "Temporal Information",
    "question": "When does the escalator's mechanical sound become dominant, and is it continuous or brief?",
    "answer": "It becomes dominant while on the escalator (00:11–00:15) and is continuous.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01431.mp4",
    "question_id": "01431_9",
    "clip_path": "clips/01431/01431__0010500_0015500.mp4"
  },
  {
    "timestamp": "00:15 - 00:17",
    "context": "[00:15 - 00:17] The user's cane strikes the metal siding once, producing a metallic clank.",
    "question_type": "Counting",
    "question": "How many times did the cane strike the metal siding, producing a clank?",
    "answer": "Once.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01431.mp4",
    "question_id": "01431_10",
    "clip_path": "clips/01431/01431__0014500_0017500.mp4"
  },
  {
    "timestamp": "00:07 - 00:15",
    "context": "[00:07 - 00:09] The initial helper says '2号线'. [00:11 - 00:15] From the front, a male helper again confirms '2号线'.",
    "question_type": "Counting",
    "question": "How many times is '2号线' spoken across these moments?",
    "answer": "Twice.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01431.mp4",
    "question_id": "01431_11",
    "clip_path": "clips/01431/01431__0006500_0015500.mp4"
  },
  {
    "timestamp": "00:00 - 00:02",
    "context": "Inside a crowded subway car, a man in a grey sweatshirt moves his silver suitcase forward, causing a loud thud as it is placed on the floor in front of him.",
    "question_type": "Sound Source Identification",
    "question": "What object generated the loud thud at the start of the video?",
    "answer": "The man's silver suitcase being set down on the floor.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01433.mp4",
    "question_id": "01433_1",
    "clip_path": "clips/01433/01433__0000000_0002500.mp4"
  },
  {
    "timestamp": "00:00 - 00:02",
    "context": "The sound is sharp and originates directly in front of the camera, approximately 1 meter away.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the thud originate relative to the camera?",
    "answer": "Directly in front of the camera, about 1 meter away.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01433.mp4",
    "question_id": "01433_2",
    "clip_path": "clips/01433/01433__0000000_0002500.mp4"
  },
  {
    "timestamp": "00:02 - 00:08",
    "context": "Several passengers offer assistance as the camera holder finds a place to stand. A woman from the front-right says, \"好, 谢谢\"; another woman from the right says, \"好, 你扶在这里吧\"; a man from the left says, \"好好好, 扶往这里\"; the camera holder says, \"谢谢.\"",
    "question_type": "Counting",
    "question": "How many distinct passengers verbally guided the camera holder during this interval?",
    "answer": "Three passengers (two women and one man).",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01433.mp4",
    "question_id": "01433_3",
    "clip_path": "clips/01433/01433__0001500_0008500.mp4"
  },
  {
    "timestamp": "00:02 - 00:08",
    "context": "The camera holder is visually impaired and using a white cane; multiple passengers guide them to a vertical pole to hold.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did several passengers provide verbal guidance to the camera holder?",
    "answer": "Because the camera holder is visually impaired and was finding a place to stand, so they helped them locate a stable handhold.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01433.mp4",
    "question_id": "01433_4",
    "clip_path": "clips/01433/01433__0001500_0008500.mp4"
  },
  {
    "timestamp": "00:08 - 00:10",
    "context": "A distinct, two-tone electronic chime sounds throughout the train car.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the two-tone electronic chime sound at this moment?",
    "answer": "It signaled that the doors were closing and the train was about to depart.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01433.mp4",
    "question_id": "01433_5",
    "clip_path": "clips/01433/01433__0007500_0010500.mp4"
  },
  {
    "timestamp": "00:10 - 00:17",
    "context": "As the train moves, a soft, intermittent jingling sound is audible from a small white crocheted bunny keychain attached to the camera holder's white cane.",
    "question_type": "Sound Source Identification",
    "question": "What produced the soft jingling while the train was moving?",
    "answer": "A small white crocheted bunny keychain attached to the camera holder's white cane rattling with the train's vibrations.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01433.mp4",
    "question_id": "01433_6",
    "clip_path": "clips/01433/01433__0009500_0017500.mp4"
  },
  {
    "timestamp": "00:17 - 00:25",
    "context": "A melodic chime plays, followed by a clear, automated female voice announcing over the train's PA system.",
    "question_type": "Temporal Information",
    "question": "What sound immediately precedes the PA announcement?",
    "answer": "A melodic chime.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01433.mp4",
    "question_id": "01433_7",
    "clip_path": "clips/01433/01433__0016500_0025500.mp4"
  },
  {
    "timestamp": "00:25 - 00:33",
    "context": "Prompted by the announcement and noticing the camera holder's white cane, a woman from the left offers a seat; others make space, and the camera holder is guided to the vacant seat.",
    "question_type": "Inferential & Contextual Causality",
    "question": "What prompted the woman to offer a seat to the camera holder?",
    "answer": "The PA announcement encouraging seating assistance and noticing the camera holder's white cane.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01433.mp4",
    "question_id": "01433_8",
    "clip_path": "clips/01433/01433__0024500_0033500.mp4"
  },
  {
    "timestamp": "00:25 - 00:33",
    "context": "A woman says, \"你到那儿去坐\"; another woman says, \"哎, 我过来一个\"; the man who gave up his seat says, \"谢谢啊.\"",
    "question_type": "Counting",
    "question": "How many different passengers spoke while arranging the seat during this interval?",
    "answer": "Three passengers.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01433.mp4",
    "question_id": "01433_9",
    "clip_path": "clips/01433/01433__0024500_0033500.mp4"
  },
  {
    "timestamp": "00:33 - 00:39",
    "context": "The train doors open and a two-tone electronic chime indicating arrival at a station is heard. Passengers can be seen entering and exiting through the open doors in front.",
    "question_type": "Cross-Modal Reasoning",
    "question": "When the two-tone arrival chime is heard, what visual action is observed?",
    "answer": "Passengers enter and exit through the open doors in front of the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01433.mp4",
    "question_id": "01433_10",
    "clip_path": "clips/01433/01433__0032500_0039500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "[00:00 - 00:06] The user asks, \"Let me ask, is the Lanzhou noodle shop here?\" The woman replies, \"Ahead, you need to go back.\" The user responds, \"Oh, I need to go back,\" and begins to turn around.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the user begin to turn around at the end of this segment?",
    "answer": "Because the woman told them they needed to go back to reach the noodle shop.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01436.mp4",
    "question_id": "01436_1",
    "clip_path": "clips/01436/01436__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:07 - 00:08",
    "context": "[00:07 - 00:08] As the user walks away from the woman who gave directions, they say, \"Okay, thank you,\" in a polite tone.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the user say \"Okay, thank you\" while walking away?",
    "answer": "To express gratitude to the woman for giving directions.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01436.mp4",
    "question_id": "01436_2",
    "clip_path": "clips/01436/01436__0006500_0008500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "[00:00 - 00:06] The user speaks in a clear, conversational voice. The woman replies in a slightly louder voice.",
    "question_type": "Sound Characteristics",
    "question": "How does the volume of the woman's reply compare to the user's question?",
    "answer": "Her reply is slightly louder than the user's clear, conversational question.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01436.mp4",
    "question_id": "01436_3",
    "clip_path": "clips/01436/01436__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:09 - 00:11",
    "context": "[00:09 - 00:11] While walking, the user speaks to themself in a quiet monologue.",
    "question_type": "Sound Characteristics",
    "question": "What is the volume/tone of the user's self-monologue during this interval?",
    "answer": "Quiet and subdued.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01436.mp4",
    "question_id": "01436_4",
    "clip_path": "clips/01436/01436__0008500_0011500.mp4"
  },
  {
    "timestamp": "00:15 - 00:29",
    "context": "[00:15 - 00:29] The user explains: they smelled Chinese medicine from a pharmacy behind them and felt the direction was wrong, realizing they had walked past it.",
    "question_type": "Inferential & Contextual Causality",
    "question": "According to the user's explanation, why did they realize their direction was wrong?",
    "answer": "Because the smell of Chinese medicine from a pharmacy behind them indicated they had already passed the correct direction.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01436.mp4",
    "question_id": "01436_5",
    "clip_path": "clips/01436/01436__0014500_0029500.mp4"
  },
  {
    "timestamp": "00:32 - 00:35",
    "context": "[00:32 - 00:35] The user's white cane falls, making a clattering sound on the tiled floor. The user says, \"Oops, the cane dropped.\"",
    "question_type": "Sound Source Identification",
    "question": "What object generated the clattering sound at this moment?",
    "answer": "The user's white cane hitting the tiled floor.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01436.mp4",
    "question_id": "01436_6",
    "clip_path": "clips/01436/01436__0031500_0035500.mp4"
  },
  {
    "timestamp": "00:32 - 00:35",
    "context": "[00:32 - 00:35] The cane falls and makes a sharp, distinct clattering sound as it hits the tiled floor.",
    "question_type": "Sound Characteristics",
    "question": "How is the clattering sound described when the cane hits the floor?",
    "answer": "Sharp and distinct.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01436.mp4",
    "question_id": "01436_7",
    "clip_path": "clips/01436/01436__0031500_0035500.mp4"
  },
  {
    "timestamp": "00:32 - 00:35",
    "context": "[00:32 - 00:35] The cane hits the tiled floor, producing the clattering sound.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the clattering sound originate relative to the surroundings?",
    "answer": "From the tiled floor where the cane landed.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01436.mp4",
    "question_id": "01436_8",
    "clip_path": "clips/01436/01436__0031500_0035500.mp4"
  },
  {
    "timestamp": "00:37 - 00:38",
    "context": "[00:37 - 00:38] The user picks up their backpack, causing a soft rustling sound from the fabric.",
    "question_type": "Sound Characteristics",
    "question": "How is the sound produced by moving the backpack described?",
    "answer": "A soft rustling from the fabric.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01436.mp4",
    "question_id": "01436_9",
    "clip_path": "clips/01436/01436__0036500_0038500.mp4"
  },
  {
    "timestamp": "00:38 - 00:46",
    "context": "[00:38 - 00:46] The user interacts with their smartphone. A series of rapid, high-pitched electronic chimes and beeps from the phone's screen reader are heard, along with finger taps. Suddenly, an unintended female electronic voice begins playing from the phone.",
    "question_type": "Sound Source Identification",
    "question": "What produced the rapid, high-pitched electronic chimes and beeps during this interval?",
    "answer": "The smartphone's screen reader.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01436.mp4",
    "question_id": "01436_10",
    "clip_path": "clips/01436/01436__0037500_0046500.mp4"
  },
  {
    "timestamp": "00:38 - 00:46",
    "context": "[00:38 - 00:46] The phone emits a series of rapid, high-pitched electronic chimes and beeps while the user taps the screen.",
    "question_type": "Sound Characteristics",
    "question": "How are the screen reader sounds characterized during this interval?",
    "answer": "They are a series of rapid, high-pitched electronic chimes and beeps.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01436.mp4",
    "question_id": "01436_11",
    "clip_path": "clips/01436/01436__0037500_0046500.mp4"
  },
  {
    "timestamp": "00:38 - 00:46",
    "context": "[00:38 - 00:46] While the user is navigating the phone, an unintended female electronic voice suddenly begins playing from the phone.",
    "question_type": "Temporal Information",
    "question": "When did the unintended female electronic voice start playing relative to the phone interaction?",
    "answer": "It started suddenly during 00:38–00:46 while the user was interacting with the smartphone.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01436.mp4",
    "question_id": "01436_12",
    "clip_path": "clips/01436/01436__0037500_0046500.mp4"
  },
  {
    "timestamp": "00:46 - 00:50",
    "context": "[00:46 - 00:50] Reacting to the unexpected audio, the user asks, \"Why did it start playing music?\" They make several tapping sounds, then say, \"Okay, let's start over.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "What prompted the user to ask, \"Why did it start playing music?\"",
    "answer": "The unexpected audio that began playing from the phone.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01436.mp4",
    "question_id": "01436_13",
    "clip_path": "clips/01436/01436__0045500_0050467.mp4"
  },
  {
    "timestamp": "00:46 - 00:50",
    "context": "[00:46 - 00:50] The user taps the screen several times before saying, \"Okay, let's start over.\"",
    "question_type": "Temporal Information",
    "question": "Did the screen-tapping sounds occur before or after the user said, \"Okay, let's start over\"?",
    "answer": "Before.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01436.mp4",
    "question_id": "01436_14",
    "clip_path": "clips/01436/01436__0045500_0050467.mp4"
  },
  {
    "timestamp": "00:46 - 00:50",
    "context": "[00:46 - 00:50] The user says two phrases: \"Why did it start playing music?\" and \"Okay, let's start over.\"",
    "question_type": "Counting",
    "question": "How many distinct spoken phrases did the user say in this interval?",
    "answer": "Two.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01436.mp4",
    "question_id": "01436_15",
    "clip_path": "clips/01436/01436__0045500_0050467.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "[00:00 - 00:07] Audible traffic noise from passing cars on the left. The user speaks: “因为这边在修地铁，所以有一些工程车什么的停在这，所以每次走这一段路的时候都感觉慌慌的.”",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why does the user say they feel anxious when walking this section?",
    "answer": "Because subway construction leads to construction vehicles parking along this stretch, making them uneasy.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01437.mp4",
    "question_id": "01437_1",
    "clip_path": "clips/01437/01437__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:07 - 00:13",
    "context": "[00:07 - 00:13] A clear, synthesized female voice from a navigation application provides guidance, originating from the phone the user holds in front.",
    "question_type": "Sound Source Identification",
    "question": "What generated the synthesized female guidance voice?",
    "answer": "The navigation application on the smartphone the user is holding.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01437.mp4",
    "question_id": "01437_2",
    "clip_path": "clips/01437/01437__0006500_0013500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "[00:00 - 00:07] The user walks next to a road with audible traffic noise from passing cars on the left.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From which direction relative to the camera did the passing traffic noise originate?",
    "answer": "From the left side.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01437.mp4",
    "question_id": "01437_3",
    "clip_path": "clips/01437/01437__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:07 - 00:13",
    "context": "[00:07 - 00:13] The app’s guidance voice is described as originating from the phone held in front of the user.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where relative to the camera did the navigation voice originate?",
    "answer": "Directly in front, from the smartphone the user is holding.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01437.mp4",
    "question_id": "01437_4",
    "clip_path": "clips/01437/01437__0006500_0013500.mp4"
  },
  {
    "timestamp": "00:13 - 00:18",
    "context": "[00:13 - 00:18] Cars continue to pass on the left, creating a continuous, low-level traffic sound.",
    "question_type": "Sound Characteristics",
    "question": "How is the traffic sound characterized during 00:13–00:18?",
    "answer": "It is continuous and low-level.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01437.mp4",
    "question_id": "01437_5",
    "clip_path": "clips/01437/01437__0012500_0018500.mp4"
  },
  {
    "timestamp": "00:07 - 00:13; 00:13 - 00:18",
    "context": "[00:07 - 00:13] The app says: “方向偏离道路, 偏右75度, 手机朝向西北.” [00:13 - 00:18] The app says: “方向偏离道路, 偏左86度, 手机朝向东南.”",
    "question_type": "Temporal Information",
    "question": "When did the two navigation announcements occur?",
    "answer": "The first occurred between 00:07 and 00:13; the second occurred between 00:13 and 00:18.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01437.mp4",
    "question_id": "01437_6",
    "clip_path": "clips/01437/01437__0006500_0018500.mp4"
  },
  {
    "timestamp": "00:07 - 00:18",
    "context": "[00:07 - 00:13] and [00:13 - 00:18] The navigation app issues two separate corrective announcements.",
    "question_type": "Counting",
    "question": "How many separate corrective voice announcements from the navigation app are heard?",
    "answer": "Two.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01437.mp4",
    "question_id": "01437_7",
    "clip_path": "clips/01437/01437__0006500_0018500.mp4"
  },
  {
    "timestamp": "00:07 - 00:18",
    "context": "[00:07 - 00:13] The app reports deviation to the right by 75°. [00:13 - 00:18] It reports deviation to the left by 86°.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the navigation app provide corrective directions in these segments?",
    "answer": "Because the user’s direction had deviated from the road.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01437.mp4",
    "question_id": "01437_8",
    "clip_path": "clips/01437/01437__0006500_0018500.mp4"
  },
  {
    "timestamp": "00:00 - 00:22",
    "context": "[00:00 - 00:22] The user says: \"I just missed this bus...\" He lifts his phone, looks at the screen, and continues, \"I might have to wait for about half an hour.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the user say he might have to wait about half an hour?",
    "answer": "Because he had just missed the bus and, after checking his phone, saw the wait for the next one would be around half an hour.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01438.mp4",
    "question_id": "01438_1",
    "clip_path": "clips/01438/01438__0000000_0022500.mp4"
  },
  {
    "timestamp": "00:00 - 00:22",
    "context": "The sounds of distant city traffic provide a continuous, low-volume background ambiance.",
    "question_type": "Sound Characteristics",
    "question": "What is the volume and continuity of the city traffic noise during the opening monologue?",
    "answer": "It is low-volume and continuous.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01438.mp4",
    "question_id": "01438_2",
    "clip_path": "clips/01438/01438__0000000_0022500.mp4"
  },
  {
    "timestamp": "00:22 - 00:33",
    "context": "While waiting, the user continues: \"While waiting for the bus, I'll take the opportunity to tell you all that... riding the bus is free.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the user start explaining bus accessibility for the visually impaired at this time?",
    "answer": "Because he was waiting for the bus and chose to use the waiting time to share that information.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01438.mp4",
    "question_id": "01438_3",
    "clip_path": "clips/01438/01438__0021500_0033500.mp4"
  },
  {
    "timestamp": "00:33 - 00:45",
    "context": "A loud, sharp pneumatic hiss is heard from the front as a red bus arrives and its doors open... The bus doors close with another pneumatic hiss.",
    "question_type": "Counting",
    "question": "How many pneumatic hisses are heard during the red bus's arrival and departure in this segment?",
    "answer": "Two—one when the doors open and another when they close.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01438.mp4",
    "question_id": "01438_4",
    "clip_path": "clips/01438/01438__0032500_0045500.mp4"
  },
  {
    "timestamp": "00:33 - 00:45",
    "context": "A loud, sharp pneumatic hiss is heard from the front as a red bus arrives and its doors open.",
    "question_type": "Sound Source Identification",
    "question": "What produced the loud, sharp pneumatic hiss when the red bus arrived?",
    "answer": "The red bus’s doors as they opened.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01438.mp4",
    "question_id": "01438_5",
    "clip_path": "clips/01438/01438__0032500_0045500.mp4"
  },
  {
    "timestamp": "00:33 - 00:45",
    "context": "A loud, sharp pneumatic hiss is heard from the front as a red bus arrives...",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From what direction relative to the camera did the pneumatic hiss originate when the red bus arrived?",
    "answer": "From the front.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01438.mp4",
    "question_id": "01438_6",
    "clip_path": "clips/01438/01438__0032500_0045500.mp4"
  },
  {
    "timestamp": "00:33 - 00:45",
    "context": "The user asks, \"What number bus is this?\" The driver replies from inside the bus, \"60.\" The user says, \"60, ah. I'm sorry.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the user say \"I'm sorry\" after the driver replied \"60\"?",
    "answer": "Because he realized it was the wrong bus route for him.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01438.mp4",
    "question_id": "01438_7",
    "clip_path": "clips/01438/01438__0032500_0045500.mp4"
  },
  {
    "timestamp": "00:46 - 01:06",
    "context": "The user asks the driver, \"Where do I scan the card?\" The driver scans the disability card, triggering a distinct electronic beep.",
    "question_type": "Sound Source Identification",
    "question": "What action triggered the distinct electronic beep on the bus?",
    "answer": "Scanning the user's disability card on the bus’s card reader.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01438.mp4",
    "question_id": "01438_8",
    "clip_path": "clips/01438/01438__0045500_0066500.mp4"
  },
  {
    "timestamp": "01:06 - 01:18",
    "context": "The driver asks, \"Do you need help?\" Then announces, \"Please give up a seat for someone in need.\" A man and a woman offer their seats.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the driver ask passengers to give up a seat?",
    "answer": "To help the user, who needed assistance finding a seat.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01438.mp4",
    "question_id": "01438_9",
    "clip_path": "clips/01438/01438__0065500_0078500.mp4"
  },
  {
    "timestamp": "01:24 - 01:34",
    "context": "While the bus is moving, accompanied by a low engine rumble. An automated voice from the bus's PA system announces the next stop.",
    "question_type": "Temporal Information",
    "question": "Is the engine sound brief or continuous while the bus is moving in this segment?",
    "answer": "Continuous—a low engine rumble throughout the movement.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01438.mp4",
    "question_id": "01438_10",
    "clip_path": "clips/01438/01438__0083500_0094500.mp4"
  },
  {
    "timestamp": "01:24 - 01:34",
    "context": "An automated voice from the bus's PA system announces the next stop in the background.",
    "question_type": "Sound Source Identification",
    "question": "What system delivered the next-stop announcement heard in the background?",
    "answer": "An automated voice from the bus’s PA system.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01438.mp4",
    "question_id": "01438_11",
    "clip_path": "clips/01438/01438__0083500_0094500.mp4"
  },
  {
    "timestamp": "01:34 - 01:39",
    "context": "The user gets ready to disembark, saying to the helpful passengers and driver, \"Okay, thank you.\" A voice replies, \"You're welcome.\" The user adds, \"Sorry to bother you,\" as he steps off.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the user thank the passengers and driver as he got off the bus?",
    "answer": "Because they had helped him during the ride.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01438.mp4",
    "question_id": "01438_12",
    "clip_path": "clips/01438/01438__0093500_0099200.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "[00:00 - 00:06] Sharp, rhythmic tapping sounds are generated by a white cane striking the tactile paving. Faint ambient sounds of passing city traffic are in the background. An electric scooter produces a low hum as it passes by on the left.",
    "question_type": "Sound Characteristics",
    "question": "How are the cane taps described acoustically at the start of the clip?",
    "answer": "They are sharp and rhythmic.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01440.mp4",
    "question_id": "01440_1",
    "clip_path": "clips/01440/01440__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "[00:00 - 00:06] Sharp, rhythmic tapping sounds are generated by a white cane striking the tactile paving of a sidewalk.",
    "question_type": "Sound Source Identification",
    "question": "What generated the tapping sounds at the beginning?",
    "answer": "A white cane striking the tactile paving.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01440.mp4",
    "question_id": "01440_2",
    "clip_path": "clips/01440/01440__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "[00:00 - 00:06] An electric scooter produces a low hum as it passes by on the left.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From which direction relative to the camera did the scooter's hum pass?",
    "answer": "From the left side.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01440.mp4",
    "question_id": "01440_3",
    "clip_path": "clips/01440/01440__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "[00:00 - 00:06] Faint ambient sounds of passing city traffic are in the background.",
    "question_type": "Sound Characteristics",
    "question": "What is the volume level of the background city traffic during this segment?",
    "answer": "Faint.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01440.mp4",
    "question_id": "01440_4",
    "clip_path": "clips/01440/01440__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "[00:00 - 00:06] Cane taps, faint ambient city traffic, and a scooter's low hum are all audible.",
    "question_type": "Counting",
    "question": "How many distinct environmental sound sources are described in this interval?",
    "answer": "Three: cane taps, background traffic, and an electric scooter's hum.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01440.mp4",
    "question_id": "01440_5",
    "clip_path": "clips/01440/01440__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:06 - 00:09",
    "context": "[00:06 - 00:09] A sudden, loud metallic crash occurs as the user's cane hits a shared bicycle, making it fall. The user exclaims, '哎哟喂... 我的妈呀!' expressing shock.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the user cry out 'Oh my gosh... Oh my god!' at this moment?",
    "answer": "Because their cane struck a shared bicycle, causing it to fall with a sudden, loud crash.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01440.mp4",
    "question_id": "01440_6",
    "clip_path": "clips/01440/01440__0005500_0009500.mp4"
  },
  {
    "timestamp": "00:09 - 00:13",
    "context": "[00:09 - 00:13] Standing over the fallen bicycle, the user says, '我把自行车扶起来' (I'll pick the bicycle up).",
    "question_type": "Inferential & Contextual Causality",
    "question": "What prompted the user to decide to pick the bicycle up?",
    "answer": "The bicycle had just fallen over after being struck by the cane.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01440.mp4",
    "question_id": "01440_7",
    "clip_path": "clips/01440/01440__0008500_0013500.mp4"
  },
  {
    "timestamp": "00:15 - 00:20",
    "context": "[00:15 - 00:20] A robotic female voice from a phone's navigation app is heard giving directions in Chinese.",
    "question_type": "Sound Source Identification",
    "question": "What is the source of the spoken directions heard here?",
    "answer": "A robotic female voice from a phone's navigation app.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01440.mp4",
    "question_id": "01440_8",
    "clip_path": "clips/01440/01440__0014500_0020500.mp4"
  },
  {
    "timestamp": "00:21 - 00:26",
    "context": "[00:21 - 00:26] As the user tries to set the bicycle upright, it produces soft metallic shifting and scraping sounds against the pavement.",
    "question_type": "Sound Characteristics",
    "question": "How are the sounds produced while the user struggles to set the bicycle upright described?",
    "answer": "Soft metallic shifting and scraping against the pavement.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01440.mp4",
    "question_id": "01440_9",
    "clip_path": "clips/01440/01440__0020500_0026500.mp4"
  },
  {
    "timestamp": "00:32 - 00:38",
    "context": "[00:32 - 00:38] The user's white cane, propped against the bicycle, clatters onto the ground. A loud, fast-paced public announcement is heard from a distant source. The bicycle falls again with a loud crash.",
    "question_type": "Sound Source Identification",
    "question": "Which object produced the clattering sound during this interval?",
    "answer": "The user's white cane clattering onto the ground.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01440.mp4",
    "question_id": "01440_10",
    "clip_path": "clips/01440/01440__0031500_0038500.mp4"
  },
  {
    "timestamp": "00:32 - 00:38",
    "context": "[00:32 - 00:38] A loud, fast-paced public announcement is heard from a distant source.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the public announcement originate relative to the camera?",
    "answer": "From a distant source.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01440.mp4",
    "question_id": "01440_11",
    "clip_path": "clips/01440/01440__0031500_0038500.mp4"
  },
  {
    "timestamp": "00:06 - 00:09 and 00:32 - 00:38",
    "context": "[00:06 - 00:09] A bicycle falls with a loud metallic crash after the cane hits it. [00:32 - 00:38] The unstable bicycle falls again with a loud crash.",
    "question_type": "Counting",
    "question": "Across these moments, how many times did the bicycle fall with a loud crash?",
    "answer": "Twice.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01440.mp4",
    "question_id": "01440_12",
    "clip_path": "clips/01440/01440__0005500_0038500.mp4"
  },
  {
    "timestamp": "00:40 - 00:51",
    "context": "[00:40 - 00:51] The user says, '都停在盲道上... 你说是我的错吗？也不能吧，他停在盲道上呀' (They are all parked on the tactile paving... Is it my fault? It can't be; it's parked on the tactile paving).",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why does the user argue that it isn't their fault the bicycle fell?",
    "answer": "Because the bicycles were parked on the tactile paving (blind path), obstructing the way.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01440.mp4",
    "question_id": "01440_13",
    "clip_path": "clips/01440/01440__0039500_0051500.mp4"
  },
  {
    "timestamp": "01:01 - 01:06",
    "context": "[01:01 - 01:06] A woman approaches from the front. The user sighs, '好烦呀' (So annoying).",
    "question_type": "Inferential & Contextual Causality",
    "question": "What prompted the user's annoyed sigh when the woman approached?",
    "answer": "Frustration with the ongoing problem of the unstable bicycle.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01440.mp4",
    "question_id": "01440_14",
    "clip_path": "clips/01440/01440__0060500_0066500.mp4"
  },
  {
    "timestamp": "01:06 - 01:23",
    "context": "[01:06 - 01:23] The user asks for help; the woman agrees and sets the bike upright. The user says, '好的，谢谢啊' and '不好意思啊.'",
    "question_type": "Counting",
    "question": "How many polite expressions (thanks/apology) did the user utter after receiving help?",
    "answer": "Two: '谢谢' (thank you) and '不好意思' (sorry).",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01440.mp4",
    "question_id": "01440_15",
    "clip_path": "clips/01440/01440__0065500_0082933.mp4"
  },
  {
    "timestamp": "01:06 - 01:23",
    "context": "[01:06 - 01:23] The woman agrees to help and takes the handlebars, successfully setting the bike upright.",
    "question_type": "Sound Source Identification",
    "question": "Who set the bicycle upright during this interaction?",
    "answer": "The woman passerby holding a green umbrella.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01440.mp4",
    "question_id": "01440_16",
    "clip_path": "clips/01440/01440__0065500_0082933.mp4"
  },
  {
    "timestamp": "00:00 - 00:02",
    "context": "[00:00 - 00:02] A person's hands hold a white uniform, creating a soft, continuous rustling sound as they handle the fabric. This suggests they are getting ready to leave after changing out of work attire.",
    "question_type": "Sound Characteristics",
    "question": "What were the quality and continuity of the rustling sound made by handling the white uniform?",
    "answer": "It was a soft, continuous rustling sound.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01444.mp4",
    "question_id": "01444_1",
    "clip_path": "clips/01444/01444__0000000_0002500.mp4"
  },
  {
    "timestamp": "00:00 - 00:02",
    "context": "[00:00 - 00:02] Hands handle a white uniform, producing soft, continuous rustling; the action suggests preparation to leave after changing out of work attire.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the rustling sound occur at the start of the video?",
    "answer": "Because the person was handling their uniform while preparing to leave after changing out of work clothes.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01444.mp4",
    "question_id": "01444_2",
    "clip_path": "clips/01444/01444__0000000_0002500.mp4"
  },
  {
    "timestamp": "00:02 - 00:05",
    "context": "[00:02 - 00:05] The person unfolds a collapsible white cane; a monologue says, “我的盲杖打开” (My blind cane opens). Three loud, sharp, metallic clicks occur as the cane’s segments snap and lock.",
    "question_type": "Sound Source Identification",
    "question": "What produced the metallic clicking sounds while the cane was being unfolded?",
    "answer": "The cane’s segments snapping and locking into place.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01444.mp4",
    "question_id": "01444_3",
    "clip_path": "clips/01444/01444__0001500_0005500.mp4"
  },
  {
    "timestamp": "00:02 - 00:05",
    "context": "[00:02 - 00:05] As the white cane opens, a series of metallic clicks is heard.",
    "question_type": "Counting",
    "question": "How many metallic clicking sounds occurred as the cane opened?",
    "answer": "Three.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01444.mp4",
    "question_id": "01444_4",
    "clip_path": "clips/01444/01444__0001500_0005500.mp4"
  },
  {
    "timestamp": "00:02 - 00:05",
    "context": "[00:02 - 00:05] The cane’s segments snap and lock, producing distinct clicks.",
    "question_type": "Sound Characteristics",
    "question": "How are the clicking sounds described acoustically?",
    "answer": "They are loud, sharp, and metallic.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01444.mp4",
    "question_id": "01444_5",
    "clip_path": "clips/01444/01444__0001500_0005500.mp4"
  },
  {
    "timestamp": "00:02 - 00:05",
    "context": "[00:02 - 00:05] The person says, “我的盲杖打开” (My blind cane opens), followed by three loud, sharp, metallic clicks as the segments lock.",
    "question_type": "Cross-Modal Reasoning",
    "question": "After the monologue stating the cane is opening, what sound follows and what does it indicate about the cane?",
    "answer": "A series of three loud, sharp, metallic clicks, indicating the segments snapped into place and the cane was ready for use.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01444.mp4",
    "question_id": "01444_6",
    "clip_path": "clips/01444/01444__0001500_0005500.mp4"
  },
  {
    "timestamp": "00:05 - 00:09",
    "context": "[00:05 - 00:09] The camera pans to a woman in purple scrubs mopping a tiled floor, creating a soft, rhythmic swishing sound.",
    "question_type": "Sound Source Identification",
    "question": "What action generated the soft, rhythmic swishing sound?",
    "answer": "The woman mopping the tiled floor.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01444.mp4",
    "question_id": "01444_7",
    "clip_path": "clips/01444/01444__0004500_0009500.mp4"
  },
  {
    "timestamp": "00:05 - 00:09",
    "context": "[00:05 - 00:09] The camera holder addresses the woman from a few meters away: “那我先走了哈，拜拜” (Then I'll leave now, bye-bye).",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Relative to the camera, where did the speech “那我先走了哈，拜拜” originate?",
    "answer": "From the camera holder’s position, i.e., directly at the camera (0 meters away).",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01444.mp4",
    "question_id": "01444_8",
    "clip_path": "clips/01444/01444__0004500_0009500.mp4"
  },
  {
    "timestamp": "00:05 - 00:09",
    "context": "[00:05 - 00:09] The camera holder tells the woman, “那我先走了哈，拜拜” (Then I'll leave now, bye-bye).",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the camera holder say, “Then I'll leave now, bye-bye” to the woman?",
    "answer": "To inform her that they were about to leave.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01444.mp4",
    "question_id": "01444_9",
    "clip_path": "clips/01444/01444__0004500_0009500.mp4"
  },
  {
    "timestamp": "00:09 - 00:16",
    "context": "[00:09 - 00:16] As the person walks through a hospital lobby, the tip of the white cane produces a distinct, rhythmic tapping on the polished tile, continuous and synchronized with steps.",
    "question_type": "Sound Characteristics",
    "question": "How is the cane tip’s tapping described in terms of rhythm and continuity?",
    "answer": "It is distinct and rhythmic, continuous, and synchronized with the person’s steps.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01444.mp4",
    "question_id": "01444_10",
    "clip_path": "clips/01444/01444__0008500_0016267.mp4"
  },
  {
    "timestamp": "00:09 - 00:16",
    "context": "[00:09 - 00:16] The cane’s tapping accompanies the walk out of the building and is noted as serving a navigation function.",
    "question_type": "Inferential & Contextual Causality",
    "question": "What is the purpose of the cane’s tapping sound during this segment?",
    "answer": "It serves as a navigation tool and a consistent auditory cue signifying the journey out of the building.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01444.mp4",
    "question_id": "01444_11",
    "clip_path": "clips/01444/01444__0008500_0016267.mp4"
  },
  {
    "timestamp": "00:09 - 00:16",
    "context": "[00:09 - 00:16] Distinct cane tapping on polished tile is heard while the person moves through a hospital lobby.",
    "question_type": "Cross-Modal Reasoning",
    "question": "Using the tapping on the polished tile as an audio cue, what is the person doing in the scene?",
    "answer": "Walking through a hospital lobby.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01444.mp4",
    "question_id": "01444_12",
    "clip_path": "clips/01444/01444__0008500_0016267.mp4"
  },
  {
    "timestamp": "00:09 - 00:16",
    "context": "[00:09 - 00:16] A monologue begins about leaving work while the cane tapping continues.",
    "question_type": "Temporal Information",
    "question": "During 00:09–00:16, does the monologue overlap with the cane tapping or occur separately?",
    "answer": "It begins simultaneously and overlaps with the continuous tapping.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01444.mp4",
    "question_id": "01444_13",
    "clip_path": "clips/01444/01444__0008500_0016267.mp4"
  },
  {
    "timestamp": "00:02 - 00:05",
    "context": "[00:02 - 00:05] A series of three metallic clicks is heard as the cane opens.",
    "question_type": "Temporal Information",
    "question": "When did the series of clicking sounds occur, and was it brief or extended?",
    "answer": "Between 00:02 and 00:05, as a brief series of three clicks.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01444.mp4",
    "question_id": "01444_14",
    "clip_path": "clips/01444/01444__0001500_0005500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "[00:00 - 00:03] The cane produces sharp, rhythmic tapping sounds on the ground directly in front while walking on asphalt.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Relative to the camera, where are the cane's tapping sounds originating?",
    "answer": "From the ground directly in front of the user.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01448.mp4",
    "question_id": "01448_1",
    "clip_path": "clips/01448/01448__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "[00:00 - 00:03] The user says, \"I also met two kind people who helped me.\"",
    "question_type": "Counting",
    "question": "How many kind people did the user say they met?",
    "answer": "Two.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01448.mp4",
    "question_id": "01448_2",
    "clip_path": "clips/01448/01448__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:07 - 00:10",
    "context": "[00:07 - 00:10] While walking, the user has a close encounter with a parked grey Tesla and exclaims, \"Almost that car.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the user exclaim \"Almost that car\"?",
    "answer": "Because they nearly bumped into a parked grey Tesla that their cane likely detected.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01448.mp4",
    "question_id": "01448_3",
    "clip_path": "clips/01448/01448__0006500_0010500.mp4"
  },
  {
    "timestamp": "00:10 - 00:18",
    "context": "[00:10 - 00:18] The user moves from asphalt to a dirt median; the cane sound changes from sharp taps to softer, duller thuds.",
    "question_type": "Cross-Modal Reasoning",
    "question": "What does the change from sharp taps to softer, duller thuds indicate about the surface the user is walking on?",
    "answer": "It indicates they left the asphalt road and are walking on a dirt median strip.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01448.mp4",
    "question_id": "01448_4",
    "clip_path": "clips/01448/01448__0009500_0018500.mp4"
  },
  {
    "timestamp": "00:18 - 00:30",
    "context": "[00:18 - 00:30] The user reflects: \"The sound of this kind of electric car is really quiet... when that car came over, I couldn't hear any sound. Luckily, the driver braked and let me pass.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "According to the user's reflection, why was the close call with the car dangerous?",
    "answer": "Because the electric car was so quiet that they couldn't hear it approaching.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01448.mp4",
    "question_id": "01448_5",
    "clip_path": "clips/01448/01448__0017500_0030500.mp4"
  },
  {
    "timestamp": "00:32 - 00:41",
    "context": "[00:32 - 00:41] The user transitions from the dirt median onto a brick sidewalk; the cane tapping becomes loud, distinct clicks.",
    "question_type": "Sound Characteristics",
    "question": "How does the cane sound on the brick sidewalk?",
    "answer": "It produces loud, distinct clicks.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01448.mp4",
    "question_id": "01448_6",
    "clip_path": "clips/01448/01448__0031500_0041500.mp4"
  },
  {
    "timestamp": "00:32 - 00:41",
    "context": "[00:32 - 00:41] As the user steps onto the brick sidewalk, the cane's sound changes.",
    "question_type": "Temporal Information",
    "question": "When does the cane's tapping sound change to loud, distinct clicks?",
    "answer": "When they transition onto the brick sidewalk between 00:32 and 00:41.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01448.mp4",
    "question_id": "01448_7",
    "clip_path": "clips/01448/01448__0031500_0041500.mp4"
  },
  {
    "timestamp": "00:45 - 00:50",
    "context": "[00:45 - 00:50] While the cane clicks on brick, the user asks about tactile paving and says, \"Oh, the soles of my shoes are a bit thick, I can't feel it clearly.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why can't the user clearly feel whether they're on tactile paving?",
    "answer": "Because their shoe soles are thick.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01448.mp4",
    "question_id": "01448_8",
    "clip_path": "clips/01448/01448__0044500_0050500.mp4"
  },
  {
    "timestamp": "01:16 - 01:23",
    "context": "[01:16 - 01:23] The cane rustles through bushes, then makes a distinct metallic tap on a low fence; the user says, \"Ah, it seems I can't walk this way.\"",
    "question_type": "Sound Source Identification",
    "question": "What generated the rustling and the metallic tap when the user reached the dead end?",
    "answer": "The cane sweeping through the bushes made the rustling, and it hitting the low fence made the metallic tap.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01448.mp4",
    "question_id": "01448_9",
    "clip_path": "clips/01448/01448__0075500_0083500.mp4"
  },
  {
    "timestamp": "02:15 - 02:24",
    "context": "[02:15 - 02:24] After going around a metro map pillar, the user detects a low, continuous mechanical hum of an escalator ahead and says, \"Ah, I heard the sound of the escalator, that's great!\"",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From which direction relative to the user does the escalator's hum come?",
    "answer": "From ahead of them.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01448.mp4",
    "question_id": "01448_10",
    "clip_path": "clips/01448/01448__0134500_0144500.mp4"
  },
  {
    "timestamp": "02:15 - 02:24",
    "context": "[02:15 - 02:24] The escalator is described as producing a low, continuous mechanical hum.",
    "question_type": "Temporal Information",
    "question": "Is the escalator's mechanical hum intermittent or continuous?",
    "answer": "It is continuous.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01448.mp4",
    "question_id": "01448_11",
    "clip_path": "clips/01448/01448__0134500_0144500.mp4"
  },
  {
    "timestamp": "02:29 - 02:34",
    "context": "[02:29 - 02:34] Standing at the top of the escalator with its hum audible, the user says, \"I finally heard the sound of the escalator. Not easy, not easy,\" expressing relief.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why does the user express relief upon hearing the escalator at 02:29–02:34?",
    "answer": "Because the escalator sound provided a crucial navigational cue after a difficult approach.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01448.mp4",
    "question_id": "01448_12",
    "clip_path": "clips/01448/01448__0148500_0154500.mp4"
  },
  {
    "timestamp": "02:44 - 02:47",
    "context": "[02:44 - 02:47] To board safely, the user uses the cane to locate the edge; the cane taps against the metal step.",
    "question_type": "Sound Source Identification",
    "question": "What object did the cane tap against before the user boarded the escalator?",
    "answer": "The metal step of the escalator.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01448.mp4",
    "question_id": "01448_13",
    "clip_path": "clips/01448/01448__0163500_0167500.mp4"
  },
  {
    "timestamp": "02:47 - 02:52",
    "context": "[02:47 - 02:52] After getting off the escalator, in a large, open station hall, the cane taps produce a strong echo on the polished floor.",
    "question_type": "Cross-Modal Reasoning",
    "question": "What does the strong echo of the cane taps reveal about the environment?",
    "answer": "That the user is in a large, open metro station hall with a polished floor.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01448.mp4",
    "question_id": "01448_14",
    "clip_path": "clips/01448/01448__0166500_0172500.mp4"
  },
  {
    "timestamp": "03:34 - 03:41",
    "context": "[03:34 - 03:41] Two cleaning staff approach and offer help. After a brief exchange, the user follows their guidance and continues in the correct direction.",
    "question_type": "Counting",
    "question": "How many cleaning staff members approached to offer help?",
    "answer": "Two.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01448.mp4",
    "question_id": "01448_15",
    "clip_path": "clips/01448/01448__0213500_0220500.mp4"
  },
  {
    "timestamp": "00:00 - 00:02",
    "context": "[00:00 - 00:02] The right hand presses the up-call button, producing a single, sharp, moderate-volume click from directly in front of the camera, immediately followed by a brief, high-pitched electronic chime indicating the call has been registered.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the brief, high-pitched electronic chime occur right after the button press?",
    "answer": "It indicated that the elevator call had been registered.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01451.mp4",
    "question_id": "01451_1",
    "clip_path": "clips/01451/01451__0000000_0002500.mp4"
  },
  {
    "timestamp": "00:00 - 00:02",
    "context": "[00:00 - 00:02] A single, sharp click is heard, immediately followed by a brief, high-pitched chime after pressing the up-call button.",
    "question_type": "Counting",
    "question": "How many discrete sound events were produced immediately by pressing the up-call button?",
    "answer": "Two: the single sharp click and the brief electronic chime.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01451.mp4",
    "question_id": "01451_2",
    "clip_path": "clips/01451/01451__0000000_0002500.mp4"
  },
  {
    "timestamp": "00:02 - 00:05",
    "context": "[00:02 - 00:05] Elevator music begins at low volume. The heavy elevator doors start to open, generating a sustained, loud mechanical whirring and scraping sound that lasts for approximately two seconds.",
    "question_type": "Temporal Information",
    "question": "How long did the loud mechanical whirring and scraping from the opening doors last?",
    "answer": "Approximately two seconds.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01451.mp4",
    "question_id": "01451_3",
    "clip_path": "clips/01451/01451__0001500_0005500.mp4"
  },
  {
    "timestamp": "00:02 - 00:05",
    "context": "[00:02 - 00:05] The elevator doors open with a sustained, loud mechanical whirring and scraping sound.",
    "question_type": "Sound Characteristics",
    "question": "What are the acoustic qualities and volume of the door-opening sound?",
    "answer": "It is a sustained, loud mechanical whirring and scraping.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01451.mp4",
    "question_id": "01451_4",
    "clip_path": "clips/01451/01451__0001500_0005500.mp4"
  },
  {
    "timestamp": "00:05 - 00:11",
    "context": "[00:05 - 00:11] Inside the elevator, a soft crinkling sound is produced by a plastic bag held in the person's left hand.",
    "question_type": "Sound Source Identification",
    "question": "What produced the soft crinkling sound inside the elevator?",
    "answer": "A plastic bag held in the person's left hand.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01451.mp4",
    "question_id": "01451_5",
    "clip_path": "clips/01451/01451__0004500_0011500.mp4"
  },
  {
    "timestamp": "00:05 - 00:11",
    "context": "[00:05 - 00:11] The right hand presses the button for the 3rd floor, which emits a soft, tactile click upon contact. The action's purpose is to select the destination floor.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the person press the 3rd floor button?",
    "answer": "To select the destination floor.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01451.mp4",
    "question_id": "01451_6",
    "clip_path": "clips/01451/01451__0004500_0011500.mp4"
  },
  {
    "timestamp": "00:16 - 00:18",
    "context": "[00:16 - 00:18] To expedite the departure, the person presses the 'door close' button, creating a soft, distinct click.",
    "question_type": "Inferential & Contextual Causality",
    "question": "What was the reason for pressing the 'door close' button at this moment?",
    "answer": "To expedite the departure by closing the doors.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01451.mp4",
    "question_id": "01451_7",
    "clip_path": "clips/01451/01451__0015500_0018500.mp4"
  },
  {
    "timestamp": "00:28 - 00:31",
    "context": "[00:28 - 00:31] The doors slide shut with a loud mechanical whirring and scraping that concludes with a soft thud as they meet, signifying the elevator is sealed and ready to ascend.",
    "question_type": "Inferential & Contextual Causality",
    "question": "What did the soft thud at the end of the door-closing sequence signify?",
    "answer": "That the elevator was sealed and ready to ascend.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01451.mp4",
    "question_id": "01451_8",
    "clip_path": "clips/01451/01451__0027500_0031500.mp4"
  },
  {
    "timestamp": "00:32 - 00:34",
    "context": "[00:32 - 00:34] After a period of travel, the elevator arrives. The doors slide open with the same loud mechanical whirring and scraping noise.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the elevator doors open at this time?",
    "answer": "Because the elevator had arrived at the destination after a period of travel.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01451.mp4",
    "question_id": "01451_9",
    "clip_path": "clips/01451/01451__0031500_0034500.mp4"
  },
  {
    "timestamp": "00:34 - 00:41",
    "context": "[00:34 - 00:41] A series of sharp, rhythmic tapping sounds begins, created by the tip of the white cane striking the tiled hallway floor as the person navigates.",
    "question_type": "Sound Source Identification",
    "question": "What generated the series of sharp, rhythmic tapping sounds in the hallway?",
    "answer": "The tip of the white cane striking the tiled hallway floor.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01451.mp4",
    "question_id": "01451_10",
    "clip_path": "clips/01451/01451__0033500_0041500.mp4"
  },
  {
    "timestamp": "00:34 - 00:41",
    "context": "[00:34 - 00:41] After stepping out, the elevator music fades into the background.",
    "question_type": "Temporal Information",
    "question": "How did the elevator music's volume change after the person stepped out?",
    "answer": "It faded into the background.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01451.mp4",
    "question_id": "01451_11",
    "clip_path": "clips/01451/01451__0033500_0041500.mp4"
  },
  {
    "timestamp": "00:41 - 00:45",
    "context": "[00:41 - 00:45] The person grasps the silver handle, producing a soft metallic click as they push it down, followed by a slightly louder, mechanical clunk as the lock disengages.",
    "question_type": "Sound Characteristics",
    "question": "How do the volumes and textures of the handle and lock sounds compare as the door opens?",
    "answer": "A soft metallic click from the handle is followed by a slightly louder mechanical clunk as the lock disengages.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01451.mp4",
    "question_id": "01451_12",
    "clip_path": "clips/01451/01451__0040500_0045500.mp4"
  },
  {
    "timestamp": "00:45 - 00:50",
    "context": "[00:45 - 00:50] As the door opens, a woman's welcoming voice is heard from inside the apartment.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From where relative to the camera did the welcoming voice originate when the door opened?",
    "answer": "From inside the apartment.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01451.mp4",
    "question_id": "01451_13",
    "clip_path": "clips/01451/01451__0044500_0050433.mp4"
  },
  {
    "timestamp": "00:45 - 00:50",
    "context": "[00:45 - 00:50] The mother replies with a warm '哎' and later exclaims '哎哟, 真棒'.",
    "question_type": "Counting",
    "question": "How many separate utterances from the mother are heard during this exchange?",
    "answer": "Two: a warm '哎' and '哎哟, 真棒'.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01451.mp4",
    "question_id": "01451_14",
    "clip_path": "clips/01451/01451__0044500_0050433.mp4"
  },
  {
    "timestamp": "00:00 - 00:12",
    "context": "[00:00 - 00:12] A person with a visual impairment navigates a shopping mall using a white cane, which produces a distinct, rhythmic tapping sound on the polished floor. While descending an escalator, they say: \"Hello everyone, I'm Fengqiao...\"",
    "question_type": "Sound Source Identification",
    "question": "What generated the distinct, rhythmic tapping sound at the start?",
    "answer": "The person's white cane contacting the polished floor.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01453.mp4",
    "question_id": "01453_1",
    "clip_path": "clips/01453/01453__0000000_0012500.mp4"
  },
  {
    "timestamp": "00:00 - 00:12",
    "context": "[00:00 - 00:12] The white cane produces a distinct, rhythmic tapping sound on the polished floor.",
    "question_type": "Sound Characteristics",
    "question": "How is the cane's tapping sound characterized?",
    "answer": "It is distinct and rhythmic.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01453.mp4",
    "question_id": "01453_2",
    "clip_path": "clips/01453/01453__0000000_0012500.mp4"
  },
  {
    "timestamp": "00:00 - 00:12",
    "context": "[00:00 - 00:12] The cane produces tapping on the polished floor as the person moves and descends an escalator.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Relative to the camera, where did the tapping originate?",
    "answer": "From the polished floor directly underfoot as they moved.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01453.mp4",
    "question_id": "01453_3",
    "clip_path": "clips/01453/01453__0000000_0012500.mp4"
  },
  {
    "timestamp": "00:00 - 00:12",
    "context": "[00:00 - 00:12] The person introduces themselves and says: \"Today I'm here to challenge myself to eat at Haidilao alone and to experience what it's like.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Based on the introduction, why is the person going to Haidilao alone?",
    "answer": "To challenge themselves and experience what it's like to dine there solo.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01453.mp4",
    "question_id": "01453_4",
    "clip_path": "clips/01453/01453__0000000_0012500.mp4"
  },
  {
    "timestamp": "00:12 - 00:22",
    "context": "[00:12 - 00:22] At the entrance, the person asks, \"Hello, is this Haidilao?\" The waiter confirms. The person states they are dining alone. The waiter replies, \"One person? Oh, okay. Come, follow me,\" and begins to lead them inside.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the waiter say, \"Come, follow me\"?",
    "answer": "Because the person confirmed they were dining alone at Haidilao, and he began leading them inside to be seated.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01453.mp4",
    "question_id": "01453_5",
    "clip_path": "clips/01453/01453__0011500_0022500.mp4"
  },
  {
    "timestamp": "00:22 - 00:58",
    "context": "[00:22 - 00:58] As the waiter guides the person through a corridor, the rhythmic tapping of the person's cane continues on the floor.",
    "question_type": "Temporal Information",
    "question": "During the corridor walk, is the cane tapping intermittent or continuous?",
    "answer": "Continuous.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01453.mp4",
    "question_id": "01453_6",
    "clip_path": "clips/01453/01453__0021500_0058500.mp4"
  },
  {
    "timestamp": "00:22 - 00:58",
    "context": "[00:22 - 00:58] The waiter leads them to a booth and says, \"Sit here, be careful,\" as the person uses their cane to locate the edge of the table before sitting down.",
    "question_type": "Cross-Modal Reasoning",
    "question": "After the waiter says, \"Sit here, be careful,\" how does the person locate the table edge before sitting?",
    "answer": "They use their cane to feel for the edge of the table before sitting down.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01453.mp4",
    "question_id": "01453_7",
    "clip_path": "clips/01453/01453__0021500_0058500.mp4"
  },
  {
    "timestamp": "01:02 - 01:15",
    "context": "[01:02 - 01:15] A waitress assists with the tablet menu while another staff member places a large panda plush on the seat opposite, a signature service for solo diners to provide companionship.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did a staff member place a large panda plush toy on the seat opposite the diner?",
    "answer": "It is a signature service to provide companionship for solo diners.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01453.mp4",
    "question_id": "01453_8",
    "clip_path": "clips/01453/01453__0061500_0075500.mp4"
  },
  {
    "timestamp": "01:02 - 01:15",
    "context": "[01:02 - 01:15] The waitress explains: \"The two-person meal is over 200, there's a four-person meal for over 300...\"",
    "question_type": "Counting",
    "question": "How many meal set sizes did the waitress explicitly mention by price category?",
    "answer": "Two: a two-person meal (over 200) and a four-person meal (over 300).",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01453.mp4",
    "question_id": "01453_9",
    "clip_path": "clips/01453/01453__0061500_0075500.mp4"
  },
  {
    "timestamp": "01:18 - 01:56",
    "context": "[01:18 - 01:56] The waitress begins cooking food in the hotpot. A slight sizzling sound is audible from the pot as she adds ingredients.",
    "question_type": "Sound Source Identification",
    "question": "What caused the slight sizzling sound during cooking?",
    "answer": "The hotpot sizzling as the waitress added ingredients.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01453.mp4",
    "question_id": "01453_10",
    "clip_path": "clips/01453/01453__0077500_0116500.mp4"
  },
  {
    "timestamp": "01:18 - 01:56",
    "context": "[01:18 - 01:56] \"A slight sizzling sound is audible from the pot as she adds ingredients.\"",
    "question_type": "Sound Characteristics",
    "question": "What was the loudness/quality of the sizzling from the pot?",
    "answer": "It was a slight, gentle sizzle.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01453.mp4",
    "question_id": "01453_11",
    "clip_path": "clips/01453/01453__0077500_0116500.mp4"
  },
  {
    "timestamp": "01:18 - 01:56",
    "context": "[01:18 - 01:56] The sizzling is heard as she adds ingredients to the hotpot.",
    "question_type": "Temporal Information",
    "question": "When did the sizzling start, relative to the cooking actions?",
    "answer": "It was heard as she added ingredients to the hotpot during 01:18–01:56.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01453.mp4",
    "question_id": "01453_12",
    "clip_path": "clips/01453/01453__0077500_0116500.mp4"
  },
  {
    "timestamp": "01:18 - 01:56",
    "context": "[01:18 - 01:56] The user asks: \"First, I want you to cook the vegetables for me, and then serve them to me...\" She serves the cooked food onto a separate plate, as the user wanted to avoid making the food too salty by putting it directly in the sauce.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the diner ask the waitress to serve the cooked vegetables onto a separate plate instead of directly into the sauce?",
    "answer": "To avoid making the food too salty.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01453.mp4",
    "question_id": "01453_13",
    "clip_path": "clips/01453/01453__0077500_0116500.mp4"
  },
  {
    "timestamp": "01:56 - 03:13",
    "context": "[01:56 - 03:13] Discussing visually impaired customers, the waitress adds: \"young people can now rely on technology... like phones, navigation, etc.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the waitress mention \"phones, navigation, etc.\" during the discussion?",
    "answer": "To explain that young people can now rely on technology for assistance and independence.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01453.mp4",
    "question_id": "01453_14",
    "clip_path": "clips/01453/01453__0115500_0193500.mp4"
  },
  {
    "timestamp": "03:13 - 03:40",
    "context": "[03:13 - 03:40] The person explains: \"I try a lot of things by myself... it's that if a person wants to do something, they can just go and do it. Right, that's the feeling I want.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "What motivation underlies the person's decision to undertake challenges like dining, concerts, and traveling alone?",
    "answer": "A desire for independence and personal freedom—the feeling that if you want to do something, you should just go do it.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01453.mp4",
    "question_id": "01453_15",
    "clip_path": "clips/01453/01453__0192500_0220500.mp4"
  },
  {
    "timestamp": "03:41 - 04:17",
    "context": "[03:41 - 04:17] A waitress brings a plate of fresh noodles and expertly stretches them by hand before placing them into the hotpot to cook, while reassuring the user they prioritize helping customers with special needs.",
    "question_type": "Cross-Modal Reasoning",
    "question": "While the waitress reassured the user about prioritizing help for customers with special needs, what tableside action accompanied this reassurance?",
    "answer": "She brought fresh noodles, stretched them by hand, and placed them into the hotpot to cook.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01453.mp4",
    "question_id": "01453_16",
    "clip_path": "clips/01453/01453__0220500_0257500.mp4"
  },
  {
    "timestamp": "03:41 - 04:17",
    "context": "[03:41 - 04:17] The waitress brings a plate of fresh noodles, stretches them by hand, then places them into the hotpot.",
    "question_type": "Counting",
    "question": "How many distinct noodle-handling actions did the waitress perform at the table?",
    "answer": "Three: she brought a plate of noodles, stretched them by hand, and placed them into the hotpot.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01453.mp4",
    "question_id": "01453_17",
    "clip_path": "clips/01453/01453__0220500_0257500.mp4"
  },
  {
    "timestamp": "00:00 - 00:05",
    "context": "In a dark movie theater aisle, a guide leads the user. The guide says there is a step ahead, and the user's soft, muffled footsteps on carpet are audible.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the guide warn the user to be careful about a step at this moment?",
    "answer": "Because there was a step ahead in the dark aisle, and the guide was preventing the user from tripping while walking.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01461.mp4",
    "question_id": "01461_1",
    "clip_path": "clips/01461/01461__0000000_0005500.mp4"
  },
  {
    "timestamp": "00:00 - 00:05",
    "context": "A female guide from the front says, \"Row 6, Seat 2,\" and the user confirms.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From which direction relative to the user did the guide announce the seat location?",
    "answer": "From the front.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01461.mp4",
    "question_id": "01461_2",
    "clip_path": "clips/01461/01461__0000000_0005500.mp4"
  },
  {
    "timestamp": "00:00 - 00:05",
    "context": "Their footsteps on the carpet are soft and muffled, audible over soft background music.",
    "question_type": "Sound Characteristics",
    "question": "How are the footsteps described acoustically?",
    "answer": "Soft and muffled.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01461.mp4",
    "question_id": "01461_3",
    "clip_path": "clips/01461/01461__0000000_0005500.mp4"
  },
  {
    "timestamp": "00:15 - 00:25",
    "context": "After the user is seated, the guide hands her a movie ticket, which makes a faint rustling sound.",
    "question_type": "Sound Source Identification",
    "question": "What produced the faint rustling sound after the user sat down?",
    "answer": "The movie ticket being handed to the user.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01461.mp4",
    "question_id": "01461_4",
    "clip_path": "clips/01461/01461__0014500_0025500.mp4"
  },
  {
    "timestamp": "00:15 - 00:25",
    "context": "The guide asks if they should come get her after the movie. The user replies: \"Okay, thank you, thank you, thank you... Yes, yes, please do, thank you... Mmm, okay, thank you.\"",
    "question_type": "Counting",
    "question": "How many times did the user say \"thank you\" in this exchange?",
    "answer": "Five times.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01461.mp4",
    "question_id": "01461_5",
    "clip_path": "clips/01461/01461__0014500_0025500.mp4"
  },
  {
    "timestamp": "00:26 - 00:41",
    "context": "While putting her ticket away, the handbag makes a distinct metallic click as she closes the clasp.",
    "question_type": "Sound Characteristics",
    "question": "What was the quality of the sound made by the handbag clasp when it was closed?",
    "answer": "A distinct metallic click.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01461.mp4",
    "question_id": "01461_6",
    "clip_path": "clips/01461/01461__0025500_0041500.mp4"
  },
  {
    "timestamp": "00:41 - 00:46",
    "context": "The user takes a plastic-wrapped item from her bag, causing a loud, sharp crinkling sound.",
    "question_type": "Sound Source Identification",
    "question": "What generated the loud, sharp crinkling sound?",
    "answer": "A plastic-wrapped item being handled.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01461.mp4",
    "question_id": "01461_7",
    "clip_path": "clips/01461/01461__0040500_0046500.mp4"
  },
  {
    "timestamp": "00:47 - 00:55",
    "context": "The user speaks about her phone. A soft click is heard as she places the phone down on her bag.",
    "question_type": "Sound Source Identification",
    "question": "What caused the soft click at the end of this segment?",
    "answer": "Placing the phone down on her bag.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01461.mp4",
    "question_id": "01461_8",
    "clip_path": "clips/01461/01461__0046500_0055500.mp4"
  },
  {
    "timestamp": "00:56 - 01:14",
    "context": "[00:56 - 01:04] A low-frequency mechanical humming and vibrating sound suddenly starts, originating from the user's chair. [01:05 - 01:14] While the humming continues, the user talks about it.",
    "question_type": "Temporal Information",
    "question": "When did the low-frequency humming start, and did it persist within this period?",
    "answer": "It started suddenly at 00:56 and continued through 01:14.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01461.mp4",
    "question_id": "01461_9",
    "clip_path": "clips/01461/01461__0055500_0074500.mp4"
  },
  {
    "timestamp": "00:56 - 01:04",
    "context": "A low-frequency mechanical humming and vibrating sound suddenly starts, originating from the user's chair. The camera pans to empty massage chairs beside her, confirming the source.",
    "question_type": "Sound Source Identification",
    "question": "What was the source of the humming and vibration?",
    "answer": "The user's massage chair.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01461.mp4",
    "question_id": "01461_10",
    "clip_path": "clips/01461/01461__0055500_0064500.mp4"
  },
  {
    "timestamp": "00:56 - 01:04",
    "context": "Startled by the sudden vibration, the user exclaims that she thought someone was behind her.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the user think someone was behind her?",
    "answer": "Because her chair unexpectedly started massaging and vibrating, which startled her.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01461.mp4",
    "question_id": "01461_11",
    "clip_path": "clips/01461/01461__0055500_0064500.mp4"
  },
  {
    "timestamp": "00:08 - 00:25",
    "context": "[00:08 - 00:15] The user says she needs guidance or she cannot find it herself. [00:15 - 00:25] The guide offers to come get her after the movie, and she gratefully agrees.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the user accept the offer to be picked up after the movie?",
    "answer": "Because she expressed reliance on her guides and said she couldn't find things herself.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01461.mp4",
    "question_id": "01461_12",
    "clip_path": "clips/01461/01461__0007500_0025500.mp4"
  },
  {
    "timestamp": "00:00 - 00:14",
    "context": "[00:00 - 00:14] The user is walking along a sidewalk, navigating with a white cane. The cane produces a series of sharp, rhythmic tapping and light scraping sounds as its tip makes contact with the paved ground and the edge of a low stone planter.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why are the sharp tapping and light scraping sounds occurring while the user walks?",
    "answer": "Because the cane tip is contacting the paved ground and the edge of a low stone planter as the user navigates.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01462.mp4",
    "question_id": "01462_1",
    "clip_path": "clips/01462/01462__0000000_0014467.mp4"
  },
  {
    "timestamp": "00:00 - 00:14",
    "context": "[00:00 - 00:14] The user says: \"Anyway, I'm mainly going to listen. The screen size doesn't matter to me.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why does the user say that the screen size doesn't matter to them?",
    "answer": "Because they are mainly going to listen.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01462.mp4",
    "question_id": "01462_2",
    "clip_path": "clips/01462/01462__0000000_0014467.mp4"
  },
  {
    "timestamp": "00:00 - 00:14",
    "context": "[00:00 - 00:14] The cane produces tapping and scraping as its tip makes contact with the paved ground and the edge of a low stone planter.",
    "question_type": "Sound Source Identification",
    "question": "What specifically is generating the tapping and scraping sounds?",
    "answer": "The white cane's tip contacting the paved ground and the edge of a low stone planter.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01462.mp4",
    "question_id": "01462_3",
    "clip_path": "clips/01462/01462__0000000_0014467.mp4"
  },
  {
    "timestamp": "00:00 - 00:14",
    "context": "[00:00 - 00:14] The cane produces a series of sharp, rhythmic tapping and light scraping sounds.",
    "question_type": "Sound Characteristics",
    "question": "How are the cane sounds described in terms of texture and pattern?",
    "answer": "They are sharp, rhythmic tapping and light scraping.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01462.mp4",
    "question_id": "01462_4",
    "clip_path": "clips/01462/01462__0000000_0014467.mp4"
  },
  {
    "timestamp": "00:00 - 00:14",
    "context": "[00:00 - 00:14] While walking, the user speaks in a clear, conversational tone, explaining their plans.",
    "question_type": "Sound Characteristics",
    "question": "What is the quality of the user's speech during this segment?",
    "answer": "Clear and conversational.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01462.mp4",
    "question_id": "01462_5",
    "clip_path": "clips/01462/01462__0000000_0014467.mp4"
  },
  {
    "timestamp": "00:00 - 00:14",
    "context": "[00:00 - 00:14] The cane sounds are coming from directly in front and very close to the camera.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where are the cane sounds located relative to the camera?",
    "answer": "Directly in front of and very close to the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01462.mp4",
    "question_id": "01462_6",
    "clip_path": "clips/01462/01462__0000000_0014467.mp4"
  },
  {
    "timestamp": "00:00 - 00:14",
    "context": "[00:00 - 00:14] The cane produces a series of tapping and scraping while the user walks.",
    "question_type": "Temporal Information",
    "question": "Over what time range are the cane's tapping and scraping heard?",
    "answer": "From 00:00 through 00:14.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01462.mp4",
    "question_id": "01462_7",
    "clip_path": "clips/01462/01462__0000000_0014467.mp4"
  },
  {
    "timestamp": "00:00 - 00:14",
    "context": "[00:00 - 00:14] The cane produces sharp, rhythmic tapping and light scraping sounds; the user also speaks.",
    "question_type": "Counting",
    "question": "How many distinct non-speech cane sound types are mentioned in this segment?",
    "answer": "Two: tapping and scraping.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01462.mp4",
    "question_id": "01462_8",
    "clip_path": "clips/01462/01462__0000000_0014467.mp4"
  },
  {
    "timestamp": "00:00 - 00:11",
    "context": "A person walks while holding a white cane and continuously taps its tip along the gray tactile paving, producing light, sharp, rhythmic clicking sounds very close to the camera. Simultaneously, they speak in a clear, conversational tone questioning the color and legitimacy of the tactile paving (yellow vs. gray).",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why does the speaker question whether the path is yellow or gray?",
    "answer": "To point out that tactile paving is typically yellow and highlight the unusual, non-standard gray paving here, questioning whether it is truly tactile paving.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01464.mp4",
    "question_id": "01464_1",
    "clip_path": "clips/01464/01464__0000000_0011500.mp4"
  },
  {
    "timestamp": "00:00 - 00:11",
    "context": "The person holds a white cane and continuously taps its tip along the tactile paving while walking, producing rhythmic clicks.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why is the cane being continuously tapped along the tactile paving as the person walks?",
    "answer": "To follow and confirm the tactile paving while walking, consistent with a visually impaired user navigating the path.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01464.mp4",
    "question_id": "01464_2",
    "clip_path": "clips/01464/01464__0000000_0011500.mp4"
  },
  {
    "timestamp": "00:00 - 00:11",
    "context": "The white cane's tip is tapped along the gray tactile paving, producing a series of light, sharp, rhythmic clicking sounds very close to the camera.",
    "question_type": "Sound Source Identification",
    "question": "What generated the series of light, sharp, rhythmic clicking sounds?",
    "answer": "The tip of the white cane tapping along the gray tactile paving on the ground.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01464.mp4",
    "question_id": "01464_3",
    "clip_path": "clips/01464/01464__0000000_0011500.mp4"
  },
  {
    "timestamp": "00:00 - 00:11",
    "context": "The tapping produces a series of light, sharp, rhythmic clicking sounds very close to the camera.",
    "question_type": "Sound Characteristics",
    "question": "What are the acoustic qualities of the clicking sounds produced by the cane?",
    "answer": "They are light, sharp, and rhythmic clicking sounds.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01464.mp4",
    "question_id": "01464_4",
    "clip_path": "clips/01464/01464__0000000_0011500.mp4"
  },
  {
    "timestamp": "00:00 - 00:11",
    "context": "The clicking sounds from the cane tapping the tactile paving are described as very close to the camera and occurring on the ground.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the clicking sounds originate relative to the camera?",
    "answer": "From the ground very close to the camera, along the tactile paving underfoot.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01464.mp4",
    "question_id": "01464_5",
    "clip_path": "clips/01464/01464__0000000_0011500.mp4"
  },
  {
    "timestamp": "00:00 - 00:11",
    "context": "Throughout the clip, the cane is continuously tapped along the tactile paving, producing a rhythmic series of clicks while the person speaks.",
    "question_type": "Temporal Information",
    "question": "When and for how long are the clicking sounds heard, and are they continuous?",
    "answer": "They are heard continuously throughout 00:00–00:11 as a rhythmic series while the person walks.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01464.mp4",
    "question_id": "01464_6",
    "clip_path": "clips/01464/01464__0000000_0011500.mp4"
  },
  {
    "timestamp": "00:00 - 00:11",
    "context": "The cane clicks occur while the person simultaneously speaks in a clear, conversational tone.",
    "question_type": "Counting",
    "question": "How many primary sound sources are simultaneously present during this segment?",
    "answer": "Two: the cane’s rhythmic clicking and the person’s speech.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01464.mp4",
    "question_id": "01464_7",
    "clip_path": "clips/01464/01464__0000000_0011500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "[00:00 - 00:03] A phone emits a series of short, high-pitched electronic beeps from a navigation app. The user asks, in an uncertain tone, “我是要在这里转弯吗?” (“Do I need to turn here?”).",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the user ask, “Do I need to turn here?” at 00:00 - 00:03?",
    "answer": "Because they were uncertain and sought to confirm their direction after hearing the navigation beeps.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01468.mp4",
    "question_id": "01468_1",
    "clip_path": "clips/01468/01468__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "[00:00 - 00:03] The phone emits a series of short, high-pitched electronic beeps from a navigation app.",
    "question_type": "Sound Source Identification",
    "question": "What generated the series of short, high-pitched electronic beeps?",
    "answer": "The phone’s navigation app.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01468.mp4",
    "question_id": "01468_2",
    "clip_path": "clips/01468/01468__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "[00:00 - 00:03] A series of short, high-pitched electronic beeps is heard from the phone.",
    "question_type": "Sound Characteristics",
    "question": "What are the pitch and duration characteristics of the beeps at the start?",
    "answer": "They are short, high-pitched electronic beeps.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01468.mp4",
    "question_id": "01468_3",
    "clip_path": "clips/01468/01468__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:03 - 00:06",
    "context": "[00:03 - 00:06] A synthetic female voice says “当前方向正确” (“Current direction is correct”), immediately followed by a short, pleasant electronic chime.",
    "question_type": "Temporal Information",
    "question": "When did the electronic chime occur relative to the spoken guidance?",
    "answer": "It occurred immediately after the spoken statement.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01468.mp4",
    "question_id": "01468_4",
    "clip_path": "clips/01468/01468__0002500_0006500.mp4"
  },
  {
    "timestamp": "00:03 - 00:06",
    "context": "[00:03 - 00:06] The app announces “当前方向正确,” then a short, pleasant chime follows.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the phone play the electronic chime after saying “当前方向正确”?",
    "answer": "To confirm the instruction that the current direction is correct.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01468.mp4",
    "question_id": "01468_5",
    "clip_path": "clips/01468/01468__0002500_0006500.mp4"
  },
  {
    "timestamp": "00:03 - 00:06",
    "context": "[00:03 - 00:06] A synthetic female voice from the phone states, “当前方向正确.”",
    "question_type": "Sound Source Identification",
    "question": "What was the source of the spoken phrase “当前方向正确” (“Current direction is correct”)?",
    "answer": "A synthetic female voice from the phone’s navigation app.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01468.mp4",
    "question_id": "01468_6",
    "clip_path": "clips/01468/01468__0002500_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "[00:00 - 00:03] Series of short, high-pitched beeps from the navigation app. [00:03 - 00:06] A short, pleasant electronic chime follows the spoken guidance.",
    "question_type": "Counting",
    "question": "How many distinct types of electronic cues did the navigation app produce in the opening six seconds?",
    "answer": "Two: a series of short, high-pitched beeps and a short, pleasant chime.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01468.mp4",
    "question_id": "01468_7",
    "clip_path": "clips/01468/01468__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:09 - 00:11",
    "context": "[00:06 - 00:09] The user asks a pedestrian, “太平洋电影院在哪个方向?” [00:09 - 00:11] The pedestrian replies enthusiastically, “到了到了” (“We’re here, we’re here”), indicating arrival.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the pedestrian say “我们到了”/“到了到了” (“We’re here, we’re here”)?",
    "answer": "Because the user had reached the Pacific Cinema, their intended destination.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01468.mp4",
    "question_id": "01468_8",
    "clip_path": "clips/01468/01468__0008500_0011500.mp4"
  },
  {
    "timestamp": "00:09 - 00:11",
    "context": "[00:09 - 00:11] Concurrent with the dialogue, the distinct, moderately loud sound of the user’s cane tip rhythmically taps and scrapes against rough asphalt.",
    "question_type": "Sound Characteristics",
    "question": "What are the volume and texture characteristics of the cane sound heard at 00:09 - 00:11?",
    "answer": "It is distinct and moderately loud, with rhythmic tapping and scraping against rough asphalt.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01468.mp4",
    "question_id": "01468_9",
    "clip_path": "clips/01468/01468__0008500_0011500.mp4"
  },
  {
    "timestamp": "00:09 - 00:11",
    "context": "[00:09 - 00:11] The pedestrian says “到了到了” while the cane tip tapping and scraping is audible as the user navigates forward.",
    "question_type": "Temporal Information",
    "question": "Was the cane sound simultaneous with the pedestrian’s reply, and during which interval?",
    "answer": "Yes, it occurred concurrently during 00:09 - 00:11.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01468.mp4",
    "question_id": "01468_10",
    "clip_path": "clips/01468/01468__0008500_0011500.mp4"
  },
  {
    "timestamp": "00:00 - 00:05",
    "context": "[00:00] Milk in the pot audibly bubbles and boils. [00:03] The user says, \"Turn off the heat and then take this pot off the stove,\" and lifts the pot to stop cooking.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the user lift the pot off the stove and say to turn off the heat at 00:00 - 00:05?",
    "answer": "Because the milk was audibly boiling, and she wanted to stop the cooking process.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01470.mp4",
    "question_id": "01470_1",
    "clip_path": "clips/01470/01470__0000000_0005500.mp4"
  },
  {
    "timestamp": "00:05 - 00:13",
    "context": "[00:05] The user sets a metal pot on the stove, producing a distinct metallic clank, then introduces the topic.",
    "question_type": "Sound Source Identification",
    "question": "What produced the distinct metallic clank heard at 00:05 - 00:13?",
    "answer": "Setting a metal pot on the stove.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01470.mp4",
    "question_id": "01470_2",
    "clip_path": "clips/01470/01470__0004500_0013500.mp4"
  },
  {
    "timestamp": "00:13 - 00:23",
    "context": "[00:13] Soft rustling of a plastic bag as the user shows fresh milk. [00:15] She pours milk into the pot, creating a continuous pouring sound. [00:21] Brief stirring causes the spoon to clink against the pot.",
    "question_type": "Sound Characteristics",
    "question": "Was the sound of the milk being poured continuous or intermittent?",
    "answer": "Continuous.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01470.mp4",
    "question_id": "01470_3",
    "clip_path": "clips/01470/01470__0012500_0023500.mp4"
  },
  {
    "timestamp": "00:24 - 00:32",
    "context": "[00:24] The user removes the pot's lid with a light metallic clink, then stirs and replaces the lid with another clink.",
    "question_type": "Counting",
    "question": "How many lid clink sounds are heard in this segment?",
    "answer": "Two clinks—one when removing the lid and one when replacing it.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01470.mp4",
    "question_id": "01470_4",
    "clip_path": "clips/01470/01470__0023500_0032500.mp4"
  },
  {
    "timestamp": "00:24 - 00:26",
    "context": "[00:24] The user removes the pot's lid, which makes a light metallic clink.",
    "question_type": "Sound Characteristics",
    "question": "What is the quality of the sound when the lid is first removed?",
    "answer": "A light metallic clink.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01470.mp4",
    "question_id": "01470_5",
    "clip_path": "clips/01470/01470__0023500_0026500.mp4"
  },
  {
    "timestamp": "00:32 - 00:41",
    "context": "[00:32 - 00:41] The user continuously stirs the milk, producing gentle sloshing and clinking sounds.",
    "question_type": "Sound Source Identification",
    "question": "What action generated the gentle sloshing and clinking sounds between 00:32 and 00:41?",
    "answer": "Continuously stirring the milk in the pot with a spoon.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01470.mp4",
    "question_id": "01470_6",
    "clip_path": "clips/01470/01470__0031500_0041500.mp4"
  },
  {
    "timestamp": "00:41 - 00:50",
    "context": "[00:41] She lifts the lid with a soft clink and says, \"Now I seem to hear the sound of boiling.\" [00:45] She says, \"Turn off the heat and then take this pot off the stove,\" and moves it to cool.",
    "question_type": "Inferential & Contextual Causality",
    "question": "What prompted the user to turn off the heat and move the pot off the stove at 00:41 - 00:50?",
    "answer": "Hearing the sound of boiling signaled it was time to stop heating and begin cooling.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01470.mp4",
    "question_id": "01470_7",
    "clip_path": "clips/01470/01470__0040500_0050500.mp4"
  },
  {
    "timestamp": "00:50 - 01:03",
    "context": "[00:50] The user adds yogurt starter from a small container, accompanied by a plastic spoon scraping sound. [00:55] She vigorously stirs, creating prominent sloshing and clinking.",
    "question_type": "Sound Source Identification",
    "question": "What produced the scraping sound heard while adding the yogurt starter?",
    "answer": "A plastic spoon scraping the small container of yogurt starter.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01470.mp4",
    "question_id": "01470_8",
    "clip_path": "clips/01470/01470__0049500_0063500.mp4"
  },
  {
    "timestamp": "00:50 - 01:03",
    "context": "[00:55 - 01:03] The user vigorously stirs the mixture with a metal spoon, creating prominent sloshing and clinking sounds.",
    "question_type": "Sound Characteristics",
    "question": "How are the sloshing and clinking sounds characterized during the vigorous stirring?",
    "answer": "They are prominent.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01470.mp4",
    "question_id": "01470_9",
    "clip_path": "clips/01470/01470__0049500_0063500.mp4"
  },
  {
    "timestamp": "01:03 - 01:07",
    "context": "[01:03] The user covers the pot and wraps it in a large plastic bag, producing a loud, continuous rustling sound. [01:06] She says she needs to put it in a very warm place.",
    "question_type": "Temporal Information",
    "question": "What are the temporal and volume characteristics of the rustling sound while wrapping the pot?",
    "answer": "It is loud and continuous.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01470.mp4",
    "question_id": "01470_10",
    "clip_path": "clips/01470/01470__0062500_0067500.mp4"
  },
  {
    "timestamp": "01:07 - 01:13",
    "context": "[01:07] After a time jump, she says it's three hours later. [01:10] She uncovers the pot, removing the metal lid with a single, clear clank.",
    "question_type": "Counting",
    "question": "How many clank sounds are heard when the pot is uncovered after the time jump?",
    "answer": "One—a single, clear clank.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01470.mp4",
    "question_id": "01470_11",
    "clip_path": "clips/01470/01470__0066500_0073500.mp4"
  },
  {
    "timestamp": "01:13 - 01:22",
    "context": "[01:13] She observes the yogurt and notes, \"if you refrigerate it, it might set even better.\" [01:20] She decides, \"So let's find a container to refrigerate it.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why does the user decide to find a container to refrigerate the yogurt?",
    "answer": "Because she believes refrigerating it might help it set better and improve the texture.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01470.mp4",
    "question_id": "01470_12",
    "clip_path": "clips/01470/01470__0072500_0082500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "As the user is guided by the arm into a pottery studio, a female voice from the guide speaks closely and clearly: “前面有一些客人做的” (There are some works made by customers in the front), followed by “我们要拐弯了” (We are going to turn) as they walk past a reception desk into the main workshop.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the female guide say “我们要拐弯了” (We are going to turn)?",
    "answer": "She was directing a change in direction while guiding the user into the main workshop area.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01472.mp4",
    "question_id": "01472_1",
    "clip_path": "clips/01472/01472__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "A female voice, originating from the person guiding the user by the arm, speaks at a close, clear, conversational volume.",
    "question_type": "Sound Source Identification",
    "question": "Who is the source of the speech heard in this segment?",
    "answer": "The female person guiding the user by the arm.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01472.mp4",
    "question_id": "01472_2",
    "clip_path": "clips/01472/01472__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "The guide’s speech is described as close, clear, and at a conversational volume.",
    "question_type": "Sound Characteristics",
    "question": "What are the volume and clarity characteristics of the speaker’s voice?",
    "answer": "It is close, clear, and at a conversational volume.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01472.mp4",
    "question_id": "01472_3",
    "clip_path": "clips/01472/01472__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "The female voice is noted as originating from the person guiding the user.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the speech originate relative to the camera?",
    "answer": "From very close proximity, coming from the person guiding the user.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01472.mp4",
    "question_id": "01472_4",
    "clip_path": "clips/01472/01472__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "The speaker first says “前面有一些客人做的,” then adds “我们要拐弯了.”",
    "question_type": "Temporal Information",
    "question": "Which utterance occurs first: the mention of customer-made works or the turning instruction?",
    "answer": "The mention of customer-made works occurs first, followed by the turning instruction.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01472.mp4",
    "question_id": "01472_5",
    "clip_path": "clips/01472/01472__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "Two spoken lines are quoted: “前面有一些客人做的” and “我们要拐弯了.”",
    "question_type": "Counting",
    "question": "How many distinct spoken sentences does the guide utter in this segment?",
    "answer": "Two.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01472.mp4",
    "question_id": "01472_6",
    "clip_path": "clips/01472/01472__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "[00:00 - 00:07] The instructor takes the finished ceramic piece and places it back on a wooden shelf, producing a soft clinking sound.",
    "question_type": "Sound Source Identification",
    "question": "What generated the soft clinking sound at the end of 00:00 - 00:07?",
    "answer": "The instructor placing the ceramic piece back on the wooden shelf.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01477.mp4",
    "question_id": "01477_1",
    "clip_path": "clips/01477/01477__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "[00:00 - 00:07] A soft clinking sound is heard when the piece is placed on the shelf.",
    "question_type": "Sound Characteristics",
    "question": "How is the clinking sound described when the piece is placed back on the shelf?",
    "answer": "It is a soft clinking sound.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01477.mp4",
    "question_id": "01477_2",
    "clip_path": "clips/01477/01477__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:07 - 00:12",
    "context": "[00:07 - 00:12] The instructor, standing to the left, explains in a clear, conversational voice about porcelain clay from Jingdezhen.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From which direction relative to the camera does the instructor’s explanation originate during 00:07 - 00:12?",
    "answer": "From the left side.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01477.mp4",
    "question_id": "01477_3",
    "clip_path": "clips/01477/01477__0006500_0012500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "[00:00 - 00:07] As the instructor places the piece back on the shelf, a soft clinking sound is produced.",
    "question_type": "Temporal Information",
    "question": "Did the soft clinking sound occur before, during, or after the ceramic piece was placed back on the shelf?",
    "answer": "It occurred as the piece was being placed back on the shelf.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01477.mp4",
    "question_id": "01477_4",
    "clip_path": "clips/01477/01477__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:33 - 00:48",
    "context": "[00:33 - 00:48] The user gasps in surprise, saying, \"Ah! Wow!\" upon realizing they were handed raw grey clay.",
    "question_type": "Counting",
    "question": "How many distinct surprised interjections did the user make?",
    "answer": "Two (“Ah!” and “Wow!”).",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01477.mp4",
    "question_id": "01477_5",
    "clip_path": "clips/01477/01477__0032500_0048500.mp4"
  },
  {
    "timestamp": "00:18 - 00:25",
    "context": "[00:18 - 00:25] The user says the pieces don't seem very big. The instructor replies, \"Right, because it will shrink a bit after firing.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "According to the instructor, why do the pieces not seem very big?",
    "answer": "Because they shrink a bit after firing.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01477.mp4",
    "question_id": "01477_6",
    "clip_path": "clips/01477/01477__0017500_0025500.mp4"
  },
  {
    "timestamp": "00:00 - 00:12",
    "context": "[00:00 - 00:07] The user marvels that the soft clay became hard. [00:07 - 00:12] The instructor explains, \"Yes, it's porcelain clay... from Jingdezhen... after it's fired, it becomes this kind of porcelain.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "What likely prompted the instructor to explain that the material is porcelain clay from Jingdezhen?",
    "answer": "The user’s amazement at the transformation from soft to hard prompted the explanation.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01477.mp4",
    "question_id": "01477_7",
    "clip_path": "clips/01477/01477__0000000_0012500.mp4"
  },
  {
    "timestamp": "00:12 - 00:18",
    "context": "[00:12 - 00:18] The instructor and user speak while examining a pink pig-shaped cup; their voices are described as close and clear.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "How close are the speakers to the camera during their discussion of the pig-shaped cup?",
    "answer": "Their voices are close-range and clear.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01477.mp4",
    "question_id": "01477_8",
    "clip_path": "clips/01477/01477__0011500_0018500.mp4"
  },
  {
    "timestamp": "00:12 - 00:18",
    "context": "[00:12 - 00:18] The instructor remarks, \"Oh, a little pig cup.\" The user responds nearby; both voices are close and clear.",
    "question_type": "Sound Characteristics",
    "question": "What is the clarity of the voices during the exchange about the pig-shaped cup?",
    "answer": "They are close and clear.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01477.mp4",
    "question_id": "01477_9",
    "clip_path": "clips/01477/01477__0011500_0018500.mp4"
  },
  {
    "timestamp": "00:48 - 01:07",
    "context": "[00:48 - 01:07] The instructor explains the technique: \"This is a hand-kneaded dinnerware set... purely hand-kneaded and then stacked... if it's not stuck well, it will fall off.\"",
    "question_type": "Temporal Information",
    "question": "During which interval does the instructor explain the hand-kneaded stacking technique and the issue with sticking?",
    "answer": "Between 00:48 and 01:07.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01477.mp4",
    "question_id": "01477_10",
    "clip_path": "clips/01477/01477__0047500_0067500.mp4"
  },
  {
    "timestamp": "00:33 - 00:48",
    "context": "[00:33 - 00:48] The user exclaims \"Ah! Wow!\" after realizing they were handed a soft piece of raw grey clay instead of a finished sculpture.",
    "question_type": "Cross-Modal Reasoning",
    "question": "Based on the surprised exclamation, what change in the object’s state did the user realize?",
    "answer": "They realized they had been handed soft raw grey clay instead of a finished hard sculpture.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01477.mp4",
    "question_id": "01477_11",
    "clip_path": "clips/01477/01477__0032500_0048500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "[00:00 - 00:07] The user speaks in a moderate tone about the transformation: \"Wow, how magical... now it has become so hard.\"",
    "question_type": "Sound Characteristics",
    "question": "What was the tone of the user's initial remark about the clay's transformation?",
    "answer": "A moderate tone.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01477.mp4",
    "question_id": "01477_12",
    "clip_path": "clips/01477/01477__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:00 - 00:13",
    "context": "[00:00 - 00:13] Walking on a stone path; a female companion to the left speaks clearly at moderate volume, describing a memorial sculpture. Faint, rhythmic footstep taps are audible.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why was the female companion speaking about the sculpture during 00:00–00:13?",
    "answer": "She was directly commenting on the memorial they were observing.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01481.mp4",
    "question_id": "01481_1",
    "clip_path": "clips/01481/01481__0000000_0013500.mp4"
  },
  {
    "timestamp": "00:00 - 00:13",
    "context": "Faint, rhythmic tapping sounds are heard while walking on the stone pathway.",
    "question_type": "Sound Source Identification",
    "question": "What generated the faint, rhythmic tapping sounds in the background at 00:00–00:13?",
    "answer": "Their footsteps on the stone pathway.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01481.mp4",
    "question_id": "01481_2",
    "clip_path": "clips/01481/01481__0000000_0013500.mp4"
  },
  {
    "timestamp": "00:00 - 00:13",
    "context": "Footstep sounds are described as 'faint, rhythmic' while in the background.",
    "question_type": "Sound Characteristics",
    "question": "How are the footstep tapping sounds characterized in terms of loudness and pattern?",
    "answer": "They are faint and rhythmic.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01481.mp4",
    "question_id": "01481_3",
    "clip_path": "clips/01481/01481__0000000_0013500.mp4"
  },
  {
    "timestamp": "00:00 - 00:13",
    "context": "The speaking companion is positioned to the camera holder's left.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From which side relative to the camera did the speaking companion's voice originate?",
    "answer": "From the left side.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01481.mp4",
    "question_id": "01481_4",
    "clip_path": "clips/01481/01481__0000000_0013500.mp4"
  },
  {
    "timestamp": "00:13 - 00:15",
    "context": "[00:13 - 00:15] A woman carrying a toddler passes; the camera holder says '不好意思啊' in a slightly apologetic tone.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Based on the situation, why did the camera holder say '不好意思啊' (Excuse me) at 00:13–00:15?",
    "answer": "Because their cane or arm got close to the passerby as she walked past.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01481.mp4",
    "question_id": "01481_5",
    "clip_path": "clips/01481/01481__0012500_0015500.mp4"
  },
  {
    "timestamp": "00:15 - 00:18",
    "context": "[00:15 - 00:18] The passerby replies '没事' in a calm, nearby voice.",
    "question_type": "Temporal Information",
    "question": "When did the passerby reply '没事' (It's okay)?",
    "answer": "Between 00:15 and 00:18.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01481.mp4",
    "question_id": "01481_6",
    "clip_path": "clips/01481/01481__0014500_0018500.mp4"
  },
  {
    "timestamp": "00:15 - 00:18",
    "context": "The passerby's response is described as 'a calm, nearby voice.'",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "What was the proximity of the passerby's reply relative to the camera?",
    "answer": "It was nearby.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01481.mp4",
    "question_id": "01481_7",
    "clip_path": "clips/01481/01481__0014500_0018500.mp4"
  },
  {
    "timestamp": "00:15 - 00:18",
    "context": "Immediately after the reply, the camera holder takes a plastic water bottle from the companion’s left hand; a soft rustling is heard.",
    "question_type": "Sound Source Identification",
    "question": "What caused the soft rustling sound immediately after the passerby said '没事'?",
    "answer": "Handling the plastic water bottle during the handoff.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01481.mp4",
    "question_id": "01481_8",
    "clip_path": "clips/01481/01481__0014500_0018500.mp4"
  },
  {
    "timestamp": "00:13 - 00:18",
    "context": "[00:13] The camera holder says '不好意思啊.' [00:15] The passerby replies '没事.'",
    "question_type": "Counting",
    "question": "How many distinct speakers are heard in the apology-and-reply exchange from 00:13 to 00:18?",
    "answer": "Two.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01481.mp4",
    "question_id": "01481_9",
    "clip_path": "clips/01481/01481__0012500_0018500.mp4"
  },
  {
    "timestamp": "00:18 - 00:19",
    "context": "[00:18 - 00:19] Now holding both a bottle and a white cane, the camera holder says, '我把盲杖这么拿' (I'll hold the cane like this).",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the camera holder say '我把盲杖这么拿' (I'll hold the cane like this) at 00:18–00:19?",
    "answer": "Because they were holding both a bottle and a cane and were adjusting their grip for comfort or stability while walking.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01481.mp4",
    "question_id": "01481_10",
    "clip_path": "clips/01481/01481__0017500_0019500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "A hand places a white flower onto a memorial area while a quiet Mandarin conversation is heard. A nearby voice says, “放在这里就可以了” (“Just put it here is fine”), indicating the purpose of the action.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Based on the nearby instruction, why was the flower placed at that specific spot?",
    "answer": "To respectfully lay the flower in the designated memorial area, as confirmed by the nearby voice.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01484.mp4",
    "question_id": "01484_1",
    "clip_path": "clips/01484/01484__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "During the flower placement, a nearby voice is heard saying, “放在这里就可以了” (“Just put it here is fine”).",
    "question_type": "Sound Source Identification",
    "question": "Who delivered the confirmation, “Just put it here is fine”?",
    "answer": "A nearby voice, likely a companion.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01484.mp4",
    "question_id": "01484_2",
    "clip_path": "clips/01484/01484__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "The action is accompanied by a quiet conversation in Mandarin.",
    "question_type": "Sound Characteristics",
    "question": "What was the volume of the conversation heard during the flower placement?",
    "answer": "Quiet.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01484.mp4",
    "question_id": "01484_3",
    "clip_path": "clips/01484/01484__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "A voice, likely a companion's, can be heard from nearby confirming the action.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the confirming voice originate relative to the camera?",
    "answer": "From nearby.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01484.mp4",
    "question_id": "01484_4",
    "clip_path": "clips/01484/01484__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:03 - 00:10",
    "context": "After placing the flower, the person begins to walk away. Soft footsteps on a hard floor are audible and synchronized with the camera’s movement.",
    "question_type": "Temporal Information",
    "question": "When do the soft footsteps start, and how long are they heard?",
    "answer": "They start after the flower is placed and are heard throughout 00:03–00:10 while walking away.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01484.mp4",
    "question_id": "01484_5",
    "clip_path": "clips/01484/01484__0002500_0009833.mp4"
  },
  {
    "timestamp": "00:03 - 00:10",
    "context": "The person walks away from the memorial; their soft footsteps on the hard floor are audible and synchronized with the camera’s movement.",
    "question_type": "Sound Source Identification",
    "question": "What generated the soft footsteps heard between 00:03 and 00:10?",
    "answer": "The person who had placed the flower, walking away on the hard floor.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01484.mp4",
    "question_id": "01484_6",
    "clip_path": "clips/01484/01484__0002500_0009833.mp4"
  },
  {
    "timestamp": "00:03 - 00:10",
    "context": "In the large, open exhibition hall, the low, reverberant murmur of other visitors becomes more prominent.",
    "question_type": "Sound Characteristics",
    "question": "How is the background crowd noise described in terms of volume and acoustic quality?",
    "answer": "A low, reverberant murmur.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01484.mp4",
    "question_id": "01484_7",
    "clip_path": "clips/01484/01484__0002500_0009833.mp4"
  },
  {
    "timestamp": "00:03 - 00:10",
    "context": "Soft footsteps are heard while walking away, and the low, reverberant murmur of other visitors becomes more prominent.",
    "question_type": "Counting",
    "question": "How many distinct concurrent sound types are notable during 00:03–00:10?",
    "answer": "Two: the soft footsteps and the murmur of other visitors.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01484.mp4",
    "question_id": "01484_8",
    "clip_path": "clips/01484/01484__0002500_0009833.mp4"
  },
  {
    "timestamp": "00:00 - 00:05",
    "context": "[00:00 - 00:05] Walking on the street while following a man in a black shirt, they converse: Man in front: \"Are you going too?\" Camera-person: \"I'm going too.\" Man: \"It's scheduled for around 2:30.\" Camera-person: \"About right.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the man clarify the meeting time during the brief exchange at the start?",
    "answer": "They were confirming their plans while walking.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01489.mp4",
    "question_id": "01489_1",
    "clip_path": "clips/01489/01489__0000000_0005500.mp4"
  },
  {
    "timestamp": "00:00 - 00:05",
    "context": "[00:00 - 00:05] The man in front asks, \"Are you going too?\"",
    "question_type": "Sound Source Identification",
    "question": "Who asked, \"Are you going too?\"",
    "answer": "The man in front of the camera-person.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01489.mp4",
    "question_id": "01489_2",
    "clip_path": "clips/01489/01489__0000000_0005500.mp4"
  },
  {
    "timestamp": "00:00 - 00:05",
    "context": "[00:00 - 00:05] They have a short, clear conversation confirming plans and time.",
    "question_type": "Temporal Information",
    "question": "Was their conversation brief or extended at the start?",
    "answer": "It was brief (short).",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01489.mp4",
    "question_id": "01489_3",
    "clip_path": "clips/01489/01489__0000000_0005500.mp4"
  },
  {
    "timestamp": "00:08 - 00:10",
    "context": "[00:08 - 00:10] While passing a group on a crowded sidewalk, a woman's voice from the immediate left says clearly, \"Thank you.\"",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From where relative to the camera did the 'Thank you' originate?",
    "answer": "From the immediate left, at close range.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01489.mp4",
    "question_id": "01489_4",
    "clip_path": "clips/01489/01489__0007500_0010500.mp4"
  },
  {
    "timestamp": "00:08 - 00:10",
    "context": "[00:08 - 00:10] A woman's voice says, \"Thank you,\" in a clear, polite tone.",
    "question_type": "Sound Characteristics",
    "question": "What was the tone and clarity of the woman's 'Thank you'?",
    "answer": "Polite and clearly audible.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01489.mp4",
    "question_id": "01489_5",
    "clip_path": "clips/01489/01489__0007500_0010500.mp4"
  },
  {
    "timestamp": "00:08 - 00:10",
    "context": "[00:08 - 00:10] A woman says, \"Thank you.\"",
    "question_type": "Counting",
    "question": "How many times was 'Thank you' said?",
    "answer": "Once.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01489.mp4",
    "question_id": "01489_6",
    "clip_path": "clips/01489/01489__0007500_0010500.mp4"
  },
  {
    "timestamp": "00:10 - 00:13",
    "context": "[00:10 - 00:13] The camera-person's white cane sweeps left and comes very close to a woman in a beige shirt. The woman's voice from the left responds, \"Oh, sorry, sorry,\" as she steps aside.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the woman apologize by saying, \"Oh, sorry, sorry\"?",
    "answer": "She was reacting to the white cane sweeping close to her.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01489.mp4",
    "question_id": "01489_7",
    "clip_path": "clips/01489/01489__0009500_0013133.mp4"
  },
  {
    "timestamp": "00:10 - 00:13",
    "context": "[00:10 - 00:13] The woman says, \"Oh, sorry, sorry.\"",
    "question_type": "Counting",
    "question": "How many times did the woman say the word 'sorry'?",
    "answer": "Twice.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01489.mp4",
    "question_id": "01489_8",
    "clip_path": "clips/01489/01489__0009500_0013133.mp4"
  },
  {
    "timestamp": "00:10 - 00:13",
    "context": "[00:10 - 00:13] The woman's apology is heard from the left.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From which direction relative to the camera did the woman's apology come?",
    "answer": "From the left side.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01489.mp4",
    "question_id": "01489_9",
    "clip_path": "clips/01489/01489__0009500_0013133.mp4"
  },
  {
    "timestamp": "00:10 - 00:13",
    "context": "[00:10 - 00:13] The white cane sweeps close to the woman; she responds, \"Oh, sorry, sorry,\" and steps aside.",
    "question_type": "Cross-Modal Reasoning",
    "question": "When the white cane swept close to the woman, what did she say and what did she do?",
    "answer": "She said, \"Oh, sorry, sorry,\" and stepped aside.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01489.mp4",
    "question_id": "01489_10",
    "clip_path": "clips/01489/01489__0009500_0013133.mp4"
  },
  {
    "timestamp": "00:00 - 00:05",
    "context": "[00:00-00:05] The cane's tip makes a series of distinct, moderately loud tapping sounds as it rhythmically strikes the tiled pavement in front of the user.",
    "question_type": "Sound Source Identification",
    "question": "What generated the rhythmic tapping sounds at the start of the video?",
    "answer": "The white cane's tip striking the tiled pavement.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01490.mp4",
    "question_id": "01490_1",
    "clip_path": "clips/01490/01490__0000000_0005500.mp4"
  },
  {
    "timestamp": "00:00 - 00:05",
    "context": "[00:00-00:05] The cane's tip makes a series of distinct, moderately loud tapping sounds as it rhythmically strikes the tiled pavement.",
    "question_type": "Sound Characteristics",
    "question": "How are the cane taps characterized in terms of texture and volume?",
    "answer": "They are distinct, moderately loud, and rhythmic.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01490.mp4",
    "question_id": "01490_2",
    "clip_path": "clips/01490/01490__0000000_0005500.mp4"
  },
  {
    "timestamp": "00:00 - 00:05",
    "context": "[00:00-00:05] A woman, walking towards the user from about 3 meters ahead, asks: 'Hey, are you from Northeast China?'",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the woman's initial question originate relative to the camera?",
    "answer": "From directly ahead at about 3 meters away, approaching the user.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01490.mp4",
    "question_id": "01490_3",
    "clip_path": "clips/01490/01490__0000000_0005500.mp4"
  },
  {
    "timestamp": "00:05 - 00:08",
    "context": "[00:05-00:08] The man and woman walk past the user on the left within 1–2 meters. The woman asks: 'You've made the appointment, right, sis?' Her voice is just beside and slightly behind the user.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From what relative position did the woman's confirming question come as they passed?",
    "answer": "From the left side, just beside and slightly behind the user within 1–2 meters.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01490.mp4",
    "question_id": "01490_4",
    "clip_path": "clips/01490/01490__0004500_0008500.mp4"
  },
  {
    "timestamp": "00:05 - 00:08",
    "context": "[00:05-00:08] As they pass closely, the man's voice is briefly audible but indistinct.",
    "question_type": "Sound Characteristics",
    "question": "What was the clarity of the man's voice as he passed the user?",
    "answer": "It was briefly audible but indistinct.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01490.mp4",
    "question_id": "01490_5",
    "clip_path": "clips/01490/01490__0004500_0008500.mp4"
  },
  {
    "timestamp": "00:08 - 00:11",
    "context": "[00:08-00:11] The user confirms: 'Yes, the appointment is made.' Simultaneously, the rhythmic tapping of the white cane on the sidewalk continues.",
    "question_type": "Temporal Information",
    "question": "During 00:08–00:11, does the cane tapping persist continuously?",
    "answer": "Yes, the rhythmic tapping continues throughout this interval.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01490.mp4",
    "question_id": "01490_6",
    "clip_path": "clips/01490/01490__0007500_0011500.mp4"
  },
  {
    "timestamp": "00:05 - 00:08",
    "context": "[00:05-00:08] The man and woman walk past the user on the left within 1–2 meters. The man's voice is briefly audible; the woman speaks clearly.",
    "question_type": "Counting",
    "question": "How many non-user speakers are audible as they pass the user?",
    "answer": "Two—the man and the woman.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01490.mp4",
    "question_id": "01490_7",
    "clip_path": "clips/01490/01490__0004500_0008500.mp4"
  },
  {
    "timestamp": "00:00 - 00:08",
    "context": "[00:00-00:05] The woman asks: 'Hey, are you from Northeast China?' [00:05-00:08] She asks: 'You've made the appointment, right, sis?'",
    "question_type": "Counting",
    "question": "How many questions does the woman ask between 00:00 and 00:08?",
    "answer": "Two questions.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01490.mp4",
    "question_id": "01490_8",
    "clip_path": "clips/01490/01490__0000000_0008500.mp4"
  },
  {
    "timestamp": "00:00 - 00:05",
    "context": "[00:00-00:05] The woman, approaching from about 3 meters ahead, asks: 'Hey, are you from Northeast China?' The user replies: 'You could even tell that? I went to university in Changchun.'",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the woman likely ask if the user was from Northeast China?",
    "answer": "She was likely inferring the user's regional background from their speech as she approached.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01490.mp4",
    "question_id": "01490_9",
    "clip_path": "clips/01490/01490__0000000_0005500.mp4"
  },
  {
    "timestamp": "00:05 - 00:08",
    "context": "[00:05-00:08] The woman asks the user: 'You've made the appointment, right, sis?' The caption notes this was to confirm their plans.",
    "question_type": "Inferential & Contextual Causality",
    "question": "What was the reason for the woman's question about the appointment?",
    "answer": "To confirm their plans and ensure the appointment had been made.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01490.mp4",
    "question_id": "01490_10",
    "clip_path": "clips/01490/01490__0004500_0008500.mp4"
  },
  {
    "timestamp": "00:08 - 00:11",
    "context": "[00:08-00:11] The user says, 'Yes, the appointment is made.' The user walks alongside a metal fence with signs indicating an entrance, corroborating the conversation about an appointment.",
    "question_type": "Cross-Modal Reasoning",
    "question": "How does the visual scene support the user's audio confirmation about the appointment?",
    "answer": "Signs indicating an entrance appear alongside the path, corroborating that they are near or approaching the appointment location.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01490.mp4",
    "question_id": "01490_11",
    "clip_path": "clips/01490/01490__0007500_0011500.mp4"
  },
  {
    "timestamp": "00:00 - 00:05",
    "context": "[00:00-00:05] The cane's tip strikes the tiled pavement in front of the user, producing rhythmic taps.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Relative to the camera, where do the cane taps originate?",
    "answer": "From the area directly in front of the user on the pavement.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01490.mp4",
    "question_id": "01490_12",
    "clip_path": "clips/01490/01490__0000000_0005500.mp4"
  },
  {
    "timestamp": "00:06 - 00:11",
    "context": "[00:06] The camera holder says, '我有残疾证可以吗?' (I have a disability certificate, is that okay?). [00:08] The person in black replies, '你用残疾证就要走前面啊' (If you use a disability certificate, you have to go to the front).",
    "question_type": "Inferential & Contextual Causality",
    "question": "Based on the preceding statement, why did the person in black instruct the camera holder to go to the front?",
    "answer": "Because the camera holder said they were using a disability certificate.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01491.mp4",
    "question_id": "01491_1",
    "clip_path": "clips/01491/01491__0005500_0011500.mp4"
  },
  {
    "timestamp": "00:11 - 00:17",
    "context": "[00:11] The camera holder asks, '行那我要拿证件吗?' (Okay, do I need to show the certificate?). [00:13] They then take out a red-covered document, creating soft rustling sounds.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the camera holder take out the red-covered document?",
    "answer": "To show the certificate after asking if it was needed.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01491.mp4",
    "question_id": "01491_2",
    "clip_path": "clips/01491/01491__0010500_0017500.mp4"
  },
  {
    "timestamp": "00:17 - 00:23",
    "context": "[00:17] While opening the red document (paper-like rustling), the person in black asks, '是在你手机里面吗?' (Is it on your phone?). Earlier, the camera holder said a friend helped make the appointment on the phone.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the person in black ask, 'Is it on your phone?'",
    "answer": "To clarify whether the appointment proof was on the phone, given the earlier mention and while the document was being opened.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01491.mp4",
    "question_id": "01491_3",
    "clip_path": "clips/01491/01491__0016500_0023333.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "[00:00] A person in black asks a question. Almost immediately, a loud, slightly distorted, pre-recorded female voice begins broadcasting from another megaphone nearby.",
    "question_type": "Sound Source Identification",
    "question": "What device generated the loud, slightly distorted female announcement voice?",
    "answer": "Another megaphone nearby that was broadcasting a pre-recorded announcement.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01491.mp4",
    "question_id": "01491_4",
    "clip_path": "clips/01491/01491__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:11 - 00:17",
    "context": "[00:13] The camera holder takes out a red-covered document, creating soft rustling sounds as they handle it.",
    "question_type": "Sound Source Identification",
    "question": "What produced the soft rustling sounds during 00:11–00:17?",
    "answer": "Handling the red-covered document.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01491.mp4",
    "question_id": "01491_5",
    "clip_path": "clips/01491/01491__0010500_0017500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "A pre-recorded female voice begins broadcasting from a megaphone nearby; it is described as loud and slightly distorted.",
    "question_type": "Sound Characteristics",
    "question": "What were the loudness and texture of the megaphone announcement voice when it began?",
    "answer": "It was loud and slightly distorted.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01491.mp4",
    "question_id": "01491_6",
    "clip_path": "clips/01491/01491__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:17 - 00:23",
    "context": "[00:17] While the red document is being opened, it makes a distinct paper-like rustling sound.",
    "question_type": "Sound Characteristics",
    "question": "How is the rustling sound characterized while the red document is being opened?",
    "answer": "A distinct, paper-like rustling.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01491.mp4",
    "question_id": "01491_7",
    "clip_path": "clips/01491/01491__0016500_0023333.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "An individual in a black shirt approaches from the front left and asks, '你们预约了吗?' (Have you made an appointment?).",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From where relative to the camera did the initial question originate?",
    "answer": "From the front left of the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01491.mp4",
    "question_id": "01491_8",
    "clip_path": "clips/01491/01491__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:11 - 00:17",
    "context": "Another person asks from the front, '你是谁的预约的?' (Whose appointment is it?).",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From which direction relative to the camera did the question 'Whose appointment is it?' come?",
    "answer": "From directly in front of the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01491.mp4",
    "question_id": "01491_9",
    "clip_path": "clips/01491/01491__0010500_0017500.mp4"
  },
  {
    "timestamp": "00:00 - 00:23",
    "context": "[00:00–00:06] The announcement begins almost immediately after the initial question. [00:06–00:23] The loud megaphone announcement continues in the background throughout the exchanges and remains audible.",
    "question_type": "Temporal Information",
    "question": "When did the megaphone announcement start, and did it persist through the scene?",
    "answer": "It started almost immediately after the initial question and continued throughout the interactions up to 00:23.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01491.mp4",
    "question_id": "01491_10",
    "clip_path": "clips/01491/01491__0000000_0023333.mp4"
  },
  {
    "timestamp": "00:11 - 00:23",
    "context": "[00:11–00:17] Soft rustling as the red document is taken out. [00:17–00:23] A distinct paper-like rustling while the document is opened.",
    "question_type": "Temporal Information",
    "question": "During which intervals did document-related rustling occur, and how did its character change?",
    "answer": "Soft rustling occurred while taking it out (00:11–00:17), then a distinct paper-like rustling while opening it (00:17–00:23).",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01491.mp4",
    "question_id": "01491_11",
    "clip_path": "clips/01491/01491__0010500_0023333.mp4"
  },
  {
    "timestamp": "00:00 - 00:23",
    "context": "Audible speakers include: the person in black, the camera holder, another person from the front, and a pre-recorded female voice on a megaphone.",
    "question_type": "Counting",
    "question": "How many distinct voices are audible across the 00:00–00:23 segment?",
    "answer": "Four: the person in black, the camera holder, another person, and the pre-recorded female announcer.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01491.mp4",
    "question_id": "01491_12",
    "clip_path": "clips/01491/01491__0000000_0023333.mp4"
  },
  {
    "timestamp": "00:06 - 00:23",
    "context": "[00:06–00:11] '你用残疾证就要走前面啊' (you have to go to the front). [00:17–00:23] '那你走前面吧。走前面绿色通道吧' (Then you go to the front. Go to the green channel at the front).",
    "question_type": "Counting",
    "question": "How many times is the instruction to go to the front given?",
    "answer": "Twice.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01491.mp4",
    "question_id": "01491_13",
    "clip_path": "clips/01491/01491__0005500_0023333.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "The camera holder starts to reply while raising their hand, which briefly obscures the camera view.",
    "question_type": "Cross-Modal Reasoning",
    "question": "As the camera holder begins replying, what visual effect occurs on the camera view?",
    "answer": "Their raised hand briefly obscures the camera view.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01491.mp4",
    "question_id": "01491_14",
    "clip_path": "clips/01491/01491__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:17 - 00:23",
    "context": "After being told, '走前面绿色通道吧' (Go to the green channel at the front), the camera holder, holding a white cane, proceeds to walk forward.",
    "question_type": "Cross-Modal Reasoning",
    "question": "Following the instruction to go to the green channel at the front, what action does the camera holder take?",
    "answer": "They walk forward while holding a white cane.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01491.mp4",
    "question_id": "01491_15",
    "clip_path": "clips/01491/01491__0016500_0023333.mp4"
  },
  {
    "timestamp": "00:00 - 00:10",
    "context": "[00:00] The camera holder presents a red disability certificate to a staff member holding a white and blue megaphone. A clear, close-range conversation ensues.",
    "question_type": "Cross-Modal Reasoning",
    "question": "After the camera holder presents the red disability certificate, what does the ensuing conversation aim to clarify?",
    "answer": "The entry procedure and the rule for using the disability certificate with a reservation.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01492.mp4",
    "question_id": "01492_1",
    "clip_path": "clips/01492/01492__0000000_0010500.mp4"
  },
  {
    "timestamp": "00:00 - 00:10",
    "context": "The camera holder says: \"My friend made the reservation for me... He said you can enter directly with the disability certificate, but I made a reservation in advance.\" The staff replies explaining: \"But since you made a reservation, if you use the disability certificate, only you can enter; you can't bring anyone else.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the staff member explain the rule about only the certificate holder being allowed to enter?",
    "answer": "Because the camera holder mentioned having a reservation while presenting the disability certificate, prompting the staff to clarify the entry condition.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01492.mp4",
    "question_id": "01492_2",
    "clip_path": "clips/01492/01492__0000000_0010500.mp4"
  },
  {
    "timestamp": "00:00 - 00:10",
    "context": "The staff member replies in a slightly louder, authoritative tone, explaining the rule.",
    "question_type": "Sound Source Identification",
    "question": "Who delivered the slightly louder, authoritative reply explaining the rule?",
    "answer": "The staff member holding the megaphone.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01492.mp4",
    "question_id": "01492_3",
    "clip_path": "clips/01492/01492__0000000_0010500.mp4"
  },
  {
    "timestamp": "00:00 - 00:10",
    "context": "The camera holder speaks at a normal conversational volume; the staff member replies in a slightly louder, authoritative tone.",
    "question_type": "Sound Characteristics",
    "question": "How did the volumes and tones of the two speakers differ?",
    "answer": "The camera holder spoke at a normal conversational volume, while the staff member replied slightly louder with an authoritative tone.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01492.mp4",
    "question_id": "01492_4",
    "clip_path": "clips/01492/01492__0000000_0010500.mp4"
  },
  {
    "timestamp": "00:00 - 00:10",
    "context": "A clear, close-range conversation ensues between the camera holder and the staff member.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the conversation originate relative to the camera?",
    "answer": "From close range near the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01492.mp4",
    "question_id": "01492_5",
    "clip_path": "clips/01492/01492__0000000_0010500.mp4"
  },
  {
    "timestamp": "00:08 - 00:10",
    "context": "After the staff explains the rule, there is a brief pause, then the camera holder says: \"Oh, okay, then I'll just use the disability certificate to enter by myself.\"",
    "question_type": "Temporal Information",
    "question": "When did the brief pause occur in relation to the exchange?",
    "answer": "After the staff member’s explanation and just before the camera holder’s concession.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01492.mp4",
    "question_id": "01492_6",
    "clip_path": "clips/01492/01492__0007500_0010500.mp4"
  },
  {
    "timestamp": "00:00 - 00:10",
    "context": "Sequence: camera holder explains; staff replies with the rule; camera holder agrees to the condition.",
    "question_type": "Counting",
    "question": "How many distinct turns of speech are described in this exchange?",
    "answer": "Three turns: the camera holder’s explanation, the staff member’s reply, and the camera holder’s agreement.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01492.mp4",
    "question_id": "01492_7",
    "clip_path": "clips/01492/01492__0000000_0010500.mp4"
  },
  {
    "timestamp": "00:00 - 00:10",
    "context": "The camera holder first explains the situation and later agrees to enter alone.",
    "question_type": "Counting",
    "question": "How many times does the camera holder speak in this segment?",
    "answer": "Twice.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01492.mp4",
    "question_id": "01492_8",
    "clip_path": "clips/01492/01492__0000000_0010500.mp4"
  },
  {
    "timestamp": "00:00 - 00:10",
    "context": "A clear, close-range conversation occurs during the entire described interval.",
    "question_type": "Temporal Information",
    "question": "During what time window does the clear, close-range conversation take place?",
    "answer": "From 00:00 to 00:10.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01492.mp4",
    "question_id": "01492_9",
    "clip_path": "clips/01492/01492__0000000_0010500.mp4"
  },
  {
    "timestamp": "00:09 - 00:10",
    "context": "After hearing the rule, the camera holder concedes: \"Oh, okay, then I'll just use the disability certificate to enter by myself.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "What was the likely reason the camera holder agreed to enter alone using the disability certificate?",
    "answer": "Because the staff member’s rule stated that if using the disability certificate with a reservation, only the certificate holder can enter.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01492.mp4",
    "question_id": "01492_10",
    "clip_path": "clips/01492/01492__0008500_0010500.mp4"
  },
  {
    "timestamp": "00:00 - 00:05",
    "context": "[00:00 - 00:05] A person, holding a white mobility cane, sits in the back of a car and speaks to the driver in a clear, moderate voice, providing specific drop-off instructions. The purpose is to ensure she is left at a precise location for easy navigation. She asks, \"Sir, in a moment, can you drop me off right at the store's entrance?\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the passenger provide specific drop-off instructions at the start?",
    "answer": "To ensure she is left at a precise location for easy navigation.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01496.mp4",
    "question_id": "01496_1",
    "clip_path": "clips/01496/01496__0000000_0005500.mp4"
  },
  {
    "timestamp": "00:00 - 00:05",
    "context": "[00:00 - 00:05] A person, holding a white mobility cane, sits in the back of a car and speaks to the driver in a clear, moderate voice, providing specific drop-off instructions.",
    "question_type": "Sound Characteristics",
    "question": "What was the clarity and volume of the passenger's voice when giving the instructions?",
    "answer": "Her voice was clear and moderate.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01496.mp4",
    "question_id": "01496_2",
    "clip_path": "clips/01496/01496__0000000_0005500.mp4"
  },
  {
    "timestamp": "00:05 - 00:06",
    "context": "[00:05 - 00:06] The male driver, audible from the front of the car, responds immediately and agreeably, saying in a calm tone, \"Ah, okay, okay, okay.\"",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From which direction in the car did the driver's response originate?",
    "answer": "From the front of the car.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01496.mp4",
    "question_id": "01496_3",
    "clip_path": "clips/01496/01496__0004500_0006500.mp4"
  },
  {
    "timestamp": "00:05 - 00:06",
    "context": "[00:05 - 00:06] The male driver responds, saying, \"Ah, okay, okay, okay.\"",
    "question_type": "Counting",
    "question": "How many times did the driver say the word \"okay\" in his response?",
    "answer": "Three times.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01496.mp4",
    "question_id": "01496_4",
    "clip_path": "clips/01496/01496__0004500_0006500.mp4"
  },
  {
    "timestamp": "00:05 - 00:06",
    "context": "[00:05 - 00:06] The male driver responds immediately and agreeably to the passenger's request.",
    "question_type": "Temporal Information",
    "question": "Was the driver's response immediate or delayed?",
    "answer": "Immediate.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01496.mp4",
    "question_id": "01496_5",
    "clip_path": "clips/01496/01496__0004500_0006500.mp4"
  },
  {
    "timestamp": "00:06 - 00:15",
    "context": "[00:06 - 00:15] The passenger explains the reason for her cautious instructions and her choice of a premium ride service by recounting a recent negative encounter: \"Let me tell you, I just encountered a driver. He was very aggressive... I was scared to death, and then I called a private car.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "What motivated the passenger to call a private car?",
    "answer": "A prior encounter with a very aggressive driver that scared her.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01496.mp4",
    "question_id": "01496_6",
    "clip_path": "clips/01496/01496__0005500_0015500.mp4"
  },
  {
    "timestamp": "00:16 - 00:17",
    "context": "[00:16 - 00:17] A short, high-pitched electronic notification chime emanates from the passenger's smartphone, which she is holding.",
    "question_type": "Sound Source Identification",
    "question": "What device generated the notification chime heard at 00:16–00:17?",
    "answer": "The passenger's smartphone she was holding.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01496.mp4",
    "question_id": "01496_7",
    "clip_path": "clips/01496/01496__0015500_0017500.mp4"
  },
  {
    "timestamp": "00:16 - 00:17",
    "context": "[00:16 - 00:17] A short, high-pitched electronic notification chime emanates from the passenger's smartphone.",
    "question_type": "Temporal Information",
    "question": "When did the notification chime occur and was it brief or prolonged?",
    "answer": "It occurred between 00:16 and 00:17 and was brief.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01496.mp4",
    "question_id": "01496_8",
    "clip_path": "clips/01496/01496__0015500_0017500.mp4"
  },
  {
    "timestamp": "00:00 - 00:05",
    "context": "[00:00 - 00:05] The passenger holds a white mobility cane while requesting to be dropped at the store's entrance; the stated purpose is precise drop-off for easy navigation.",
    "question_type": "Cross-Modal Reasoning",
    "question": "How does the visual detail of the passenger holding a white mobility cane help explain her request to be dropped right at the store's entrance?",
    "answer": "The cane underscores her need for a precise drop-off to aid navigation, aligning with her request to be left at an exact location.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01496.mp4",
    "question_id": "01496_9",
    "clip_path": "clips/01496/01496__0000000_0005500.mp4"
  },
  {
    "timestamp": "00:18 - 00:21",
    "context": "[00:18 - 00:21] The passenger acknowledges with a soft \"Mmm\" and then says, \"Thank you.\"",
    "question_type": "Sound Source Identification",
    "question": "Who produced the soft \"Mmm\" and the \"Thank you\" at the end?",
    "answer": "The passenger.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01496.mp4",
    "question_id": "01496_10",
    "clip_path": "clips/01496/01496__0017500_0021500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "The user asks, \"Hey, miss... let me ask you, where is the restroom? Roughly which direction should I go?\" The other person replies, \"Just go straight ahead from where you are.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the respondent say, \"Just go straight ahead from where you are\"?",
    "answer": "To give the user directions to the restroom after being asked.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01506.mp4",
    "question_id": "01506_1",
    "clip_path": "clips/01506/01506__0000000_0006833.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "The user's voice is heard from the camera's position, while the respondent's voice comes from the immediate front.",
    "question_type": "Sound Source Identification",
    "question": "What produced the voice heard from the camera's position?",
    "answer": "The camera holder (the user).",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01506.mp4",
    "question_id": "01506_2",
    "clip_path": "clips/01506/01506__0000000_0006833.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "The camera holder initiates a conversation at a normal, clear volume.",
    "question_type": "Sound Characteristics",
    "question": "How is the user's speaking volume described when initiating the conversation?",
    "answer": "Normal and clear.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01506.mp4",
    "question_id": "01506_3",
    "clip_path": "clips/01506/01506__0000000_0006833.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "The respondent's voice comes from the immediate front.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the respondent's voice originate relative to the camera?",
    "answer": "From the immediate front.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01506.mp4",
    "question_id": "01506_4",
    "clip_path": "clips/01506/01506__0000000_0006833.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "During this interval, the user asks for restroom directions and receives a reply.",
    "question_type": "Temporal Information",
    "question": "During what time interval does the question-and-reply exchange occur?",
    "answer": "Between 00:00 and 00:07.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01506.mp4",
    "question_id": "01506_5",
    "clip_path": "clips/01506/01506__0000000_0006833.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "The user's voice is heard from the camera's position, while the respondent's voice comes from the immediate front.",
    "question_type": "Counting",
    "question": "How many distinct speakers are heard in the exchange?",
    "answer": "Two.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01506.mp4",
    "question_id": "01506_6",
    "clip_path": "clips/01506/01506__0000000_0006833.mp4"
  },
  {
    "timestamp": "00:00 - 00:08",
    "context": "The user says in a slightly anxious tone, \"Are you sure? I can't remember the way now.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the user initiate the conversation at the start of the clip?",
    "answer": "Because they were concerned about their orientation and couldn't remember the way.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01510.mp4",
    "question_id": "01510_1",
    "clip_path": "clips/01510/01510__0000000_0008500.mp4"
  },
  {
    "timestamp": "00:00 - 00:08",
    "context": "As they walk on the polished marble floor, the cane produces a series of light, rhythmic tapping sounds.",
    "question_type": "Sound Source Identification",
    "question": "What generated the light, rhythmic tapping sounds?",
    "answer": "The user's white cane contacting the polished marble floor.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01510.mp4",
    "question_id": "01510_2",
    "clip_path": "clips/01510/01510__0000000_0008500.mp4"
  },
  {
    "timestamp": "00:00 - 00:08",
    "context": "The cane produces a series of light, rhythmic tapping sounds.",
    "question_type": "Sound Characteristics",
    "question": "How are the cane tapping sounds described?",
    "answer": "They are light and rhythmic.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01510.mp4",
    "question_id": "01510_3",
    "clip_path": "clips/01510/01510__0000000_0008500.mp4"
  },
  {
    "timestamp": "00:00 - 00:08",
    "context": "The user speaks in a slightly anxious tone.",
    "question_type": "Sound Characteristics",
    "question": "What was the user's vocal tone when asking for confirmation?",
    "answer": "Slightly anxious.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01510.mp4",
    "question_id": "01510_4",
    "clip_path": "clips/01510/01510__0000000_0008500.mp4"
  },
  {
    "timestamp": "00:00 - 00:08",
    "context": "Their companion reassures them from a close distance in front, saying, \"It's fine, no need to remember. I'll remember.\"",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From where relative to the camera did the companion's reassurance originate?",
    "answer": "From a close distance directly in front.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01510.mp4",
    "question_id": "01510_5",
    "clip_path": "clips/01510/01510__0000000_0008500.mp4"
  },
  {
    "timestamp": "00:00 - 00:08",
    "context": "As they walk on the polished marble floor, the cane produces a series of light, rhythmic tapping sounds.",
    "question_type": "Temporal Information",
    "question": "Did the cane tapping occur briefly or throughout the walking segment?",
    "answer": "It occurred as a series of taps throughout the walking segment.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01510.mp4",
    "question_id": "01510_6",
    "clip_path": "clips/01510/01510__0000000_0008500.mp4"
  },
  {
    "timestamp": "00:00 - 00:08",
    "context": "The user asks, \"Are you sure?\" and later asks, \"Can you remember the way?\"",
    "question_type": "Counting",
    "question": "How many questions did the user ask during this exchange?",
    "answer": "Two.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01510.mp4",
    "question_id": "01510_7",
    "clip_path": "clips/01510/01510__0000000_0008500.mp4"
  },
  {
    "timestamp": "00:00 - 00:08",
    "context": "The user speaks first, and their companion replies with reassurance.",
    "question_type": "Counting",
    "question": "How many distinct speakers are involved in the conversation?",
    "answer": "Two: the user and their companion.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01510.mp4",
    "question_id": "01510_8",
    "clip_path": "clips/01510/01510__0000000_0008500.mp4"
  },
  {
    "timestamp": "00:00 - 00:08",
    "context": "They walk past other shoppers and approach an escalator while the companion says, \"It's fine, no need to remember. I'll remember,\" and the user adds, \"I can't remember it at all.\"",
    "question_type": "Cross-Modal Reasoning",
    "question": "As they approach the escalator, what does the audio dialogue indicate about their navigation strategy?",
    "answer": "The companion will remember the route, and the user is relying completely on their guide for orientation.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01510.mp4",
    "question_id": "01510_9",
    "clip_path": "clips/01510/01510__0000000_0008500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "[00:00] As the elevator doors slide open with a faint metallic scraping sound, revealing a subway platform.",
    "question_type": "Sound Source Identification",
    "question": "What generated the faint metallic scraping sound at the start?",
    "answer": "The elevator doors sliding open.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01515.mp4",
    "question_id": "01515_1",
    "clip_path": "clips/01515/01515__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "[00:00] As the elevator doors slide open with a faint metallic scraping sound.",
    "question_type": "Sound Characteristics",
    "question": "What was the volume and quality of the sound made by the elevator doors opening?",
    "answer": "It was a faint, metallic scraping sound.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01515.mp4",
    "question_id": "01515_2",
    "clip_path": "clips/01515/01515__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "[00:00 - 00:07] The elevator doors slide open with a faint metallic scraping sound.",
    "question_type": "Temporal Information",
    "question": "When did the metallic scraping sound occur and how long did it last?",
    "answer": "It occurred as the doors were opening, during 00:00–00:07.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01515.mp4",
    "question_id": "01515_3",
    "clip_path": "clips/01515/01515__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "[00:00 - 00:07] A male subway staff member in a grey uniform and face mask, located directly in front, responds and steps forward to assist.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where was the subway staff member located relative to the camera when he first responded?",
    "answer": "Directly in front of the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01515.mp4",
    "question_id": "01515_4",
    "clip_path": "clips/01515/01515__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "[00:00] A male voice from the camera's perspective says, \"Come and help me.\" [00:00 - 00:07] The staff member immediately responds and steps forward to assist.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the staff member step forward to assist?",
    "answer": "Because the camera wearer requested help by saying, \"Come and help me.\"",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01515.mp4",
    "question_id": "01515_5",
    "clip_path": "clips/01515/01515__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "[00:00] A male voice from the camera's perspective says, \"Come and help me.\"",
    "question_type": "Sound Source Identification",
    "question": "Who said, \"Come and help me\"?",
    "answer": "The camera wearer (a male voice from the camera's perspective).",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01515.mp4",
    "question_id": "01515_6",
    "clip_path": "clips/01515/01515__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "[00:00 - 00:07] The camera wearer says, \"Thank you.\" The staff member asks, \"Where are you going?\" The camera wearer replies, \"Xinjiekou,\" and the staff member confirms the destination.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the staff member ask, \"Where are you going?\"",
    "answer": "He needed to know the destination (Xinjiekou) to guide the camera wearer appropriately.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01515.mp4",
    "question_id": "01515_7",
    "clip_path": "clips/01515/01515__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:17 - 00:20",
    "context": "[00:17 - 00:20] While standing and waiting for the train, the staff member informs, \"Wait a bit more, the train isn't here yet.\" The camera wearer replies, \"Mm, okay, okay.\"",
    "question_type": "Temporal Information",
    "question": "When did the staff member inform the camera wearer that the train wasn't there yet?",
    "answer": "Between 00:17 and 00:20.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01515.mp4",
    "question_id": "01515_8",
    "clip_path": "clips/01515/01515__0016500_0019867.mp4"
  },
  {
    "timestamp": "00:17 - 00:20",
    "context": "[00:17 - 00:20] The staff member, positioned to the left, tells the camera wearer, \"Wait a bit more, the train isn't here yet.\"",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From which direction relative to the camera did the staff member speak when saying to wait?",
    "answer": "From the left side of the camera wearer.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01515.mp4",
    "question_id": "01515_9",
    "clip_path": "clips/01515/01515__0016500_0019867.mp4"
  },
  {
    "timestamp": "00:00 - 00:15",
    "context": "[00:00 - 00:07] The camera wearer says, \"Thank you.\" [00:07 - 00:15] The camera wearer thanks the staff member again for the assistance.",
    "question_type": "Counting",
    "question": "How many times did the camera wearer say \"Thank you\" during this segment?",
    "answer": "Twice.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01515.mp4",
    "question_id": "01515_10",
    "clip_path": "clips/01515/01515__0000000_0015500.mp4"
  },
  {
    "timestamp": "00:00 - 00:08",
    "context": "[00:00 - 00:03] User asks, \"Which station are we going to?\" A nearby male companion replies, \"To Xinjiekou.\" [00:03 - 00:08] A second staff member says, \"I will have the staff at Xinjiekou come pick you up,\" explaining she is arranging assistance at their destination.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the second staff member say she would have staff at Xinjiekou come pick them up?",
    "answer": "She was arranging assistance at their destination, Xinjiekou.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01520.mp4",
    "question_id": "01520_1",
    "clip_path": "clips/01520/01520__0000000_0008500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "The user asks, \"Which station are we going to?\" A male companion nearby replies, \"To Xinjiekou.\"",
    "question_type": "Sound Source Identification",
    "question": "Who said, \"To Xinjiekou\"?",
    "answer": "The nearby male companion.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01520.mp4",
    "question_id": "01520_2",
    "clip_path": "clips/01520/01520__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "The user asks in a clear, close-range female voice, \"Which station are we going to?\"",
    "question_type": "Sound Characteristics",
    "question": "What was the quality of the user's voice when asking about the destination?",
    "answer": "A clear, close-range female voice.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01520.mp4",
    "question_id": "01520_3",
    "clip_path": "clips/01520/01520__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:03 - 00:08",
    "context": "A second staff member steps forward and speaks in a clear, authoritative tone from the front.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From which direction relative to the camera did the second staff member speak?",
    "answer": "From the front.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01520.mp4",
    "question_id": "01520_4",
    "clip_path": "clips/01520/01520__0002500_0008500.mp4"
  },
  {
    "timestamp": "00:10 - 00:14",
    "context": "The first staff member gives instructions: \"Ma'am, you hold the man, and I'll hold you to take the elevator, okay?\"",
    "question_type": "Temporal Information",
    "question": "When did the instruction about taking the elevator occur?",
    "answer": "Between 00:10 and 00:14.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01520.mp4",
    "question_id": "01520_5",
    "clip_path": "clips/01520/01520__0009500_0014500.mp4"
  },
  {
    "timestamp": "00:10 - 00:14",
    "context": "The first staff member in the white shirt gives gentle, close-range instructions with a calm and helpful voice.",
    "question_type": "Sound Characteristics",
    "question": "How would you describe the tone and delivery of the first staff member’s instructions?",
    "answer": "Gentle, close-range, with a calm and helpful tone.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01520.mp4",
    "question_id": "01520_6",
    "clip_path": "clips/01520/01520__0009500_0014500.mp4"
  },
  {
    "timestamp": "00:19 - 00:23",
    "context": "Continuing to walk, the staff member provides guidance from directly in front: \"Come, hold me like this.\"",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the guiding voice originate relative to the camera during 00:19 - 00:23?",
    "answer": "Directly in front of the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01520.mp4",
    "question_id": "01520_7",
    "clip_path": "clips/01520/01520__0018500_0023500.mp4"
  },
  {
    "timestamp": "00:00 - 00:03",
    "context": "The user confirms, \"Xinjiekou, right?\" and the male companion affirms with, \"Yes, yes, yes.\"",
    "question_type": "Counting",
    "question": "How many times did the male companion say \"yes\"?",
    "answer": "Three times.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01520.mp4",
    "question_id": "01520_8",
    "clip_path": "clips/01520/01520__0000000_0003500.mp4"
  },
  {
    "timestamp": "00:15 - 00:18",
    "context": "As they begin to walk, the staff member instructs: \"Come on... hold tight. Slower, a bit slower, let's go slowly.\"",
    "question_type": "Temporal Information",
    "question": "When did they begin walking and how long did this instruction segment last?",
    "answer": "They began walking at 00:15, and the instruction segment lasted about 3 seconds until 00:18.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01520.mp4",
    "question_id": "01520_9",
    "clip_path": "clips/01520/01520__0014500_0018500.mp4"
  },
  {
    "timestamp": "00:19 - 00:23",
    "context": "The staff member addresses the male companion: \"Sir, slow down.\"",
    "question_type": "Sound Source Identification",
    "question": "Who told the male companion, \"Sir, slow down\"?",
    "answer": "The guiding female staff member in front.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01520.mp4",
    "question_id": "01520_10",
    "clip_path": "clips/01520/01520__0018500_0023500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "[00:00 - 00:07] A high-pitched, melodic public announcement chime sounds clearly in the background as the user is guided through a subway station.",
    "question_type": "Sound Source Identification",
    "question": "What produced the high-pitched, melodic chime heard between 00:00 and 00:07?",
    "answer": "A public announcement chime in the subway station.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01521.mp4",
    "question_id": "01521_1",
    "clip_path": "clips/01521/01521__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "[00:00 - 00:07] The low, continuous rumbling sound of a rolling suitcase, being pulled by a man walking directly ahead, is audible on the smooth floor.",
    "question_type": "Sound Characteristics",
    "question": "How is the rolling suitcase's sound described?",
    "answer": "A low, continuous rumbling on the smooth floor.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01521.mp4",
    "question_id": "01521_2",
    "clip_path": "clips/01521/01521__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "[00:00 - 00:07] The rolling suitcase is being pulled by a man walking directly ahead.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From what direction relative to the camera did the suitcase rumbling originate?",
    "answer": "From directly ahead, where the man pulling it was walking.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01521.mp4",
    "question_id": "01521_3",
    "clip_path": "clips/01521/01521__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:07 - 00:09",
    "context": "[00:07 - 00:09] Upon reaching the elevator area, the rumbling sound of the suitcase abruptly stops as the group comes to a halt.",
    "question_type": "Temporal Information",
    "question": "When did the suitcase rumbling stop, and how abrupt was it?",
    "answer": "It stopped abruptly between 00:07 and 00:09.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01521.mp4",
    "question_id": "01521_4",
    "clip_path": "clips/01521/01521__0006500_0009500.mp4"
  },
  {
    "timestamp": "00:07 - 00:09",
    "context": "[00:07 - 00:09] Upon reaching the elevator area, the rumbling sound of the suitcase abruptly stops as the group comes to a halt.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the suitcase's rumbling stop at the elevator area?",
    "answer": "Because the group reached the elevator and came to a halt.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01521.mp4",
    "question_id": "01521_5",
    "clip_path": "clips/01521/01521__0006500_0009500.mp4"
  },
  {
    "timestamp": "00:09 - 00:15",
    "context": "[00:09 - 00:15] The female staff member turns to face the user and provides instructions in a clear, mid-volume voice.",
    "question_type": "Sound Source Identification",
    "question": "Who provided the instructions heard between 00:09 and 00:15?",
    "answer": "A female staff member in a uniform.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01521.mp4",
    "question_id": "01521_6",
    "clip_path": "clips/01521/01521__0008500_0015500.mp4"
  },
  {
    "timestamp": "00:09 - 00:15",
    "context": "[00:09 - 00:15] She provides instructions in a clear, mid-volume voice.",
    "question_type": "Sound Characteristics",
    "question": "What were the volume and clarity of the staff member's voice?",
    "answer": "Clear and mid-volume.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01521.mp4",
    "question_id": "01521_7",
    "clip_path": "clips/01521/01521__0008500_0015500.mp4"
  },
  {
    "timestamp": "00:09 - 00:15",
    "context": "[00:09 - 00:15] The user or their companion replies from nearby, 'Okay, thank you.'",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the reply 'Okay, thank you' originate relative to the camera?",
    "answer": "From nearby, indicating close proximity.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01521.mp4",
    "question_id": "01521_8",
    "clip_path": "clips/01521/01521__0008500_0015500.mp4"
  },
  {
    "timestamp": "00:09 - 00:20",
    "context": "[00:09 - 00:15] Staff member gives instructions; the user or their companion replies 'Okay, thank you.' [00:15 - 00:20] The companion says 'Thank you,' and the staff member responds kindly with further guidance.",
    "question_type": "Counting",
    "question": "How many distinct speech turns occur between 00:09 and 00:20?",
    "answer": "Four speech turns: the staff member's instruction, the user's or companion's reply, the companion's additional thanks, and the staff member's response.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01521.mp4",
    "question_id": "01521_9",
    "clip_path": "clips/01521/01521__0008500_0020033.mp4"
  },
  {
    "timestamp": "00:15 - 00:20",
    "context": "[00:15 - 00:20] The staff member says, 'It's alright, I'll press the button for you. You can just sit down. Just sit down. Okay.' This suggests the user is in a wheelchair and is being assisted with their transit.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why does the staff member say she will press the button and tell the user to sit down?",
    "answer": "She is assisting the user—likely in a wheelchair—so they can remain seated while she handles the elevator button.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01521.mp4",
    "question_id": "01521_10",
    "clip_path": "clips/01521/01521__0014500_0020033.mp4"
  },
  {
    "timestamp": "00:01 - 00:08",
    "context": "As the user approaches a building entrance, their white stick produces a series of light, rhythmic taps on the outdoor pavement, accompanied by background pop music from the nearby coffee shop.",
    "question_type": "Sound Source Identification",
    "question": "What generated the light, rhythmic tapping heard as the user approached the entrance?",
    "answer": "The user's white stick contacting the outdoor pavement.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01522.mp4",
    "question_id": "01522_1",
    "clip_path": "clips/01522/01522__0000500_0008500.mp4"
  },
  {
    "timestamp": "00:01 - 00:08",
    "context": "Their white stick produces a series of light, rhythmic taps on the outdoor pavement.",
    "question_type": "Sound Characteristics",
    "question": "What is the quality of the stick's tapping on the outdoor pavement?",
    "answer": "Light and rhythmic.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01522.mp4",
    "question_id": "01522_2",
    "clip_path": "clips/01522/01522__0000500_0008500.mp4"
  },
  {
    "timestamp": "00:01 - 00:08",
    "context": "Accompanied by background pop music from the nearby coffee shop.",
    "question_type": "Sound Source Identification",
    "question": "What is the source of the background pop music at the beginning?",
    "answer": "A nearby coffee shop.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01522.mp4",
    "question_id": "01522_3",
    "clip_path": "clips/01522/01522__0000500_0008500.mp4"
  },
  {
    "timestamp": "00:08 - 00:14",
    "context": "The user mistakenly enters a long, tiled corridor. The stick's tapping sound becomes sharper and more echoey, clearly resonating in the enclosed space as they walk forward.",
    "question_type": "Sound Characteristics",
    "question": "How did the stick's tapping change inside the tiled corridor?",
    "answer": "It became sharper and more echoey, resonating in the enclosed space.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01522.mp4",
    "question_id": "01522_4",
    "clip_path": "clips/01522/01522__0007500_0014500.mp4"
  },
  {
    "timestamp": "00:08 - 00:14",
    "context": "The user mistakenly enters a long, tiled corridor. The stick's tapping sound becomes sharper and more echoey.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the tapping sound become sharper and more echoey during this interval?",
    "answer": "Because the user entered a long, tiled corridor, an enclosed space that caused the sound to resonate.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01522.mp4",
    "question_id": "01522_5",
    "clip_path": "clips/01522/01522__0007500_0014500.mp4"
  },
  {
    "timestamp": "00:08 - 00:14",
    "context": "A brief, metallic scraping sound is audible as they adjust the length of the stick.",
    "question_type": "Sound Source Identification",
    "question": "What produced the brief, metallic scraping sound?",
    "answer": "Adjusting the length of the stick.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01522.mp4",
    "question_id": "01522_6",
    "clip_path": "clips/01522/01522__0007500_0014500.mp4"
  },
  {
    "timestamp": "00:14 - 00:18",
    "context": "The user stops abruptly, causing the tapping to cease. Realizing they are in the wrong place, they speak to themselves in a confused tone.",
    "question_type": "Inferential & Contextual Causality",
    "question": "What caused the tapping to cease at this point?",
    "answer": "The user stopped abruptly.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01522.mp4",
    "question_id": "01522_7",
    "clip_path": "clips/01522/01522__0013500_0018500.mp4"
  },
  {
    "timestamp": "00:14 - 00:18",
    "context": "They speak to themselves in a confused tone: '我走错了吗? 啊这里不是咖啡店'.",
    "question_type": "Sound Characteristics",
    "question": "What was the tone of the user's self-talk?",
    "answer": "Confused.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01522.mp4",
    "question_id": "01522_8",
    "clip_path": "clips/01522/01522__0013500_0018500.mp4"
  },
  {
    "timestamp": "00:18 - 00:21",
    "context": "The user turns around to exit the corridor, and the sharp, rhythmic tapping of the stick on the tiled floor resumes.",
    "question_type": "Temporal Information",
    "question": "When did the sharp, rhythmic tapping resume?",
    "answer": "Between 00:18 and 00:21, as the user turned around to exit the corridor.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01522.mp4",
    "question_id": "01522_9",
    "clip_path": "clips/01522/01522__0017500_0021500.mp4"
  },
  {
    "timestamp": "00:21 - 00:25",
    "context": "An automated, pre-recorded female voice from a speaker is heard saying, '欢迎选购' (Welcome, please make a selection).",
    "question_type": "Sound Source Identification",
    "question": "What delivered the '欢迎选购' message outside the coffee shop?",
    "answer": "An automated, pre-recorded female voice from a speaker.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01522.mp4",
    "question_id": "01522_10",
    "clip_path": "clips/01522/01522__0020500_0025500.mp4"
  },
  {
    "timestamp": "00:21 - 00:25",
    "context": "The user audibly confirms they are in the right place by saying, '啊对对对, 我听到了' (Ah, right, right, right, I heard it).",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the user say 'Ah, right, right, right, I heard it'?",
    "answer": "Hearing the automated welcome message confirmed they were at the coffee shop.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01522.mp4",
    "question_id": "01522_11",
    "clip_path": "clips/01522/01522__0020500_0025500.mp4"
  },
  {
    "timestamp": "00:21 - 00:41",
    "context": "[00:21 - 00:25] An automated '欢迎选购' message plays. [00:36 - 00:41] Opening the door triggers an electronic chime and another playback of '欢迎选购'.",
    "question_type": "Counting",
    "question": "How many times was the automated '欢迎选购' message heard between 00:21 and 00:41?",
    "answer": "Twice—once outside the shop and again when the door was opened.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01522.mp4",
    "question_id": "01522_12",
    "clip_path": "clips/01522/01522__0020500_0041500.mp4"
  },
  {
    "timestamp": "00:29 - 00:36",
    "context": "The user probes the wooden door frame with their stick, creating several soft, dull thuds.",
    "question_type": "Counting",
    "question": "Did the probing of the door frame produce a single thud or several?",
    "answer": "Several soft, dull thuds.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01522.mp4",
    "question_id": "01522_13",
    "clip_path": "clips/01522/01522__0028500_0036500.mp4"
  },
  {
    "timestamp": "00:29 - 00:36",
    "context": "The user probes the wooden door frame with their stick, creating several soft, dull thuds.",
    "question_type": "Sound Source Identification",
    "question": "What caused the soft, dull thuds near the door?",
    "answer": "The stick striking the wooden door frame as the user probed it.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01522.mp4",
    "question_id": "01522_14",
    "clip_path": "clips/01522/01522__0028500_0036500.mp4"
  },
  {
    "timestamp": "00:36 - 00:41",
    "context": "A man inside the shop opens the door, which triggers an electronic chime and another playback of the automated '欢迎选购' message.",
    "question_type": "Inferential & Contextual Causality",
    "question": "What action triggered the electronic chime and the second '欢迎选购' playback?",
    "answer": "The man opening the door.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01522.mp4",
    "question_id": "01522_15",
    "clip_path": "clips/01522/01522__0035500_0041500.mp4"
  },
  {
    "timestamp": "00:36 - 00:41",
    "context": "The user expresses their gratitude by saying, '谢谢啊' (Thank you), as the man holds the door open for them to enter.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the user say '谢谢啊' (Thank you)?",
    "answer": "Because the man held the door open for them.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01522.mp4",
    "question_id": "01522_16",
    "clip_path": "clips/01522/01522__0035500_0041500.mp4"
  },
  {
    "timestamp": "00:41 - 00:46",
    "context": "Now inside the coffee shop, the user walks across a wooden floor. The sound of the stick tapping becomes a deeper, more resonant thud.",
    "question_type": "Sound Characteristics",
    "question": "What are the acoustic qualities of the stick taps on the wooden floor inside the shop?",
    "answer": "A deeper, more resonant thud.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01522.mp4",
    "question_id": "01522_17",
    "clip_path": "clips/01522/01522__0040500_0046500.mp4"
  },
  {
    "timestamp": "00:41 - 00:46",
    "context": "The ambient noise of the shop, including faint music and distant customer chatter, becomes the primary background sound.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "What is the relative distance of the customer chatter inside the shop?",
    "answer": "It is distant.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01522.mp4",
    "question_id": "01522_18",
    "clip_path": "clips/01522/01522__0040500_0046500.mp4"
  },
  {
    "timestamp": "00:00 - 00:05",
    "context": "The cashier, from the front and approximately 1 meter away, says, '好, 谢谢你啊' (Okay, thank you).",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From what direction and approximate distance did the cashier's speech originate?",
    "answer": "From the front, about 1 meter away.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01524.mp4",
    "question_id": "01524_1",
    "clip_path": "clips/01524/01524__0000000_0005500.mp4"
  },
  {
    "timestamp": "00:00 - 00:05",
    "context": "The user holds up their phone to make a payment and says, '我来扫一下码' (Let me scan the code), indicating intent to use a mobile payment method.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the user say '我来扫一下码' at the checkout?",
    "answer": "To indicate they intended to pay by scanning a code with their phone (using mobile payment).",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01524.mp4",
    "question_id": "01524_2",
    "clip_path": "clips/01524/01524__0000000_0005500.mp4"
  },
  {
    "timestamp": "00:09 - 00:15",
    "context": "A continuous, low-volume whirring sound emanates from the receipt printer at the checkout counter in front of the user.",
    "question_type": "Temporal Information",
    "question": "Was the receipt printer's whirring brief or continuous during 00:09–00:15?",
    "answer": "It was continuous over that interval.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01524.mp4",
    "question_id": "01524_3",
    "clip_path": "clips/01524/01524__0008500_0015500.mp4"
  },
  {
    "timestamp": "00:09 - 00:15",
    "context": "A continuous, low-volume whirring sound emanates from the receipt printer at the checkout counter in front of the user.",
    "question_type": "Sound Characteristics",
    "question": "What are the volume and texture of the receipt printer's sound?",
    "answer": "A low-volume whirring sound.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01524.mp4",
    "question_id": "01524_4",
    "clip_path": "clips/01524/01524__0008500_0015500.mp4"
  },
  {
    "timestamp": "00:09 - 00:15",
    "context": "A continuous, low-volume whirring sound emanates from the receipt printer at the checkout counter in front of the user.",
    "question_type": "Sound Source Identification",
    "question": "What generated the whirring sound heard between 00:09 and 00:15?",
    "answer": "The receipt printer at the checkout counter in front of the user.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01524.mp4",
    "question_id": "01524_5",
    "clip_path": "clips/01524/01524__0008500_0015500.mp4"
  },
  {
    "timestamp": "00:16 - 00:18",
    "context": "As the payment is being processed, the user tells the cashier, '那你来点一下吧' (Then you can go ahead and tap it).",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the user ask the cashier to 'tap it' while the payment was processing?",
    "answer": "Because the transaction required the cashier to perform a confirmation/tap to proceed.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01524.mp4",
    "question_id": "01524_6",
    "clip_path": "clips/01524/01524__0015500_0018500.mp4"
  },
  {
    "timestamp": "00:25 - 00:26",
    "context": "After a short pause, the user asks, '好了吗?' (Is it done?), seeking confirmation that the payment has gone through.",
    "question_type": "Inferential & Contextual Causality",
    "question": "What prompted the user to ask '好了吗?' at 00:25–00:26?",
    "answer": "They were seeking confirmation that the payment had completed after a brief pause.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01524.mp4",
    "question_id": "01524_7",
    "clip_path": "clips/01524/01524__0024500_0026500.mp4"
  },
  {
    "timestamp": "00:28 - 00:29",
    "context": "The cashier says '好' (Okay), accompanied by the sharp, brief sound of a receipt being torn from the printer.",
    "question_type": "Sound Source Identification",
    "question": "What produced the sharp, brief sound at 00:28–00:29?",
    "answer": "A receipt being torn from the printer.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01524.mp4",
    "question_id": "01524_8",
    "clip_path": "clips/01524/01524__0027500_0029500.mp4"
  },
  {
    "timestamp": "00:28 - 00:29",
    "context": "A sharp, brief sound accompanies the cashier confirming the transaction.",
    "question_type": "Sound Characteristics",
    "question": "How would you describe the tearing sound at 00:28–00:29?",
    "answer": "Sharp and brief.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01524.mp4",
    "question_id": "01524_9",
    "clip_path": "clips/01524/01524__0027500_0029500.mp4"
  },
  {
    "timestamp": "00:32 - 00:36",
    "context": "The cashier hands over a white plastic bag, which produces a loud, crinkling rustle.",
    "question_type": "Sound Source Identification",
    "question": "What object generated the loud, crinkling rustle at 00:32–00:36?",
    "answer": "The white plastic bag being handed to the user.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01524.mp4",
    "question_id": "01524_10",
    "clip_path": "clips/01524/01524__0031500_0036500.mp4"
  },
  {
    "timestamp": "00:36 - 00:42",
    "context": "As the user walks away from the counter, the plastic bag rustles loudly and continuously.",
    "question_type": "Temporal Information",
    "question": "How did the plastic bag's rustling behave between 00:36 and 00:42?",
    "answer": "It was loud and continuous throughout that period.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01524.mp4",
    "question_id": "01524_11",
    "clip_path": "clips/01524/01524__0035500_0042500.mp4"
  },
  {
    "timestamp": "00:48 - 00:51",
    "context": "A pleasant, melodic chime plays from the store's speaker system, likely an automated welcome or thank you message.",
    "question_type": "Sound Source Identification",
    "question": "What was the source of the pleasant, melodic chime at 00:48–00:51?",
    "answer": "The store's speaker system.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01524.mp4",
    "question_id": "01524_12",
    "clip_path": "clips/01524/01524__0047500_0051500.mp4"
  },
  {
    "timestamp": "00:48 - 00:51",
    "context": "A pleasant, melodic chime plays from the store's speaker system, likely an automated welcome or thank you message for customers entering or leaving.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Based on the context, why did the melodic chime play?",
    "answer": "It was likely an automated welcome/thank-you message triggered for customers entering or leaving.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01524.mp4",
    "question_id": "01524_13",
    "clip_path": "clips/01524/01524__0047500_0051500.mp4"
  },
  {
    "timestamp": "00:51 - 00:53",
    "context": "Another customer opens the wooden-framed glass door, which makes a soft rubbing sound as it swings open.",
    "question_type": "Sound Characteristics",
    "question": "What is the quality of the door's sound as it opens?",
    "answer": "A soft rubbing sound as it swings open.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01524.mp4",
    "question_id": "01524_14",
    "clip_path": "clips/01524/01524__0050500_0053500.mp4"
  },
  {
    "timestamp": "00:54 - 00:55",
    "context": "Immediately after exiting the store, a loud, sharp car horn honks twice from the left, from a vehicle in the outdoor plaza.",
    "question_type": "Counting",
    "question": "How many times did the car horn honk at 00:54–00:55?",
    "answer": "Twice.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01524.mp4",
    "question_id": "01524_15",
    "clip_path": "clips/01524/01524__0053500_0055500.mp4"
  },
  {
    "timestamp": "00:54 - 00:55",
    "context": "Immediately after exiting the store, a loud, sharp car horn honks twice from the left, from a vehicle in the outdoor plaza.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From which direction did the car horn originate relative to the user?",
    "answer": "From the left, coming from a vehicle in the outdoor plaza.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01524.mp4",
    "question_id": "01524_16",
    "clip_path": "clips/01524/01524__0053500_0055500.mp4"
  },
  {
    "timestamp": "00:54 - 00:55",
    "context": "Immediately after exiting the store, a loud, sharp car horn honks twice.",
    "question_type": "Temporal Information",
    "question": "When did the car horn occur relative to the user exiting the store?",
    "answer": "Immediately after exiting.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01524.mp4",
    "question_id": "01524_17",
    "clip_path": "clips/01524/01524__0053500_0055500.mp4"
  },
  {
    "timestamp": "00:00 - 00:05",
    "context": "In an airport baggage claim, a staff member on the left asks from the immediate left, '你的行李箱是什么颜色的?' to help find the operator's bag. The operator holding a white cane replies, '不知道耶'. The staff responds with a soft '哦' and a reassuring '没关系'.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the staff member ask '你的行李箱是什么颜色的?' at the start of the clip?",
    "answer": "To assist in locating the camera operator's luggage.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01525.mp4",
    "question_id": "01525_1",
    "clip_path": "clips/01525/01525__0000000_0005500.mp4"
  },
  {
    "timestamp": "00:00 - 00:05",
    "context": "After the operator answers '不知道耶', the staff member acknowledges with a soft '哦' and adds a reassuring '没关系', understanding the situation.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the staff member say '没关系' after the exchange?",
    "answer": "She was reassuring the operator after they said they didn't know the luggage color, indicating she understood the situation.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01525.mp4",
    "question_id": "01525_2",
    "clip_path": "clips/01525/01525__0000000_0005500.mp4"
  },
  {
    "timestamp": "00:00 - 00:05",
    "context": "A female airport staff member visible on the left speaks from the immediate left, asking, '你的行李箱是什么颜色的?'.",
    "question_type": "Sound Source Identification",
    "question": "Who asked '你的行李箱是什么颜色的?'",
    "answer": "The female airport staff member on the left.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01525.mp4",
    "question_id": "01525_3",
    "clip_path": "clips/01525/01525__0000000_0005500.mp4"
  },
  {
    "timestamp": "00:00 - 00:05",
    "context": "The camera operator replies to the staff member with '不知道耶'.",
    "question_type": "Sound Source Identification",
    "question": "Who said '不知道耶'?",
    "answer": "The camera operator.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01525.mp4",
    "question_id": "01525_4",
    "clip_path": "clips/01525/01525__0000000_0005500.mp4"
  },
  {
    "timestamp": "00:00 - 00:05",
    "context": "Following the operator's reply, the staff member acknowledges with a soft '哦'.",
    "question_type": "Sound Characteristics",
    "question": "What was the volume/quality of the staff member's '哦'?",
    "answer": "It was soft.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01525.mp4",
    "question_id": "01525_5",
    "clip_path": "clips/01525/01525__0000000_0005500.mp4"
  },
  {
    "timestamp": "00:00 - 00:05",
    "context": "After acknowledging, the staff member adds '没关系' in a reassuring manner.",
    "question_type": "Sound Characteristics",
    "question": "What was the tone of the staff member's '没关系'?",
    "answer": "Reassuring.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01525.mp4",
    "question_id": "01525_6",
    "clip_path": "clips/01525/01525__0000000_0005500.mp4"
  },
  {
    "timestamp": "00:00 - 00:05",
    "context": "The staff member's voice originates from the immediate left of the camera as she asks about the luggage color.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From which direction did the staff member's voice originate relative to the camera?",
    "answer": "From the immediate left of the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01525.mp4",
    "question_id": "01525_7",
    "clip_path": "clips/01525/01525__0000000_0005500.mp4"
  },
  {
    "timestamp": "00:00 - 00:05",
    "context": "The camera operator responds '不知道耶' from their own perspective.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the response '不知道耶' originate relative to the camera?",
    "answer": "From the camera operator's position, i.e., at the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01525.mp4",
    "question_id": "01525_8",
    "clip_path": "clips/01525/01525__0000000_0005500.mp4"
  },
  {
    "timestamp": "00:00 - 00:05",
    "context": "Sequence: staff asks about color -> operator says '不知道耶' -> staff says '哦' and then '没关系'.",
    "question_type": "Temporal Information",
    "question": "Did '没关系' occur before or after the operator's '不知道耶'?",
    "answer": "After.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01525.mp4",
    "question_id": "01525_9",
    "clip_path": "clips/01525/01525__0000000_0005500.mp4"
  },
  {
    "timestamp": "00:00 - 00:05",
    "context": "Within the short exchange, the staff member speaks three times: the question, a soft '哦', and '没关系'.",
    "question_type": "Counting",
    "question": "How many distinct utterances did the staff member make in this exchange?",
    "answer": "Three.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01525.mp4",
    "question_id": "01525_10",
    "clip_path": "clips/01525/01525__0000000_0005500.mp4"
  },
  {
    "timestamp": "00:00 - 00:05",
    "context": "All parts of the exchange (question, reply, acknowledgment, reassurance) occur within the opening five seconds.",
    "question_type": "Temporal Information",
    "question": "When does this conversation occur in the clip?",
    "answer": "Between 00:00 and 00:05.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01525.mp4",
    "question_id": "01525_11",
    "clip_path": "clips/01525/01525__0000000_0005500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "In an airport baggage claim area, a female companion standing near the camera holder asks, \"What color is your luggage?\" The exchange reveals she is helping the visually impaired person locate their luggage from the carousel.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the companion ask, \"What color is your luggage?\"",
    "answer": "To help the visually impaired camera holder locate their luggage on the carousel.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01527.mp4",
    "question_id": "01527_1",
    "clip_path": "clips/01527/01527__0000000_0006478.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "The camera holder replies softly, \"I don't know,\" followed by a brief, lighthearted laugh from both individuals.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did both individuals laugh briefly after the reply?",
    "answer": "Because the camera holder’s \"I don't know\" response made the moment lighthearted.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01527.mp4",
    "question_id": "01527_2",
    "clip_path": "clips/01527/01527__0000000_0006478.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "After the brief, lighthearted laugh, the companion says reassuringly, \"It's okay.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the companion say, \"It's okay\"?",
    "answer": "To reassure the camera holder after they couldn’t identify the luggage color.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01527.mp4",
    "question_id": "01527_3",
    "clip_path": "clips/01527/01527__0000000_0006478.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "A female companion standing near the camera holder asks in a clear, conversational tone, \"What color is your luggage?\"",
    "question_type": "Sound Source Identification",
    "question": "Who asked, \"What color is your luggage?\"",
    "answer": "The female companion standing near the camera holder.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01527.mp4",
    "question_id": "01527_4",
    "clip_path": "clips/01527/01527__0000000_0006478.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "The camera holder, visually navigating with a white cane, replies softly, \"I don't know.\"",
    "question_type": "Sound Source Identification",
    "question": "Who responded, \"I don't know\"?",
    "answer": "The camera holder (the visually impaired person).",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01527.mp4",
    "question_id": "01527_5",
    "clip_path": "clips/01527/01527__0000000_0006478.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "The companion's question is described as being asked in a clear, conversational tone.",
    "question_type": "Sound Characteristics",
    "question": "How was the companion's question delivered?",
    "answer": "In a clear, conversational tone.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01527.mp4",
    "question_id": "01527_6",
    "clip_path": "clips/01527/01527__0000000_0006478.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "The camera holder replies softly, \"I don't know.\"",
    "question_type": "Sound Characteristics",
    "question": "How was the camera holder’s reply spoken?",
    "answer": "Softly.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01527.mp4",
    "question_id": "01527_7",
    "clip_path": "clips/01527/01527__0000000_0006478.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "This is followed by a brief, lighthearted laugh from both individuals.",
    "question_type": "Sound Characteristics",
    "question": "What were the duration and mood of the laughter?",
    "answer": "Brief and lighthearted.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01527.mp4",
    "question_id": "01527_8",
    "clip_path": "clips/01527/01527__0000000_0006478.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "The female companion is described as standing near the camera holder when speaking.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the companion’s speech originate relative to the camera?",
    "answer": "From a nearby position next to the camera holder, i.e., near the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01527.mp4",
    "question_id": "01527_9",
    "clip_path": "clips/01527/01527__0000000_0006478.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "After the camera holder’s soft reply, both individuals laugh briefly, and then the companion says, \"It's okay.\"",
    "question_type": "Temporal Information",
    "question": "What followed the camera holder’s soft reply?",
    "answer": "A brief, lighthearted laugh from both, followed by the companion saying, \"It's okay.\"",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01527.mp4",
    "question_id": "01527_10",
    "clip_path": "clips/01527/01527__0000000_0006478.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "A brief, lighthearted laugh from both individuals is heard.",
    "question_type": "Counting",
    "question": "How many people laughed?",
    "answer": "Two—the companion and the camera holder.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01527.mp4",
    "question_id": "01527_11",
    "clip_path": "clips/01527/01527__0000000_0006478.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "The camera holder is visually navigating with a white cane, and the conversation centers on identifying the luggage color to find it.",
    "question_type": "Cross-Modal Reasoning",
    "question": "How does the visual detail of the white cane help explain the audio exchange?",
    "answer": "It indicates the camera holder is visually impaired, explaining why they didn’t know the luggage color and why the companion is assisting in locating it.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01527.mp4",
    "question_id": "01527_12",
    "clip_path": "clips/01527/01527__0000000_0006478.mp4"
  },
  {
    "timestamp": "00:00 - 00:17",
    "context": "She delivers a continuous monologue in a clear, moderate-volume female voice, recounting a past experience: \"第一次我带那个导盲犬跟他的时候，我也不是很会带嘛，因为我第一次带有导盲犬的。\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why does she say she wasn't very good at handling the guide dog?",
    "answer": "Because it was her first time using a guide dog.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01530.mp4",
    "question_id": "01530_1",
    "clip_path": "clips/01530/01530__0000000_0017485.mp4"
  },
  {
    "timestamp": "00:00 - 00:17",
    "context": "She recounts asking whether she needed him to hold onto her: \"需不需要他搀着我。他说不用了，你只要带着这个狗走就可以了，它会牵着我走。\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the other person tell her there was no need for him to hold onto her?",
    "answer": "Because she was unsure if she needed his physical assistance, and he clarified the guide dog would lead her.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01530.mp4",
    "question_id": "01530_2",
    "clip_path": "clips/01530/01530__0000000_0017485.mp4"
  },
  {
    "timestamp": "00:00 - 00:17",
    "context": "As she walks, the sound of her own footsteps on the floor is audible.",
    "question_type": "Sound Source Identification",
    "question": "What generated the audible footsteps during this segment?",
    "answer": "Her own footsteps while she walked.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01530.mp4",
    "question_id": "01530_3",
    "clip_path": "clips/01530/01530__0000000_0017485.mp4"
  },
  {
    "timestamp": "00:00 - 00:17",
    "context": "She delivers a continuous monologue in a clear, moderate-volume female voice.",
    "question_type": "Sound Source Identification",
    "question": "Who is speaking in the continuous monologue?",
    "answer": "The user herself, a female speaker.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01530.mp4",
    "question_id": "01530_4",
    "clip_path": "clips/01530/01530__0000000_0017485.mp4"
  },
  {
    "timestamp": "00:00 - 00:17",
    "context": "She delivers a continuous monologue in a clear, moderate-volume female voice.",
    "question_type": "Sound Characteristics",
    "question": "What are the volume and clarity characteristics of her monologue?",
    "answer": "It is clear and at a moderate volume.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01530.mp4",
    "question_id": "01530_5",
    "clip_path": "clips/01530/01530__0000000_0017485.mp4"
  },
  {
    "timestamp": "00:00 - 00:17",
    "context": "She delivers a continuous monologue in a clear, moderate-volume female voice.",
    "question_type": "Temporal Information",
    "question": "Is her monologue brief or continuous over this time span?",
    "answer": "Continuous throughout 00:00–00:17.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01530.mp4",
    "question_id": "01530_6",
    "clip_path": "clips/01530/01530__0000000_0017485.mp4"
  },
  {
    "timestamp": "00:00 - 00:17",
    "context": "The user walks through the terminal. As she walks, the sound of her own footsteps on the floor is audible.",
    "question_type": "Temporal Information",
    "question": "When are her footsteps audible, and do they persist while she is walking?",
    "answer": "They are audible during her walk across the segment, persisting as she moves from 00:00 to 00:17.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01530.mp4",
    "question_id": "01530_7",
    "clip_path": "clips/01530/01530__0000000_0017485.mp4"
  },
  {
    "timestamp": "00:00 - 00:17",
    "context": "She says: \"还有两个呢是有导盲犬但是他没有带。\" (There were two others who have guide dogs, but they didn't bring them.)",
    "question_type": "Counting",
    "question": "How many others did she say had guide dogs but did not bring them?",
    "answer": "Two.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01530.mp4",
    "question_id": "01530_8",
    "clip_path": "clips/01530/01530__0000000_0017485.mp4"
  },
  {
    "timestamp": "00:00 - 00:17",
    "context": "As she walks, the sound of her own footsteps on the floor is audible. She delivers a continuous monologue in a clear, moderate-volume female voice.",
    "question_type": "Counting",
    "question": "How many main sound sources are heard simultaneously during this segment?",
    "answer": "Two: her speech and her footsteps.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01530.mp4",
    "question_id": "01530_9",
    "clip_path": "clips/01530/01530__0000000_0017485.mp4"
  },
  {
    "timestamp": "00:00 - 00:17",
    "context": "As she walks, the sound of her own footsteps on the floor is audible.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where do the footsteps originate relative to the camera?",
    "answer": "From the user herself at close range, effectively near and centered relative to the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01530.mp4",
    "question_id": "01530_10",
    "clip_path": "clips/01530/01530__0000000_0017485.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "[00:00 - 00:07] A female companion provides close-range guidance with a clear, calm voice, saying: \"We are preparing to board the plane. Take it slow... Okay, okay, come hold on to me. This ramp is a bit steep now. Okay.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the companion tell the user to take it slow and to hold on to her while mentioning the ramp being steep?",
    "answer": "To safely guide the visually impaired user onto the steep ramp while boarding the plane.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01532.mp4",
    "question_id": "01532_1",
    "clip_path": "clips/01532/01532__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "[00:00 - 00:07] The soft thuds of their footsteps are audible on the smooth floor as they begin their walk toward the aircraft.",
    "question_type": "Sound Source Identification",
    "question": "What produced the soft thuds heard in this segment?",
    "answer": "Their footsteps on the smooth floor.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01532.mp4",
    "question_id": "01532_2",
    "clip_path": "clips/01532/01532__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "[00:00 - 00:07] The soft thuds of their footsteps are audible on the smooth floor as they begin their walk toward the aircraft.",
    "question_type": "Sound Characteristics",
    "question": "How are the footsteps acoustically described?",
    "answer": "They are soft thuds.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01532.mp4",
    "question_id": "01532_3",
    "clip_path": "clips/01532/01532__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "[00:00 - 00:07] A female companion provides close-range guidance with a clear, calm voice.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the companion’s speech originate relative to the camera?",
    "answer": "From close range, positioned closely to the camera/user.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01532.mp4",
    "question_id": "01532_4",
    "clip_path": "clips/01532/01532__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "[00:00 - 00:07] Companion says: \"... Take it slow... Okay, okay, come hold on to me. This ramp is a bit steep now. Okay.\"",
    "question_type": "Counting",
    "question": "How many times did the companion say “Okay” in this segment?",
    "answer": "Three times.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01532.mp4",
    "question_id": "01532_5",
    "clip_path": "clips/01532/01532__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:07 - 00:18",
    "context": "[00:07 - 00:18] While continuing down the jet bridge, they are accompanied by the light, rhythmic tapping of their white cane on the floor.",
    "question_type": "Sound Source Identification",
    "question": "What is the source of the light, rhythmic tapping heard during this segment?",
    "answer": "The user's white cane tapping on the floor.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01532.mp4",
    "question_id": "01532_6",
    "clip_path": "clips/01532/01532__0006500_0018413.mp4"
  },
  {
    "timestamp": "00:07 - 00:18",
    "context": "[00:07 - 00:18] The audio features light, rhythmic tapping from the white cane as they continue walking.",
    "question_type": "Sound Characteristics",
    "question": "What are the acoustic qualities of the tapping sound?",
    "answer": "It is light and rhythmic.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01532.mp4",
    "question_id": "01532_7",
    "clip_path": "clips/01532/01532__0006500_0018413.mp4"
  },
  {
    "timestamp": "00:07 - 00:18",
    "context": "[00:07 - 00:18] User asks, \"Are both sides transparent?\" Companion replies: \"Yes, ah. Both sides are transparent glass... This is a kind of... like a bridge. It can extend and retract. Be careful, be careful.\" The caption notes her reply directly responds to the user's question to provide context.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the companion describe the transparent sides and the bridge’s ability to extend and retract?",
    "answer": "She was directly responding to the user’s question to help them understand the structure they were navigating.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01532.mp4",
    "question_id": "01532_8",
    "clip_path": "clips/01532/01532__0006500_0018413.mp4"
  },
  {
    "timestamp": "00:07 - 00:18",
    "context": "[00:07 - 00:18] Companion says, \"Be careful, be careful.\"",
    "question_type": "Counting",
    "question": "How many times did the companion say “Be careful” in this segment?",
    "answer": "Twice.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01532.mp4",
    "question_id": "01532_9",
    "clip_path": "clips/01532/01532__0006500_0018413.mp4"
  },
  {
    "timestamp": "00:07 - 00:18",
    "context": "[00:07 - 00:18] While continuing down the jet bridge, they are accompanied by the light, rhythmic tapping of their white cane on the floor.",
    "question_type": "Temporal Information",
    "question": "Is the cane tapping brief or does it continue throughout this interval?",
    "answer": "It continues throughout the interval as they walk.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01532.mp4",
    "question_id": "01532_10",
    "clip_path": "clips/01532/01532__0006500_0018413.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "A female voice, originating from immediately next to the camera, provides navigational instructions: \"As we walk over now, there are no steps along the way. Then when we are about to turn, I will tell you.\" This guidance is given to help the visually impaired person navigate.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why is the female voice providing navigational instructions during this segment?",
    "answer": "To help the visually impaired person safely navigate the path, informing them there are no steps and that she will indicate when to turn.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01534.mp4",
    "question_id": "01534_1",
    "clip_path": "clips/01534/01534__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "A very faint, low-volume scraping sound is continuously audible, caused by the tip of the white cane sliding across the polished floor.",
    "question_type": "Sound Source Identification",
    "question": "What generated the faint scraping sound heard during the clip?",
    "answer": "The tip of the white cane sliding across the polished floor.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01534.mp4",
    "question_id": "01534_2",
    "clip_path": "clips/01534/01534__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "A very faint, low-volume scraping sound is continuously audible from the cane tip on the polished floor.",
    "question_type": "Sound Characteristics",
    "question": "What are the volume and texture characteristics of the scraping sound?",
    "answer": "It is very faint and low in volume, with a subtle scraping texture.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01534.mp4",
    "question_id": "01534_3",
    "clip_path": "clips/01534/01534__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "A female voice, originating from immediately next to the camera, provides clear, conversational navigational instructions.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where does the woman's voice originate relative to the camera?",
    "answer": "Immediately next to the camera, at very close range.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01534.mp4",
    "question_id": "01534_4",
    "clip_path": "clips/01534/01534__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "The faint scraping from the cane tip is continuously audible throughout the segment.",
    "question_type": "Temporal Information",
    "question": "Is the cane's scraping sound brief or continuous, and over what timespan is it heard?",
    "answer": "It is continuous, audible across the entire 00:00–00:06 interval.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01534.mp4",
    "question_id": "01534_5",
    "clip_path": "clips/01534/01534__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "She says: \"As we walk over now, there are no steps along the way. Then when we are about to turn, I will tell you.\"",
    "question_type": "Counting",
    "question": "How many distinct instruction statements does the woman make in her guidance?",
    "answer": "Two statements.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01534.mp4",
    "question_id": "01534_6",
    "clip_path": "clips/01534/01534__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "A very faint scraping sound is caused by the tip of the white cane sliding across the polished floor in the airport terminal.",
    "question_type": "Cross-Modal Reasoning",
    "question": "Based on the scraping sound produced by the cane tip, what surface are they likely traversing?",
    "answer": "A polished floor.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01534.mp4",
    "question_id": "01534_7",
    "clip_path": "clips/01534/01534__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "The woman says, \"there are no steps along the way,\" while guiding the traveler through the terminal.",
    "question_type": "Cross-Modal Reasoning",
    "question": "When the woman states there are no steps along the way, what does this indicate about the immediate path's layout?",
    "answer": "The path is flat with no steps to navigate.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01534.mp4",
    "question_id": "01534_8",
    "clip_path": "clips/01534/01534__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:10",
    "context": "A woman with a child on an electric scooter stops beside the camera holder. The woman's voice, originating from the immediate right, initiates a curious conversation about the guide dog.",
    "question_type": "Inferential & Contextual Causality",
    "question": "What was the likely reason the woman initiated the conversation?",
    "answer": "She was curious about the guide dog.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01536.mp4",
    "question_id": "01536_1",
    "clip_path": "clips/01536/01536__0000000_0010500.mp4"
  },
  {
    "timestamp": "00:00 - 00:10",
    "context": "The woman's voice, originating from the immediate right, initiates a normal-volume conversation about the dog.",
    "question_type": "Sound Source Identification",
    "question": "Whose voice originated from the immediate right at the start of the clip?",
    "answer": "The woman who stopped beside the camera holder.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01536.mp4",
    "question_id": "01536_2",
    "clip_path": "clips/01536/01536__0000000_0010500.mp4"
  },
  {
    "timestamp": "00:00 - 00:10",
    "context": "The woman's voice, originating from the immediate right, initiates a curious conversation at a normal volume about the dog.",
    "question_type": "Sound Characteristics",
    "question": "What was the volume level of the woman's voice when she began speaking?",
    "answer": "Normal volume.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01536.mp4",
    "question_id": "01536_3",
    "clip_path": "clips/01536/01536__0000000_0010500.mp4"
  },
  {
    "timestamp": "00:00 - 00:10",
    "context": "The woman's voice, originating from the immediate right, initiates a conversation about the dog.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From what direction relative to the camera did the woman's speech come?",
    "answer": "From the immediate right, beside the camera holder.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01536.mp4",
    "question_id": "01536_4",
    "clip_path": "clips/01536/01536__0000000_0010500.mp4"
  },
  {
    "timestamp": "00:00 - 00:10",
    "context": "During the first 10 seconds, a conversation occurs between the woman and the camera holder about the guide dog.",
    "question_type": "Temporal Information",
    "question": "When did the conversation between the woman and the camera holder occur within the clip?",
    "answer": "Between 00:00 and 00:10.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01536.mp4",
    "question_id": "01536_5",
    "clip_path": "clips/01536/01536__0000000_0010500.mp4"
  },
  {
    "timestamp": "00:00 - 00:10",
    "context": "Woman: \"5岁了?\"; Woman: \"他在工作吗?\"; Woman: \"他在干嘛?\"",
    "question_type": "Counting",
    "question": "How many distinct questions did the woman ask?",
    "answer": "Three.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01536.mp4",
    "question_id": "01536_6",
    "clip_path": "clips/01536/01536__0000000_0010500.mp4"
  },
  {
    "timestamp": "00:00 - 00:10",
    "context": "Woman: \"他在干嘛?\" (What is he doing?) Camera Holder: \"带路呀.\" (He's guiding the way.)",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the camera holder say \"带路呀.\" (He's guiding the way.)?",
    "answer": "He was responding to the woman's question about what the dog was doing.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01536.mp4",
    "question_id": "01536_7",
    "clip_path": "clips/01536/01536__0000000_0010500.mp4"
  },
  {
    "timestamp": "00:00 - 00:21",
    "context": "[00:00 - 00:06] First person: “你看一下这个有没有... 你帮我把这个拿来” (asks to examine/get the product). [00:06 - 00:21] Camera operator reads the box aloud to understand its function.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the camera operator start reading the Chinese characters on the box aloud at 00:06?",
    "answer": "Because the first person had asked them to check the product and they were trying to identify its function.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01538.mp4",
    "question_id": "01538_1",
    "clip_path": "clips/01538/01538__0000000_0021500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "Two individuals converse in a store. The first person, at close range, says: “你看一下这个有没有... 你帮我把这个拿来.”",
    "question_type": "Sound Source Identification",
    "question": "Who produced the mid-volume request to examine and get the product?",
    "answer": "The first person speaking near the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01538.mp4",
    "question_id": "01538_2",
    "clip_path": "clips/01538/01538__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "The first person asks in a clear, mid-volume voice from a close distance.",
    "question_type": "Sound Characteristics",
    "question": "What was the volume and clarity of the first person's initial request?",
    "answer": "It was clear and mid-volume.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01538.mp4",
    "question_id": "01538_3",
    "clip_path": "clips/01538/01538__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:00 - 00:06",
    "context": "The first person makes the request from a close distance to the camera operator.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the initial request originate relative to the camera?",
    "answer": "From a close distance to the camera.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01538.mp4",
    "question_id": "01538_4",
    "clip_path": "clips/01538/01538__0000000_0006500.mp4"
  },
  {
    "timestamp": "00:06 - 00:21",
    "context": "The camera operator reads the packaging aloud in a contemplative, slow-paced voice to understand its function.",
    "question_type": "Temporal Information",
    "question": "Was the reading brief or sustained, and how long did it last?",
    "answer": "It was sustained, lasting about 15 seconds from 00:06 to 00:21.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01538.mp4",
    "question_id": "01538_5",
    "clip_path": "clips/01538/01538__0005500_0021500.mp4"
  },
  {
    "timestamp": "00:06 - 00:21",
    "context": "Spoken words: “看看...舒缓...保湿...特护...特护霜, 是吗?”",
    "question_type": "Counting",
    "question": "How many distinct product descriptors were read aloud before asking “right?”",
    "answer": "Four: soothing, moisturizing, special care, and special care cream.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01538.mp4",
    "question_id": "01538_6",
    "clip_path": "clips/01538/01538__0005500_0021500.mp4"
  },
  {
    "timestamp": "00:21 - 00:25",
    "context": "The first person, standing nearby, confirms: “对, 它就是舒缓特护霜.”",
    "question_type": "Sound Source Identification",
    "question": "Who delivered the clear, affirmative confirmation of the product’s identity?",
    "answer": "The first person standing nearby.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01538.mp4",
    "question_id": "01538_7",
    "clip_path": "clips/01538/01538__0020500_0025500.mp4"
  },
  {
    "timestamp": "00:21 - 00:28",
    "context": "[00:21 - 00:25] First person confirms it’s a soothing special care cream. [00:25 - 00:28] The camera operator responds: “哇, 好棒呀. 拿一个吧.”",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the camera operator decide to purchase the item at 00:25 - 00:28?",
    "answer": "Because, after the confirmation that it was a soothing special care cream, they were satisfied and chose to get one.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01538.mp4",
    "question_id": "01538_8",
    "clip_path": "clips/01538/01538__0020500_0028200.mp4"
  },
  {
    "timestamp": "00:25 - 00:28",
    "context": "The camera operator expresses satisfaction in a happy, slightly higher-pitched voice: “哇, 好棒呀. 拿一个吧.”",
    "question_type": "Sound Characteristics",
    "question": "What was the emotional tone and pitch quality of the purchase decision statement?",
    "answer": "Happy and slightly higher-pitched.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01538.mp4",
    "question_id": "01538_9",
    "clip_path": "clips/01538/01538__0024500_0028200.mp4"
  },
  {
    "timestamp": "00:25 - 00:28",
    "context": "After “拿一个吧,” the other person agrees immediately with a short, supportive “嗯, 可以.”",
    "question_type": "Temporal Information",
    "question": "When did the agreement occur, and was it immediate or delayed?",
    "answer": "Between 00:25 and 00:28, and it was immediate.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01538.mp4",
    "question_id": "01538_10",
    "clip_path": "clips/01538/01538__0024500_0028200.mp4"
  },
  {
    "timestamp": "00:00 - 00:08",
    "context": "From a first-person view, a white cane strikes stone tiles, creating sharp, rhythmic tapping. A nearby female companion loudly warns, “小心! (Be careful!)”, followed by “别动啊 (Don't move!)”.",
    "question_type": "Sound Source Identification",
    "question": "What generated the sharp, rhythmic tapping at the start of the video?",
    "answer": "The white cane striking the stone tiles.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01540.mp4",
    "question_id": "01540_1",
    "clip_path": "clips/01540/01540__0000000_0008500.mp4"
  },
  {
    "timestamp": "00:00 - 00:08",
    "context": "A female voice from a nearby companion loudly calls out, “小心! (Be careful!)”.",
    "question_type": "Sound Characteristics",
    "question": "What was the volume of the companion’s warning at the beginning?",
    "answer": "Loud.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01540.mp4",
    "question_id": "01540_2",
    "clip_path": "clips/01540/01540__0000000_0008500.mp4"
  },
  {
    "timestamp": "00:00 - 00:08",
    "context": "After a loud “小心! (Be careful!)”, the first speaker adds, “别动啊 (Don't move!)”, coordinating movement to avoid contact.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the speaker say “别动啊 (Don't move!)” at this moment?",
    "answer": "To coordinate movements and avoid a collision.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01540.mp4",
    "question_id": "01540_3",
    "clip_path": "clips/01540/01540__0000000_0008500.mp4"
  },
  {
    "timestamp": "00:00 - 00:08",
    "context": "The warning voice is described as coming from a nearby companion.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Relative to the camera, where did the warning “小心!” originate?",
    "answer": "From a nearby companion (close to the camera).",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01540.mp4",
    "question_id": "01540_4",
    "clip_path": "clips/01540/01540__0000000_0008500.mp4"
  },
  {
    "timestamp": "00:08 - 00:13",
    "context": "As they continue forward toward another visually impaired person, the cane’s rhythmic tapping persists.",
    "question_type": "Temporal Information",
    "question": "During 00:08–00:13, is the cane tapping intermittent or continuous?",
    "answer": "Continuous; it persists as they approach the other person.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01540.mp4",
    "question_id": "01540_5",
    "clip_path": "clips/01540/01540__0007500_0013500.mp4"
  },
  {
    "timestamp": "00:13 - 00:23",
    "context": "After meeting, both canes are heard tapping as the filmer gives navigation instructions based on the direction they came from.",
    "question_type": "Counting",
    "question": "How many canes’ tapping sounds are audible once they begin walking together?",
    "answer": "Two.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01540.mp4",
    "question_id": "01540_6",
    "clip_path": "clips/01540/01540__0012500_0023500.mp4"
  },
  {
    "timestamp": "00:13 - 00:23",
    "context": "The filmer says, “走，从我的后方…这是我来的那个方向…就从我来的这个方向咱过去,” indicating route memory and guidance.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why does the filmer specify going from “the direction I came from”?",
    "answer": "To guide their companion using verbal directions grounded in her memory of the route.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01540.mp4",
    "question_id": "01540_7",
    "clip_path": "clips/01540/01540__0012500_0023500.mp4"
  },
  {
    "timestamp": "00:24 - 00:28",
    "context": "While passing a parked white car, one asks, “要不要再收起来？(Should we fold it up again?)” about the cane, and checks if it bumped the car.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did they consider folding the cane near the parked car?",
    "answer": "Because they were concerned the cane might be an obstacle and risk bumping into the car.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01540.mp4",
    "question_id": "01540_8",
    "clip_path": "clips/01540/01540__0023500_0028500.mp4"
  },
  {
    "timestamp": "00:34 - 00:41",
    "context": "They reach a metal fence, and the cane produces a duller tapping sound against the fence’s base compared to earlier on pavement.",
    "question_type": "Sound Characteristics",
    "question": "How did the cane’s sound change upon reaching the metal fence?",
    "answer": "It became a duller tapping compared to the earlier sharp taps on stone tiles.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01540.mp4",
    "question_id": "01540_9",
    "clip_path": "clips/01540/01540__0033500_0041500.mp4"
  },
  {
    "timestamp": "00:34 - 00:41",
    "context": "At the metal fence, the cane’s taps sound duller as it contacts the fence’s base.",
    "question_type": "Cross-Modal Reasoning",
    "question": "What surface interaction most likely caused the cane’s taps to sound duller here?",
    "answer": "Contact with the metal fence’s base rather than the pavement.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01540.mp4",
    "question_id": "01540_10",
    "clip_path": "clips/01540/01540__0033500_0041500.mp4"
  },
  {
    "timestamp": "00:51 - 01:00",
    "context": "While joking about the cane being a “长签子 (long skewer)”, the companion warns, “车 (Car)”, as they maneuver between parked vehicles.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the companion say “车 (Car)” during this segment?",
    "answer": "To warn about a nearby vehicle while they were moving between parked cars.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01540.mp4",
    "question_id": "01540_11",
    "clip_path": "clips/01540/01540__0050500_0060500.mp4"
  },
  {
    "timestamp": "01:00 - 01:05",
    "context": "They arrive at an open area believed to be an entrance; the speaker says, “我们可以从这进去了…来过 (We should be able to get in from here. I’ve been here before).”",
    "question_type": "Inferential & Contextual Causality",
    "question": "What gave the speaker confidence they could enter from that spot?",
    "answer": "Her prior experience—she had been there before.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01540.mp4",
    "question_id": "01540_12",
    "clip_path": "clips/01540/01540__0059500_0065500.mp4"
  },
  {
    "timestamp": "01:05 - 01:17",
    "context": "They laugh about the route difference; the speaker notes, “我以前在这边干了两年，只不过他现在变化比较大 (I used to work here for two years, but it has changed a lot now).”",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why does the speaker say the route differs from hers?",
    "answer": "Because the area has changed a lot since she used to work there.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01540.mp4",
    "question_id": "01540_13",
    "clip_path": "clips/01540/01540__0064500_0077500.mp4"
  },
  {
    "timestamp": "01:17 - 01:27",
    "context": "After pausing and asking, “听到了吗？(Did you hear that?)”, they decide, “咱们要去湖边… (We’re going to the lakeside).”",
    "question_type": "Cross-Modal Reasoning",
    "question": "Following the moment of listening, what destination do they choose?",
    "answer": "They decide to go to the lakeside.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01540.mp4",
    "question_id": "01540_14",
    "clip_path": "clips/01540/01540__0076500_0087500.mp4"
  },
  {
    "timestamp": "01:27 - 01:31",
    "context": "At a plaza with vendors and a passing car in the background, ambient sound includes the distant chatter of people and traffic.",
    "question_type": "Sound Characteristics",
    "question": "Are the people’s chatter and traffic sounds near or distant relative to the camera?",
    "answer": "Distant.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01540.mp4",
    "question_id": "01540_15",
    "clip_path": "clips/01540/01540__0086500_0090953.mp4"
  },
  {
    "timestamp": "01:27 - 01:31",
    "context": "Ambient sound now includes the distant chatter of people and traffic while the cane tapping continues.",
    "question_type": "Counting",
    "question": "How many distinct ambient non-cane sound types are audible here?",
    "answer": "Two: people’s chatter and traffic.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01540.mp4",
    "question_id": "01540_16",
    "clip_path": "clips/01540/01540__0086500_0090953.mp4"
  },
  {
    "timestamp": "00:00 - 00:10",
    "context": "[00:00 - 00:10] The rhythmic tapping of the white cane on the brick sidewalk is audible as the conversation begins.",
    "question_type": "Sound Source Identification",
    "question": "What produced the rhythmic tapping heard at the start?",
    "answer": "The visually impaired woman's white cane contacting the brick sidewalk.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01561.mp4",
    "question_id": "01561_1",
    "clip_path": "clips/01561/01561__0000000_0010500.mp4"
  },
  {
    "timestamp": "00:00 - 00:10",
    "context": "[00:00 - 00:10] The operator suggests memorizing the route to the pharmacy. The woman agrees with a laugh.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the woman laugh at the end of this segment?",
    "answer": "She laughed while agreeing to the operator’s suggestion to try memorizing the route to the pharmacy.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01561.mp4",
    "question_id": "01561_2",
    "clip_path": "clips/01561/01561__0000000_0010500.mp4"
  },
  {
    "timestamp": "00:10 - 00:23",
    "context": "[00:10 - 00:23] Throughout the conversation, the crisp, repetitive tapping of the cane continues as she walks forward, probing the path ahead.",
    "question_type": "Temporal Information",
    "question": "During this segment, is the cane tapping intermittent or continuous?",
    "answer": "It is continuous throughout the segment.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01561.mp4",
    "question_id": "01561_3",
    "clip_path": "clips/01561/01561__0009500_0023500.mp4"
  },
  {
    "timestamp": "00:10 - 00:23",
    "context": "[00:10 - 00:23] The operator asks if she has been practicing. She replies she hasn't, saying, \"I'm afraid my feet would be uncomfortable.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the woman say she hadn't been practicing?",
    "answer": "Because she was afraid her feet would be uncomfortable.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01561.mp4",
    "question_id": "01561_4",
    "clip_path": "clips/01561/01561__0009500_0023500.mp4"
  },
  {
    "timestamp": "00:24 - 00:35",
    "context": "[00:24 - 00:35] As she navigates around a low metal bollard, the cane sound changes from a sharp tap on brick to a duller thud on concrete.",
    "question_type": "Inferential & Contextual Causality",
    "question": "What caused the cane sound to change from a sharp tap to a duller thud?",
    "answer": "She moved from tapping on brick to tapping on concrete while going around a low metal bollard.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01561.mp4",
    "question_id": "01561_5",
    "clip_path": "clips/01561/01561__0023500_0035500.mp4"
  },
  {
    "timestamp": "00:24 - 00:35",
    "context": "[00:24 - 00:35] Approaching a white car parked on the sidewalk, the cane makes a hollow, plastic-on-metal sound as it taps the car's front bumper.",
    "question_type": "Sound Source Identification",
    "question": "What generated the hollow, plastic-on-metal sound?",
    "answer": "The cane tapping the white car’s front bumper.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01561.mp4",
    "question_id": "01561_6",
    "clip_path": "clips/01561/01561__0023500_0035500.mp4"
  },
  {
    "timestamp": "00:24 - 00:35",
    "context": "[00:24 - 00:35] A loud, sharp metallic banging sound is heard from a nearby construction site.",
    "question_type": "Sound Source Identification",
    "question": "What was the source of the loud, sharp metallic banging?",
    "answer": "A nearby construction site.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01561.mp4",
    "question_id": "01561_7",
    "clip_path": "clips/01561/01561__0023500_0035500.mp4"
  },
  {
    "timestamp": "00:41 - 00:52",
    "context": "[00:41 - 00:52] A man across the street calls out clear, loud directions from the front right.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From where relative to the camera did the man's voice originate?",
    "answer": "From the front right, across the street.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01561.mp4",
    "question_id": "01561_8",
    "clip_path": "clips/01561/01561__0040500_0052500.mp4"
  },
  {
    "timestamp": "00:41 - 00:52",
    "context": "[00:41 - 00:52] In direct response to his voice, the woman walks past the car, navigates around a parked scooter, and turns left onto the correct path.",
    "question_type": "Cross-Modal Reasoning",
    "question": "After the man called out directions, what actions did the woman take?",
    "answer": "She walked past the car, went around a parked electric scooter, and turned left onto the correct path.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01561.mp4",
    "question_id": "01561_9",
    "clip_path": "clips/01561/01561__0040500_0052500.mp4"
  },
  {
    "timestamp": "00:41 - 00:52",
    "context": "[00:41 - 00:52] Man calls out: \"Go up... You go forward, keep going forward... Turn left.\"",
    "question_type": "Counting",
    "question": "How many distinct directional commands did the man give?",
    "answer": "Three—“Go up,” “You go forward, keep going forward,” and “Turn left.”",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01561.mp4",
    "question_id": "01561_10",
    "clip_path": "clips/01561/01561__0040500_0052500.mp4"
  },
  {
    "timestamp": "01:01 - 01:08",
    "context": "[01:01 - 01:08] The operator prompts her to navigate on her own and says with a laugh, \"I'm following you now.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the operator laugh and say, “I’m following you now”?",
    "answer": "To jokingly reinforce the training context and prompt her to navigate independently.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01561.mp4",
    "question_id": "01561_11",
    "clip_path": "clips/01561/01561__0060500_0068500.mp4"
  },
  {
    "timestamp": "01:10 - 01:16",
    "context": "[01:10 - 01:16] A bystander off-camera says, \"Walk on the right side.\" The woman replies with a soft \"Thank you.\"",
    "question_type": "Sound Characteristics",
    "question": "How is the woman's \"Thank you\" described in terms of volume?",
    "answer": "It is soft.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01561.mp4",
    "question_id": "01561_12",
    "clip_path": "clips/01561/01561__0069500_0076500.mp4"
  },
  {
    "timestamp": "01:10 - 01:16",
    "context": "[01:10 - 01:16] A bystander off-camera provides a hint: \"Walk on the right side.\" The operator says, \"Someone gave you a hint.\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the operator say, “Someone gave you a hint”?",
    "answer": "Because a bystander off-camera had just advised her to \"Walk on the right side.\"",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01561.mp4",
    "question_id": "01561_13",
    "clip_path": "clips/01561/01561__0069500_0076500.mp4"
  },
  {
    "timestamp": "01:31 - 01:46",
    "context": "[01:31 - 01:46] After reaching the corner, the operator says, \"You can listen now, which direction is the road in?\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the operator ask her to listen for the road’s direction?",
    "answer": "To prompt her to use hearing for orientation.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01561.mp4",
    "question_id": "01561_14",
    "clip_path": "clips/01561/01561__0090500_0106500.mp4"
  },
  {
    "timestamp": "01:47 - 01:56",
    "context": "[01:47 - 01:56] The operator says, \"Shouldn't you keep the road on your left side? That way your direction is correct, right?\"",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why should the woman keep the road on her left side?",
    "answer": "To ensure her walking direction remains correct.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01561.mp4",
    "question_id": "01561_15",
    "clip_path": "clips/01561/01561__0106500_0116500.mp4"
  },
  {
    "timestamp": "02:04 - 02:12",
    "context": "[02:04 - 02:12] The operator reminds her that the pharmacy was playing music and to listen for it.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the operator remind her that the pharmacy plays music?",
    "answer": "To encourage her to use the music as an auditory landmark to locate the pharmacy.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01561.mp4",
    "question_id": "01561_16",
    "clip_path": "clips/01561/01561__0123500_0132500.mp4"
  },
  {
    "timestamp": "02:13 - 02:21",
    "context": "[02:13 - 02:21] With the conversation paused, the faint hum of distant city traffic is audible in the background.",
    "question_type": "Sound Source Identification",
    "question": "What background environmental sound is audible during this interval besides the cane?",
    "answer": "The faint hum of distant city traffic.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01561.mp4",
    "question_id": "01561_17",
    "clip_path": "clips/01561/01561__0132500_0141500.mp4"
  },
  {
    "timestamp": "02:13 - 02:21",
    "context": "[02:13 - 02:21] With the conversation paused, the primary sound becomes the light, rhythmic tapping of the white cane.",
    "question_type": "Sound Characteristics",
    "question": "What is the primary sound in this interval, and how is it described?",
    "answer": "The light, rhythmic tapping of the white cane.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01561.mp4",
    "question_id": "01561_18",
    "clip_path": "clips/01561/01561__0132500_0141500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "[00:00 - 00:07] The user touches and presses a round metal button labeled \"一键报站\" (One-click station announcement), which causes a distinct, audible click.",
    "question_type": "Sound Source Identification",
    "question": "What produced the distinct audible click at the start?",
    "answer": "The round metal \"一键报站\" (one-click station announcement) button when the user pressed it.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01564.mp4",
    "question_id": "01564_1",
    "clip_path": "clips/01564/01564__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:00 - 00:07",
    "context": "[00:00 - 00:07] The user speaks about the button's function; her speech is clear and at a medium volume.",
    "question_type": "Sound Characteristics",
    "question": "How is the user's speech volume described during her explanation of the button's function?",
    "answer": "Clear and at a medium volume.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01564.mp4",
    "question_id": "01564_2",
    "clip_path": "clips/01564/01564__0000000_0007500.mp4"
  },
  {
    "timestamp": "00:07 - 00:18",
    "context": "[00:07 - 00:18] Immediately after the button is pressed, a quiet, automated female voice emanates from a speaker on the bus stop pole, announcing routes.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "From where did the automated female announcement originate relative to the scene?",
    "answer": "From a speaker on the bus stop pole.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01564.mp4",
    "question_id": "01564_3",
    "clip_path": "clips/01564/01564__0006500_0018500.mp4"
  },
  {
    "timestamp": "00:07 - 00:18",
    "context": "[00:07 - 00:18] The automated female voice announces routes; the user comments on the low volume.",
    "question_type": "Sound Characteristics",
    "question": "What was the volume characteristic of the automated announcement?",
    "answer": "It was quiet/low in volume.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01564.mp4",
    "question_id": "01564_4",
    "clip_path": "clips/01564/01564__0006500_0018500.mp4"
  },
  {
    "timestamp": "00:07 - 00:18",
    "context": "[00:07 - 00:18] The automated voice announces: \"11 Road, 18 Road is shipping... 316 has arrived at the station.\"",
    "question_type": "Counting",
    "question": "How many bus routes were mentioned in the automated announcement?",
    "answer": "Three: routes 11, 18, and 316.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01564.mp4",
    "question_id": "01564_5",
    "clip_path": "clips/01564/01564__0006500_0018500.mp4"
  },
  {
    "timestamp": "00:00 - 00:18",
    "context": "[00:00 - 00:07] The user presses the one-click announcement button. [00:07 - 00:18] An automated voice immediately plays from the pole speaker.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the automated announcement start playing at the bus stop?",
    "answer": "Because the user pressed the one-click station announcement button, triggering it.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01564.mp4",
    "question_id": "01564_6",
    "clip_path": "clips/01564/01564__0000000_0018500.mp4"
  },
  {
    "timestamp": "00:18 - 00:26",
    "context": "[00:18 - 00:26] A blue bus is seen stopping; earlier the announcement said \"316 has arrived.\" The user says, \"The one that stopped should be 316.\"",
    "question_type": "Cross-Modal Reasoning",
    "question": "Based on the earlier audio announcement that 316 had arrived, which route did the stopping blue bus most likely correspond to?",
    "answer": "Route 316.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01564.mp4",
    "question_id": "01564_7",
    "clip_path": "clips/01564/01564__0017500_0026500.mp4"
  },
  {
    "timestamp": "00:27 - 00:42",
    "context": "[00:27 - 00:42] The user explains that when not making specific announcements, the system continuously plays commercials and music to help locate the bus stop.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why does the bus stop system continuously play commercials and music when not announcing routes?",
    "answer": "To act as an auditory beacon so visually impaired people can locate the bus stop.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01564.mp4",
    "question_id": "01564_8",
    "clip_path": "clips/01564/01564__0026500_0042500.mp4"
  },
  {
    "timestamp": "00:42 - 00:57",
    "context": "[00:42 - 00:57] The user takes out her smartphone, and a high-pitched, rapid voice assistant is heard as she navigates and says, \"Open the Yinchuan line button.\"",
    "question_type": "Sound Source Identification",
    "question": "What device produced the high-pitched, rapid voice during the interface navigation?",
    "answer": "The smartphone’s voice assistant/screen reader.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01564.mp4",
    "question_id": "01564_9",
    "clip_path": "clips/01564/01564__0041500_0057500.mp4"
  },
  {
    "timestamp": "00:57 - 01:13",
    "context": "[00:57 - 01:13] The user compares app data with the stop's announcements; the phone's screen reader continues to narrate information in the background.",
    "question_type": "Temporal Information",
    "question": "During this interval, does the phone’s screen reader speak briefly or continue narrating?",
    "answer": "It continues narrating in the background.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01564.mp4",
    "question_id": "01564_10",
    "clip_path": "clips/01564/01564__0056500_0073500.mp4"
  },
  {
    "timestamp": "01:13 - 01:34",
    "context": "[01:13 - 01:34] The user questions the app’s long wait time and accuracy, deciding to seek help from others.",
    "question_type": "Inferential & Contextual Causality",
    "question": "What prompted the user to seek assistance from others at the bus stop?",
    "answer": "Her uncertainty about the app’s accuracy and the long reported wait time.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01564.mp4",
    "question_id": "01564_11",
    "clip_path": "clips/01564/01564__0072500_0094500.mp4"
  },
  {
    "timestamp": "01:35 - 01:46",
    "context": "[01:35 - 01:46] The user walks to two people sitting on the bench (footsteps audible) and asks for help; a woman responds.",
    "question_type": "Counting",
    "question": "How many people were sitting on the bench that the user approached for help?",
    "answer": "Two.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01564.mp4",
    "question_id": "01564_12",
    "clip_path": "clips/01564/01564__0094500_0106500.mp4"
  },
  {
    "timestamp": "01:48 - 02:00",
    "context": "[01:48 - 02:00] A blue bus arrives with engine and brake sounds; the bus door opens with a pneumatic hiss.",
    "question_type": "Sound Source Identification",
    "question": "What produced the pneumatic hiss when the bus arrived?",
    "answer": "The bus door opening.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01564.mp4",
    "question_id": "01564_13",
    "clip_path": "clips/01564/01564__0107500_0120500.mp4"
  },
  {
    "timestamp": "02:01 - 02:22",
    "context": "[02:01 - 02:22] The first bus's doors close with a loud beep and hiss. A second bus (route 37) pulls up and later departs.",
    "question_type": "Counting",
    "question": "Before the user boards route 40, how many buses pulled up to the stop in this segment?",
    "answer": "Two.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01564.mp4",
    "question_id": "01564_14",
    "clip_path": "clips/01564/01564__0120500_0142500.mp4"
  },
  {
    "timestamp": "02:01 - 02:22",
    "context": "[02:01 - 02:22] The first bus's doors close with a loud beep and hiss.",
    "question_type": "Sound Characteristics",
    "question": "How are the door-closing sounds of the first bus described?",
    "answer": "A loud beep followed by a hiss.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01564.mp4",
    "question_id": "01564_15",
    "clip_path": "clips/01564/01564__0120500_0142500.mp4"
  },
  {
    "timestamp": "03:00 - 03:06",
    "context": "[03:00 - 03:06] The phone's screen reader announces the bus is \"one stop\" away, and a nearby woman alerts that bus 40 has arrived.",
    "question_type": "Cross-Modal Reasoning",
    "question": "Immediately after the phone said the bus was \"one stop\" away, what did the nearby woman inform the user?",
    "answer": "That bus number 40 had arrived.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01564.mp4",
    "question_id": "01564_16",
    "clip_path": "clips/01564/01564__0179500_0186500.mp4"
  },
  {
    "timestamp": "03:06 - 03:16",
    "context": "[03:06 - 03:16] The user taps her transit card on the fare machine, which produces a loud electronic beep and an automated announcement: \"爱心卡\" (Love card).",
    "question_type": "Sound Characteristics",
    "question": "What sounds indicated a successful fare tap when the user boarded?",
    "answer": "A loud electronic beep and an automated announcement saying \"爱心卡\" (Love card).",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01564.mp4",
    "question_id": "01564_17",
    "clip_path": "clips/01564/01564__0185500_0196500.mp4"
  },
  {
    "timestamp": "03:17 - 03:21",
    "context": "[03:17 - 03:21] After boarding, the bus doors close with a series of beeps and a final, loud pneumatic whoosh.",
    "question_type": "Temporal Information",
    "question": "What was the sequence of sounds as the bus doors closed after she boarded?",
    "answer": "A series of beeps followed by a final, loud pneumatic whoosh.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01564.mp4",
    "question_id": "01564_18",
    "clip_path": "clips/01564/01564__0196500_0201500.mp4"
  },
  {
    "timestamp": "00:00 - 00:05",
    "context": "[00:00 - 00:05] Rhythmic, moderate-volume cane taps on asphalt. The user says, “Mengmeng said I walked all day and I'm too tired. She wants me to go buy her food.”",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why is the user going to buy food?",
    "answer": "Because Mengmeng asked her to buy food, saying she’d walked all day and was tired.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01570.mp4",
    "question_id": "01570_1",
    "clip_path": "clips/01570/01570__0000000_0005500.mp4"
  },
  {
    "timestamp": "00:00 - 00:05",
    "context": "[00:00 - 00:05] Rhythmic, moderate-volume tapping is heard while the user walks on an asphalt road.",
    "question_type": "Sound Source Identification",
    "question": "What produced the rhythmic tapping sound at the start?",
    "answer": "The user’s cane tapping on the asphalt road.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01570.mp4",
    "question_id": "01570_2",
    "clip_path": "clips/01570/01570__0000000_0005500.mp4"
  },
  {
    "timestamp": "00:00 - 00:05",
    "context": "[00:00 - 00:05] The cane produces a steady pattern of taps at a moderate volume.",
    "question_type": "Sound Characteristics",
    "question": "What are the rhythm and volume characteristics of the cane tapping?",
    "answer": "Rhythmic and moderate-volume.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01570.mp4",
    "question_id": "01570_3",
    "clip_path": "clips/01570/01570__0000000_0005500.mp4"
  },
  {
    "timestamp": "00:23 - 00:27",
    "context": "[00:23 - 00:27] A brief exchange near the entrance is heard from the front, about 2 meters away.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the nearby entrance conversation originate relative to the camera?",
    "answer": "From directly in front, about 2 meters away.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01570.mp4",
    "question_id": "01570_4",
    "clip_path": "clips/01570/01570__0022500_0027500.mp4"
  },
  {
    "timestamp": "00:27 - 00:34",
    "context": "[00:27 - 00:34] Soft rustling from transparent plastic door curtains as the user enters; indoor food court murmur replaces outdoor sounds.",
    "question_type": "Cross-Modal Reasoning",
    "question": "The soft rustling of plastic door curtains indicates what action is happening?",
    "answer": "The user is entering the canteen through the transparent plastic curtains.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01570.mp4",
    "question_id": "01570_5",
    "clip_path": "clips/01570/01570__0026500_0034500.mp4"
  },
  {
    "timestamp": "00:27 - 00:34",
    "context": "[00:27 - 00:34] After passing through the door curtains, the cane tapping stops and indoor ambience takes over.",
    "question_type": "Temporal Information",
    "question": "When did the cane tapping cease?",
    "answer": "It stopped once the user was inside the canteen after passing through the plastic door curtains.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01570.mp4",
    "question_id": "01570_6",
    "clip_path": "clips/01570/01570__0026500_0034500.mp4"
  },
  {
    "timestamp": "00:40 - 00:48",
    "context": "[00:40 - 00:48] The user pays by phone; a loud, clear digital voice says, “Payment successful, 19 yuan.”",
    "question_type": "Sound Source Identification",
    "question": "What device emitted the “Payment successful, 19 yuan” announcement?",
    "answer": "The payment terminal.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01570.mp4",
    "question_id": "01570_7",
    "clip_path": "clips/01570/01570__0039500_0048500.mp4"
  },
  {
    "timestamp": "00:40 - 00:48",
    "context": "[00:40 - 00:48] The user orders: “I want two jianbings… and add roasted sausage and spicy strips.”",
    "question_type": "Counting",
    "question": "How many jianbings did the user order?",
    "answer": "Two.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01570.mp4",
    "question_id": "01570_8",
    "clip_path": "clips/01570/01570__0039500_0048500.mp4"
  },
  {
    "timestamp": "01:11 - 01:28",
    "context": "[01:11 - 01:28] The user orders two douhuas and pays; a short, sharp beep is followed by “Payment successful.”",
    "question_type": "Counting",
    "question": "How many douhuas did the user order at the dessert stall?",
    "answer": "Two.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01570.mp4",
    "question_id": "01570_9",
    "clip_path": "clips/01570/01570__0070500_0088500.mp4"
  },
  {
    "timestamp": "01:11 - 01:28",
    "context": "[01:11 - 01:28] A short, sharp beep from the payment machine is followed by an automated voice: “Payment successful.”",
    "question_type": "Temporal Information",
    "question": "After the short beep at the dessert stall, what sound occurred next?",
    "answer": "An automated voice saying, “Payment successful.”",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01570.mp4",
    "question_id": "01570_10",
    "clip_path": "clips/01570/01570__0070500_0088500.mp4"
  },
  {
    "timestamp": "01:30 - 01:56",
    "context": "[01:30 - 01:56] While toppings are added, soft clinking of a metal spoon against metal topping containers is audible.",
    "question_type": "Sound Source Identification",
    "question": "What produced the soft clinking sounds during the topping process?",
    "answer": "A metal spoon striking the metal topping containers.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01570.mp4",
    "question_id": "01570_11",
    "clip_path": "clips/01570/01570__0089500_0116500.mp4"
  },
  {
    "timestamp": "01:30 - 01:56",
    "context": "[01:30 - 01:56] The user requests: “One needs peanuts, raisins, popping boba… and coconut jelly.”",
    "question_type": "Counting",
    "question": "How many topping types were requested for the first douhua?",
    "answer": "Four: peanuts, raisins, popping boba, and coconut jelly.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01570.mp4",
    "question_id": "01570_12",
    "clip_path": "clips/01570/01570__0089500_0116500.mp4"
  },
  {
    "timestamp": "00:56 - 01:00",
    "context": "[00:56 - 01:00] The vendor says, “There’s one cold noodle ahead.” The user replies she’ll buy something else first and come back.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the user choose to buy something else before returning to the jianbing stall?",
    "answer": "Because there was one cold noodle order ahead of hers.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01570.mp4",
    "question_id": "01570_13",
    "clip_path": "clips/01570/01570__0055500_0060500.mp4"
  },
  {
    "timestamp": "01:57 - 02:11",
    "context": "[01:57 - 02:11] The vendor packs two desserts with distinct rustling and explains, “I put the first one you chose on top.”",
    "question_type": "Inferential & Contextual Causality",
    "question": "What was the purpose of the vendor explaining the packing method for the desserts?",
    "answer": "To help the user identify them—the first chosen dessert was placed on top.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01570.mp4",
    "question_id": "01570_14",
    "clip_path": "clips/01570/01570__0116500_0131500.mp4"
  },
  {
    "timestamp": "02:20 - 03:00",
    "context": "[02:20 - 03:00] During jianbing preparation, the sizzling of batter hitting a hot griddle is prominent.",
    "question_type": "Sound Source Identification",
    "question": "What caused the prominent sizzling sound while the vendor was preparing food?",
    "answer": "Batter hitting the hot griddle while making jianbings.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01570.mp4",
    "question_id": "01570_15",
    "clip_path": "clips/01570/01570__0139500_0180500.mp4"
  },
  {
    "timestamp": "03:22 - 03:35",
    "context": "[03:22 - 03:35] After finishing, the vendor hands over the jianbings and says one has more chili and the other has less.",
    "question_type": "Cross-Modal Reasoning",
    "question": "Following the cooking sounds, what items were handed to the user and how were they differentiated?",
    "answer": "Two jianbings; one was spicier (more chili) and the other had less chili.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01570.mp4",
    "question_id": "01570_16",
    "clip_path": "clips/01570/01570__0201500_0215500.mp4"
  },
  {
    "timestamp": "00:00 - 00:12",
    "context": "[00:00 - 00:12] The user walks along a paved sidewalk; her white cane produces a continuous, rhythmic tapping sound on the ground.",
    "question_type": "Sound Characteristics",
    "question": "How would you describe the cane's tapping sound during 00:00–00:12?",
    "answer": "Continuous and rhythmic.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01574.mp4",
    "question_id": "01574_1",
    "clip_path": "clips/01574/01574__0000000_0012500.mp4"
  },
  {
    "timestamp": "00:12 - 00:27",
    "context": "[00:12 - 00:27] She walks up a moss-covered brick ramp. The cane's tapping becomes slightly duller and occasionally scrapes on the uneven surface.",
    "question_type": "Sound Characteristics",
    "question": "How did the cane's tapping texture change when she went up the ramp?",
    "answer": "It became slightly duller with occasional scraping.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01574.mp4",
    "question_id": "01574_2",
    "clip_path": "clips/01574/01574__0011500_0027500.mp4"
  },
  {
    "timestamp": "00:12 - 00:27",
    "context": "[00:12 - 00:27] She says, “I was a little defiant today... I'll show her I can come up this way too,” while choosing the ramp over the stairs.",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did she choose the ramp instead of the stairs in this segment?",
    "answer": "She wanted to be defiant and show her friend she could come up the ramp too.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01574.mp4",
    "question_id": "01574_3",
    "clip_path": "clips/01574/01574__0011500_0027500.mp4"
  },
  {
    "timestamp": "00:12 - 00:27",
    "context": "[00:12 - 00:27] As she reaches the top of the ramp, a soft grunt of exertion is audible.",
    "question_type": "Temporal Information",
    "question": "When did the soft grunt of exertion occur?",
    "answer": "As she reached the top of the ramp.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01574.mp4",
    "question_id": "01574_4",
    "clip_path": "clips/01574/01574__0011500_0027500.mp4"
  },
  {
    "timestamp": "00:27 - 00:50",
    "context": "[00:27 - 00:50] Inside the workout area, she continues on a paved path; the cane's tapping remains constant, providing auditory feedback.",
    "question_type": "Temporal Information",
    "question": "Was the cane's tapping constant or variable while inside the workout area?",
    "answer": "Constant, providing auditory feedback about the ground.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01574.mp4",
    "question_id": "01574_5",
    "clip_path": "clips/01574/01574__0026500_0050500.mp4"
  },
  {
    "timestamp": "00:50 - 01:06",
    "context": "[00:50 - 01:06] Approaching a metal picnic table, her cane makes several light, metallic clinking sounds as it taps the table and benches.",
    "question_type": "Sound Source Identification",
    "question": "What produced the light, metallic clinking sounds near the table?",
    "answer": "Her cane tapping the metal picnic table and benches.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01574.mp4",
    "question_id": "01574_6",
    "clip_path": "clips/01574/01574__0049500_0066500.mp4"
  },
  {
    "timestamp": "00:50 - 01:06",
    "context": "[00:50 - 01:06] After finding the table, she places her cane on it, creating a soft clattering sound.",
    "question_type": "Sound Characteristics",
    "question": "What was the quality of the sound when she placed her cane on the table?",
    "answer": "A soft clattering sound.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01574.mp4",
    "question_id": "01574_7",
    "clip_path": "clips/01574/01574__0049500_0066500.mp4"
  },
  {
    "timestamp": "01:06 - 01:22",
    "context": "[01:06 - 01:22] She opens the gift; rustling tissue paper is heard. Unfolding the note produces a crisp rustle.",
    "question_type": "Sound Characteristics",
    "question": "What was the sound quality of the paper as it unfolded?",
    "answer": "A crisp rustle.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01574.mp4",
    "question_id": "01574_8",
    "clip_path": "clips/01574/01574__0065500_0082500.mp4"
  },
  {
    "timestamp": "01:06 - 01:22",
    "context": "[01:06 - 01:22] She says, “I'm too embarrassed to find a Be My Eyes volunteer. I'll ask a friend to help me read it.”",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did she decide not to use a Be My Eyes volunteer?",
    "answer": "She felt too embarrassed and chose to ask a friend to read instead.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01574.mp4",
    "question_id": "01574_9",
    "clip_path": "clips/01574/01574__0065500_0082500.mp4"
  },
  {
    "timestamp": "01:22 - 01:52",
    "context": "[01:22 - 01:52] She holds up her phone, which reads the note aloud using a clear text-to-speech voice.",
    "question_type": "Sound Source Identification",
    "question": "What device read the note aloud with a clear voice?",
    "answer": "Her phone using text-to-speech.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01574.mp4",
    "question_id": "01574_10",
    "clip_path": "clips/01574/01574__0081500_0112500.mp4"
  },
  {
    "timestamp": "01:22 - 01:52",
    "context": "[01:22 - 01:52] The phone's audio says the gift is a mattress already on her bed; she laughs and says, “I ran here for nothing.”",
    "question_type": "Inferential & Contextual Causality",
    "question": "What prompted her to say, “I ran here for nothing”?",
    "answer": "The note revealed the real gift—a mattress—was already on her bed, making the trip unnecessary.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01574.mp4",
    "question_id": "01574_11",
    "clip_path": "clips/01574/01574__0081500_0112500.mp4"
  },
  {
    "timestamp": "01:52 - 02:09",
    "context": "[01:52 - 02:09] She places the paper back (rustling) and unscrews the thermos lid, producing a distinct plastic-on-metal squeak.",
    "question_type": "Sound Source Identification",
    "question": "What action caused the distinct plastic-on-metal squeaking sound?",
    "answer": "Unscrewing the thermos lid.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01574.mp4",
    "question_id": "01574_12",
    "clip_path": "clips/01574/01574__0111500_0129500.mp4"
  },
  {
    "timestamp": "01:52 - 02:09",
    "context": "[01:52 - 02:09] As she handles the thermos, a second woman's voice is heard from nearby, surprising her.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "Where did the second woman's voice originate relative to the user?",
    "answer": "From nearby.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01574.mp4",
    "question_id": "01574_13",
    "clip_path": "clips/01574/01574__0111500_0129500.mp4"
  },
  {
    "timestamp": "01:52 - 02:09",
    "context": "[01:52 - 02:09] Before the friend's voice appears, two non-speech sounds occur: rustling paper and the thermos lid squeak.",
    "question_type": "Counting",
    "question": "Before the friend's voice is heard, how many distinct non-speech sounds occur in this segment?",
    "answer": "Two: the paper rustling as it’s returned to the box and the plastic-on-metal squeak from unscrewing the thermos lid.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01574.mp4",
    "question_id": "01574_14",
    "clip_path": "clips/01574/01574__0111500_0129500.mp4"
  },
  {
    "timestamp": "02:09 - 02:17",
    "context": "[02:09 - 02:17] The friend says, “I'm here to get you to exercise!” Then a megaphone she holds erupts: “Startup sound... playing sound...”",
    "question_type": "Cross-Modal Reasoning",
    "question": "When the friend held up a megaphone, what sound immediately followed?",
    "answer": "A loud, pre-recorded electronic voice announcing, “Startup sound... playing sound...”",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01574.mp4",
    "question_id": "01574_15",
    "clip_path": "clips/01574/01574__0128500_0137500.mp4"
  },
  {
    "timestamp": "02:09 - 02:17",
    "context": "[02:09 - 02:17] A pre-recorded electronic voice suddenly erupts from the megaphone.",
    "question_type": "Sound Characteristics",
    "question": "How would you describe the megaphone's startup audio in terms of volume and type?",
    "answer": "Loud and electronic, with pre-recorded voice prompts.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01574.mp4",
    "question_id": "01574_16",
    "clip_path": "clips/01574/01574__0128500_0137500.mp4"
  },
  {
    "timestamp": "02:17 - 02:53",
    "context": "[02:17 - 02:53] The megaphone continues to blast a loud, cheerful, rhythmic exercise chant.",
    "question_type": "Temporal Information",
    "question": "Is the exercise chant from the megaphone brief or continuous during this interval?",
    "answer": "Continuous; it keeps blasting throughout 02:17–02:53.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01574.mp4",
    "question_id": "01574_17",
    "clip_path": "clips/01574/01574__0136500_0173500.mp4"
  },
  {
    "timestamp": "02:17 - 02:53",
    "context": "[02:17 - 02:53] The friend approaches while holding the blaring megaphone.",
    "question_type": "Spatial Location (Direction & Distance)",
    "question": "How does the megaphone’s spatial relationship to the camera change during this segment?",
    "answer": "It moves closer as the friend approaches with the blaring megaphone.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01574.mp4",
    "question_id": "01574_18",
    "clip_path": "clips/01574/01574__0136500_0173500.mp4"
  },
  {
    "timestamp": "02:17 - 02:53",
    "context": "[02:17 - 02:53] The user, over the loud chant, says, “I'm worried the grandpas will be annoyed by you.”",
    "question_type": "Inferential & Contextual Causality",
    "question": "Why did the user worry that “the grandpas will be annoyed”?",
    "answer": "Because her friend was loudly blasting a cheerful exercise chant on a megaphone in the park.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01574.mp4",
    "question_id": "01574_19",
    "clip_path": "clips/01574/01574__0136500_0173500.mp4"
  },
  {
    "timestamp": "00:27 - 00:50",
    "context": "[00:27 - 00:50] She says, “And then she said it's on the table over there,” and navigates toward the area where they usually use the fitness equipment.",
    "question_type": "Cross-Modal Reasoning",
    "question": "Based on her spoken cue “it's on the table over there,” where does she head next?",
    "answer": "Toward the table area in the workout zone where they usually use the fitness equipment.",
    "video_path": "/inspire/hdd/project/brainsignals/zhubingwen-253108120125/EgoBlind/videos/01574.mp4",
    "question_id": "01574_20",
    "clip_path": "clips/01574/01574__0026500_0050500.mp4"
  }
]